import time
import statistics
import os
import psutil
import pandas as pd
import numpy as np
from ATF.core.adaptoflux import AdaptoFlux
import logging
from tqdm import tqdm
import threading  # âœ… ç”¨äºè®°å½•æ´»è·ƒçº¿ç¨‹æ•°
import platform   # âœ… ç”¨äºè®°å½• Python ç‰ˆæœ¬

# ========================
# é…ç½®åŒºï¼ˆè¯·æ ¹æ®ä½ çš„è·¯å¾„ä¿®æ”¹ï¼‰
# ========================
REPEAT_ROUNDS = 3           # æ€»å…±è·‘3è½®ï¼Œä¸¢å¼ƒç¬¬1è½®ï¼ˆé¢„çƒ­ï¼‰
INFERENCE_PER_ROUND = 100 # æ¯è½®æ¨ç†æ¬¡æ•°ï¼ˆæ³¨æ„ï¼šä½ å¾ªç¯ä¸­æ˜¯100æ¬¡ï¼Œè¿™é‡Œåº”ä¿æŒä¸€è‡´æˆ–ä¿®æ­£ï¼‰
WARMUP_ROUNDS = 1           # å‰1è½®ä¸ºé¢„çƒ­ï¼Œä¸è®¡å…¥ç»Ÿè®¡
TEST_DATA_PATH = 'experiments/PipelineParallel_exp2/data/test_processed.csv'  # æµ‹è¯•æ•°æ®è·¯å¾„
MODEL_BASE_DIR = 'experiments/PipelineParallel_exp2/models'

# è‡ªåŠ¨ç”Ÿæˆ model_1 åˆ° model_30ï¼ˆå…±30ä¸ªï¼‰
selected_models = [
    os.path.join(MODEL_BASE_DIR, f"model_{i}") 
    for i in range(1, 31)  # 1 to 30
]

# è¾“å‡º CSV è·¯å¾„
OUTPUT_CSV = 'experiments/PipelineParallel_exp2/results_GIL/benchmark_results.csv'

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(name)s: %(message)s')
logger = logging.getLogger(__name__)

# ========================
# æ ¸å¿ƒç±»ä¸å‡½æ•°
# ========================

class PipelineExecutor:
    """å°è£… AdaptoFlux æ¨¡å‹çš„æµæ°´çº¿æ¨ç†æ‰§è¡Œå™¨"""
    def __init__(self, model_path: str, num_cores: int):
        self.model_path = model_path
        self.num_cores = num_cores
        self.adaptoflux = None
        self._load_model()

    def _load_model(self):
        """åŠ è½½ AdaptoFlux æ¨¡å‹"""
        try:
            # åˆ›å»ºç©º AdaptoFlux å®ä¾‹ï¼ˆvalues/labels ä¼šåœ¨æ¨ç†æ—¶ä¼ å…¥ï¼‰
            self.adaptoflux = AdaptoFlux(
                values=np.zeros((1, 1)),  # å ä½ï¼Œå®é™…æ¨ç†æ—¶ä¼ å…¥
                labels=np.zeros(1),       # å ä½
                methods_path='experiments/PipelineParallel_exp2/scripts/methods_GIL.py'
            )
            # åŠ è½½ä¿å­˜çš„å›¾ç»“æ„
            self.adaptoflux.load_model(folder=self.model_path)
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ {self.model_path}: {e}")
            raise e

    def forward(self, values: np.ndarray):
        """æ‰§è¡Œä¸€æ¬¡æµæ°´çº¿æ¨ç†ï¼ˆä¸å…³å¿ƒè¾“å‡ºï¼Œåªæµ‹æ€§èƒ½ï¼‰"""
        try:
            _ = self.adaptoflux.infer_with_task_parallel(values, num_workers=self.num_cores)
            return True  # æˆåŠŸ
        except Exception as e:
            logger.error(f"âŒ æ¨ç†å¤±è´¥: {e}")
            return False  # å¤±è´¥

    def get_graph_node_count(self):
        """è·å–è®¡ç®—å›¾ä¸­å®é™…å‚ä¸æ‰§è¡Œçš„èŠ‚ç‚¹æ•°é‡ï¼ˆæ’é™¤ root å’Œ collapseï¼‰"""
        if not self.adaptoflux or not hasattr(self.adaptoflux, 'graph'):
            return 0
        return len([
            n for n in self.adaptoflux.graph.nodes 
            if n not in ["root", "collapse"]
        ])

def load_input_data():
    """åŠ è½½æµ‹è¯•æ•°æ®ï¼Œå–å‰100è¡Œç”¨äºæ¨ç†"""
    try:
        df = pd.read_csv(TEST_DATA_PATH)
        if 'Survived' in df.columns:
            values = df.drop(columns=['Survived']).values
        else:
            values = df.values
        # å–å‰100è¡Œ
        values = values[:100].astype(np.float64)
        logger.info(f"âœ… è¾“å…¥æ•°æ®åŠ è½½æˆåŠŸ: {values.shape}")
        return values
    except Exception as e:
        logger.error(f"âŒ è¾“å…¥æ•°æ®åŠ è½½å¤±è´¥: {e}")
        raise e

def log_result(model_path: str, num_cores: int, avg_latency_ms: float,
               throughput: float, std_latency_ms: float,
               graph_node_count: int, thread_count_before: int, thread_count_after: int):
    """å°†ç»“æœè¿½åŠ å†™å…¥ CSV æ–‡ä»¶ï¼ˆè‹¥æ–‡ä»¶ä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»ºå¹¶å†™å…¥è¡¨å¤´ï¼‰"""
    import csv
    import os

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)

    file_exists = os.path.isfile(OUTPUT_CSV)

    with open(OUTPUT_CSV, 'a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        if not file_exists:
            # å†™å…¥è¡¨å¤´ï¼ˆä»…å½“æ–‡ä»¶ä¸å­˜åœ¨æ—¶ï¼‰
            writer.writerow([
                'model_path', 'num_cores', 'avg_latency_ms',
                'throughput_samples_per_sec', 'std_latency_ms',
                'timestamp', 'cpu_util_percent', 'memory_mb', 'python_version',
                'graph_node_count', 'thread_count_before', 'thread_count_after'
            ])
            logger.info("ğŸ†• åˆ›å»ºæ–°ç»“æœæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´")

        # è·å–é¢å¤–ä¿¡æ¯
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        cpu_util = psutil.cpu_percent(interval=None)  # çŸ­æš‚é—´éš”è·å–æ›´å‡†ç¡®çš„ CPU åˆ©ç”¨ç‡
        memory_mb = psutil.Process().memory_info().rss / (1024 * 1024)  # MB
        py_version = platform.python_version()

        # å†™å…¥æ•°æ®è¡Œ
        writer.writerow([
            model_path, num_cores, f"{avg_latency_ms:.6f}",
            f"{throughput:.6f}", f"{std_latency_ms:.6f}",
            timestamp, f"{cpu_util:.1f}", f"{memory_mb:.1f}", py_version,
            graph_node_count, thread_count_before, thread_count_after
        ])
    logger.info(f"ğŸ“Š ç»“æœå·²è¿½åŠ : {model_path} @ {num_cores}æ ¸")

# ========================
# ä¸»æ‰§è¡Œé€»è¾‘ï¼ˆå¸¦è¿›åº¦æ¡ + ETA + è·³è¿‡å·²æµ‹è¯•é…ç½®ï¼‰
# ========================

def main():
    """ä¸»å‡½æ•°ï¼šéå†æ‰€æœ‰æ¨¡å‹å’Œæ ¸å¿ƒé…ç½®ï¼Œæ‰§è¡Œæ€§èƒ½æµ‹è¯•"""
    # åŠ è½½è¾“å…¥æ•°æ®ï¼ˆå‰100è¡Œï¼‰
    fixed_input = load_input_data()

    # æ„å»ºæ‰€æœ‰é…ç½®ç»„åˆ
    all_configs = [(model_path, num_cores) for model_path in selected_models for num_cores in [1, 2, 4, 8, 16]]

    # âœ… è·³è¿‡å·²æµ‹è¯•çš„é…ç½®
    existing_configs = set()
    if os.path.exists(OUTPUT_CSV):
        try:
            df_existing = pd.read_csv(OUTPUT_CSV)
            for _, row in df_existing.iterrows():
                # ç¡®ä¿å­—æ®µå­˜åœ¨
                if 'model_path' in row and 'num_cores' in row:
                    existing_configs.add((row['model_path'], row['num_cores']))
            logger.info(f"ğŸ“‹ å·²æ£€æµ‹åˆ° {len(existing_configs)} ä¸ªå·²å®Œæˆçš„é…ç½®ï¼Œå°†è·³è¿‡")
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å–å·²æœ‰ç»“æœå¤±è´¥ï¼Œä¸è·³è¿‡ä»»ä½•é…ç½®: {e}")

    # è¿‡æ»¤æ‰å·²æµ‹è¯•çš„é…ç½®
    filtered_configs = [
        (mp, nc) for mp, nc in all_configs 
        if (mp, nc) not in existing_configs
    ]
    total_configs = len(filtered_configs)

    if total_configs == 0:
        logger.info("ğŸ‰ æ‰€æœ‰é…ç½®å‡å·²æµ‹è¯•å®Œæˆï¼Œæ— éœ€é‡å¤è¿è¡Œã€‚")
        return

    logger.info(f"ğŸ¯ æ€»å…±è¦æµ‹è¯• {total_configs} ä¸ªé…ç½®ï¼ˆ{len(selected_models)} ä¸ªæ¨¡å‹ Ã— 5 æ ¸å¿ƒï¼‰ï¼Œè·³è¿‡ {len(all_configs) - total_configs} ä¸ª")

    # ä¸»è¿›åº¦æ¡
    start_time_total = time.time()
    with tqdm(total=total_configs, desc="Overall Progress", unit="config",
              ncols=120, colour="green", dynamic_ncols=True) as pbar:

        for idx, (model_path, num_cores) in enumerate(filtered_configs):
            logger.info(f"\nğŸš€ å¼€å§‹æµ‹è¯• [{idx+1}/{total_configs}]: {model_path} @ {num_cores} cores")

            try:
                # åˆå§‹åŒ–æ‰§è¡Œå™¨
                executor = PipelineExecutor(model_path, num_cores)
                graph_node_count = executor.get_graph_node_count()

                latencies = []
                throughputs = []

                for round_idx in range(REPEAT_ROUNDS):
                    logger.info(f"â±ï¸  ç¬¬ {round_idx + 1} è½®æ¨ç†å¼€å§‹...")

                    start_time_round = time.perf_counter()

                    success_count = 0
                    thread_count_before = threading.active_count()  # âœ… è®°å½•æ¨ç†å‰æ´»è·ƒçº¿ç¨‹æ•°

                    # âœ… å­è¿›åº¦æ¡ï¼šæ¯è½® INFERENCE_PER_ROUND æ¬¡æ¨ç†
                    with tqdm(total=INFERENCE_PER_ROUND, desc=f"Round {round_idx+1} Inference",
                            unit="iter", ncols=80, leave=False, colour="blue") as sub_pbar:
                        for i in range(INFERENCE_PER_ROUND):
                            if executor.forward(fixed_input):
                                success_count += 1
                            sub_pbar.update(1)

                    thread_count_after = threading.active_count()
                    end_time_round = time.perf_counter()
                    total_time_sec = end_time_round - start_time_round

                    # âœ… è®¡ç®—æœ¬è½®æŒ‡æ ‡ï¼ˆåŸºäº INFERENCE_PER_ROUND æ¬¡æ¨ç†ï¼‰
                    avg_latency_ms = (total_time_sec * 1000) / INFERENCE_PER_ROUND
                    throughput = INFERENCE_PER_ROUND / total_time_sec

                    if round_idx >= WARMUP_ROUNDS:
                        latencies.append(avg_latency_ms)
                        throughputs.append(throughput)

                    logger.info(
                        f"âœ… ç¬¬ {round_idx + 1} è½®å®Œæˆ: "
                        f"Latency={avg_latency_ms:.2f}ms, "
                        f"Throughput={throughput:.1f} samples/sec, "
                        f"Success={success_count}/{INFERENCE_PER_ROUND}, "
                        f"Threads: {thread_count_before} â†’ {thread_count_after}"
                    )

                # è®¡ç®—æœ€ç»ˆç»“æœï¼ˆä»…ä½¿ç”¨åä¸¤è½®ï¼‰
                if len(latencies) > 0:
                    final_avg_latency = statistics.mean(latencies)
                    final_avg_throughput = statistics.mean(throughputs)
                    final_std_latency = statistics.stdev(latencies) if len(latencies) > 1 else 0.0

                    log_result(
                        model_path,
                        num_cores,
                        final_avg_latency,
                        final_avg_throughput,
                        final_std_latency,
                        graph_node_count,
                        thread_count_before,
                        thread_count_after
                    )

                    logger.info(
                        f"ğŸ“ˆ æœ€ç»ˆç»“æœ: "
                        f"Avg Latency={final_avg_latency:.2f}ms Â± {final_std_latency:.2f}, "
                        f"Throughput={final_avg_throughput:.1f} samples/sec, "
                        f"Graph Nodes={graph_node_count}, "
                        f"Threads {thread_count_before} â†’ {thread_count_after}"
                    )
                else:
                    logger.error("âŒ æ— æœ‰æ•ˆè½®æ¬¡æ•°æ®")

            except Exception as e:
                logger.error(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥ {model_path} @ {num_cores} cores: {e}")

            finally:
                # æ›´æ–°ä¸»è¿›åº¦æ¡ + ETA
                pbar.update(1)
                elapsed_total = time.time() - start_time_total
                completed = pbar.n
                total = pbar.total
                if completed > 0 and total > 0:
                    avg_time_per_config = elapsed_total / completed
                    remaining_configs = total - completed
                    eta_seconds = int(avg_time_per_config * remaining_configs)
                    eta_str = time.strftime("%H:%M:%S", time.gmtime(eta_seconds))
                    pbar.set_postfix({"ETA": eta_str}, refresh=True)

    logger.info(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼ç»“æœå·²è¿½åŠ è‡³: {OUTPUT_CSV}")


if __name__ == "__main__":
    main()