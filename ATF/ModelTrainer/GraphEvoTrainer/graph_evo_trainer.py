# graph_evo_trainer.py
from ..model_trainer import ModelTrainer
import numpy as np
import logging
import random
import copy
from typing import Optional, List, Dict, Any, Tuple
import os
import json
import traceback
from ...PathGenerator.path_generator import PathGenerator
from ...GraphProcessor.graph_processor import GraphProcessor
from ...core.evolved_method import EvolvedMethod
from collections import defaultdict
import networkx as nx
from networkx.readwrite import json_graph
from datetime import datetime

from .Components import (
    BFSSubgraphSampler,
    SubgraphIOExtractor,
    SubgraphReplacer,
    MSEEquivalenceChecker
)

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class GraphEvoTrainer(ModelTrainer):
    """
    ä¸€ä¸ªç»§æ‰¿è‡ª ModelTrainer çš„å…·ä½“è®­ç»ƒå™¨ã€‚
    è¯¥è®­ç»ƒå™¨å®ç°äº† AdaptoFlux çš„å›¾æ¼”ï¼ˆGraphEvoï¼‰ä¼˜åŒ–æ¡†æ¶ã€‚
    é€šè¿‡â€œå¤šæ ·åˆå§‹åŒ– â†’ é€èŠ‚ç‚¹ç²¾ç‚¼ â†’ æ¨¡å—åŒ–å‹ç¼© â†’ æ–¹æ³•æ± è¿›åŒ–â€çš„é—­ç¯æµç¨‹ï¼Œå®ç°å›¾ç»“æ„çš„è‡ªè¿›åŒ–ä¼˜åŒ–ã€‚
    """

    def __init__(
        self,
        adaptoflux_instance,
        num_initial_models: int = 5,
        max_refinement_steps: int = 100,
        compression_threshold: float = 0.95,
        max_init_layers: int = 3,
        init_mode: str = "fixed",
        init_layers_list: Optional[List[int]] = None,
        frozen_nodes: Optional[List[str]] = None,
        frozen_methods: Optional[List[str]] = None,
        refinement_strategy: str = "random_single",
        candidate_pool_mode: str = "group",
        fallback_mode: Optional[str] = None,
        enable_compression: bool = False,
        enable_evolution: bool = True,
        evolution_sampling_frequency: int = 1,
        evolution_trigger_count: int = 3,
        evolution_cleanup_mode: str = "full",
        consensus_threshold: Optional[float] = None,  # <-- æ–°å¢
        methods_per_evolution: int = 1,
        min_subgraph_size_for_evolution: int = 2,  # <-- æ–°å¢å‚æ•°
        verbose: bool = True,
        **kwargs
    ):
        """
        åˆå§‹åŒ– GraphEvoTrainerï¼Œç”¨äºæ‰§è¡Œ AdaptoFlux çš„å›¾ç»“æ„è‡ªè¿›åŒ–ä¼˜åŒ–æµç¨‹ã€‚

        æœ¬è®­ç»ƒå™¨é€šè¿‡å››ä¸ªæ ¸å¿ƒé˜¶æ®µå®ç°å›¾ç»“æ„çš„è¿­ä»£ä¼˜åŒ–ï¼š
        1. **å¤šæ ·åˆå§‹åŒ–**ï¼šç”Ÿæˆå¤šä¸ªéšæœºåˆå§‹å›¾ï¼Œé€‰æ‹©æ€§èƒ½æœ€ä¼˜è€…ä½œä¸ºèµ·ç‚¹ï¼›
        2. **é€èŠ‚ç‚¹ç²¾ç‚¼**ï¼šå¯¹å›¾ä¸­å¯ä¼˜åŒ–èŠ‚ç‚¹å°è¯•æ–¹æ³•æ›¿æ¢ï¼Œå±€éƒ¨æœç´¢æ›´ä¼˜ç»“æ„ï¼›
        3. **æ–¹æ³•æ± è¿›åŒ–**ï¼ˆå¯é€‰ï¼‰ï¼šåŸºäºè®°å½•çš„é«˜æ€§èƒ½å›¾ç»“æ„ï¼ŒæŠ½è±¡æ–°æ–¹æ³•æ³¨å…¥æ–¹æ³•æ± ã€‚
        4. **æ¨¡å—åŒ–å‹ç¼©**ï¼ˆå¯é€‰ï¼‰ï¼šè¯†åˆ«å¹¶æ›¿æ¢ç­‰æ•ˆçš„é«˜æ•ˆå­å›¾ï¼Œå®ç°ç»“æ„ç®€åŒ–ï¼›
        

        å‚æ•°æ§åˆ¶è¯´æ˜ï¼š
        - **åˆå§‹åŒ–æ§åˆ¶**ï¼šé€šè¿‡ `num_initial_models`ã€`init_mode` ç­‰æ§åˆ¶åˆå§‹å¤šæ ·æ€§ï¼›
        - **ç²¾ç‚¼æ§åˆ¶**ï¼šé€šè¿‡ `refinement_strategy`ã€`frozen_nodes` ç­‰æ§åˆ¶å±€éƒ¨æœç´¢è¡Œä¸ºï¼›
        - **è¿›åŒ–æ§åˆ¶**ï¼šé€šè¿‡ `evolution_sampling_frequency`ã€`evolution_trigger_count` ç­‰æ§åˆ¶è¿›åŒ–è§¦å‘æœºåˆ¶ã€‚
        - **å‹ç¼©æ§åˆ¶**ï¼šé€šè¿‡ `enable_compression`ã€`compression_threshold` æ§åˆ¶æ˜¯å¦å¯ç”¨åŠç­‰æ•ˆæ€§æ ‡å‡†ï¼›
        :param adaptoflux_instance: å·²åˆå§‹åŒ–çš„ AdaptoFlux å®ä¾‹ï¼Œä½œä¸ºä¼˜åŒ–çš„åŸºç¡€æ¨¡æ¿ã€‚
        :param num_initial_models: å¤šæ ·åˆå§‹åŒ–é˜¶æ®µç”Ÿæˆçš„å€™é€‰æ¨¡å‹æ•°é‡ã€‚
        :param max_refinement_steps: å•æ¬¡ç²¾ç‚¼é˜¶æ®µå…è®¸çš„æœ€å¤§ä¼˜åŒ–æ­¥æ•°ã€‚
        :param max_total_refinement_attempts: æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å…è®¸çš„æœ€å¤§å€™é€‰æ–¹æ³•è¯„ä¼°æ¬¡æ•°ï¼ˆç”¨äºé™åˆ¶è®¡ç®—èµ„æºæ¶ˆè€—ï¼‰ã€‚è‹¥ä¸º None åˆ™ä¸é™åˆ¶ã€‚
        :param compression_threshold: æ¨¡å—åŒ–å‹ç¼©é˜¶æ®µåˆ¤å®šå­å›¾ç­‰æ•ˆçš„ MSE ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå€¼è¶Šå°è¦æ±‚è¶Šä¸¥æ ¼ï¼‰ã€‚
        :param max_init_layers: åˆå§‹åŒ–æ—¶æ¯ä¸ªå€™é€‰æ¨¡å‹æœ€å¤šæ·»åŠ çš„éšæœºå±‚æ•°ï¼ˆä»…åœ¨ init_mode="fixed" æ—¶ç”Ÿæ•ˆï¼‰ã€‚
        :param init_mode: åˆå§‹åŒ–æ¨¡å¼ï¼Œ"fixed" è¡¨ç¤ºæ‰€æœ‰æ¨¡å‹æ·»åŠ ç›¸åŒå±‚æ•°ï¼Œ"list" è¡¨ç¤ºæŒ‰ init_layers_list æŒ‡å®šã€‚
        :param init_layers_list: å½“ init_mode="list" æ—¶ï¼ŒæŒ‡å®šæ¯ä¸ªå€™é€‰æ¨¡å‹çš„åˆå§‹åŒ–å±‚æ•°ï¼Œé•¿åº¦éœ€ â‰¥ num_initial_modelsã€‚
        :param frozen_nodes: æ˜¾å¼å†»ç»“çš„èŠ‚ç‚¹åç§°åˆ—è¡¨ï¼ˆå¦‚ ["root", "collapse"]ï¼‰ï¼Œè¿™äº›èŠ‚ç‚¹åœ¨ç²¾ç‚¼é˜¶æ®µä¸ä¼šè¢«ä¿®æ”¹ã€‚
        :param frozen_methods: æ˜¾å¼å†»ç»“çš„æ–¹æ³•åç§°åˆ—è¡¨ï¼ˆå¦‚ ["return_value"]ï¼‰ï¼Œä½¿ç”¨è¿™äº›æ–¹æ³•çš„èŠ‚ç‚¹å°†è‡ªåŠ¨è¢«å†»ç»“ã€‚
        :param refinement_strategy: ç²¾ç‚¼ç­–ç•¥ï¼Œ"random_single"ï¼ˆéšæœºå•ç‚¹ä¼˜åŒ–ï¼‰æˆ– "full_sweep"ï¼ˆå…¨å›¾éå†ä¼˜åŒ–ï¼‰ã€‚
        :param candidate_pool_mode: æ„å»ºå€™é€‰æ–¹æ³•æ± çš„ç­–ç•¥ï¼Œ"all"ï¼ˆæ‰€æœ‰æ–¹æ³•ï¼‰ã€"group"ï¼ˆåŒç»„æ–¹æ³•ï¼‰æˆ– "self"ï¼ˆä»…è‡ªèº«ï¼‰ã€‚
        :param fallback_mode: å½“æ— ç±»å‹å…¼å®¹æ–¹æ³•æ—¶çš„å…œåº•ç­–ç•¥ï¼Œ"all"/"group_first"/"self"/"error"ã€‚
        :param enable_compression: æ˜¯å¦å¯ç”¨æ¨¡å—åŒ–å‹ç¼©é˜¶æ®µã€‚
        :param enable_evolution: æ˜¯å¦å¯ç”¨æ–¹æ³•æ± è¿›åŒ–é˜¶æ®µã€‚
        :param evolution_sampling_frequency: æ¯éš”å¤šå°‘ä¸ªè®­ç»ƒè½®æ¬¡ï¼ˆå³ä¸€æ¬¡å®Œæ•´çš„ã€Œç²¾ç‚¼+å‹ç¼©ã€å¾ªç¯ï¼‰è®°å½•ä¸€æ¬¡å½“å‰å›¾ç»“æ„å¿«ç…§ï¼Œç”¨äºåç»­æ–¹æ³•æ± è¿›åŒ–ã€‚ä¾‹å¦‚è®¾ä¸º 2 è¡¨ç¤ºæ¯ 2 è½®ä¿å­˜ä¸€æ¬¡å¿«ç…§ã€‚
        :param evolution_trigger_count: å½“ç´¯è®¡è®°å½•çš„å›¾å¿«ç…§æ•°é‡è¾¾åˆ°æ­¤å€¼æ—¶ï¼Œè§¦å‘ä¸€æ¬¡æ–¹æ³•æ± è¿›åŒ–ã€‚
        :param evolution_cleanup_mode: è¿›åŒ–å®Œæˆåå¦‚ä½•æ¸…ç†å·²è®°å½•çš„å¿«ç…§ï¼Œ"full"ï¼ˆæ¸…ç©ºå…¨éƒ¨ï¼‰æˆ– "oldest"ï¼ˆä»…ç§»é™¤æœ€æ—©çš„ä¸€ä¸ªï¼‰ã€‚
        :param methods_per_evolution: æ¯æ¬¡æ–¹æ³•æ± è¿›åŒ–æ—¶ï¼Œæœ€å¤šä»è®°å½•çš„å›¾ç»“æ„ä¸­æŠ½è±¡å¹¶æ·»åŠ çš„æ–°æ–¹æ³•æ•°é‡ã€‚
        :param verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—ä¿¡æ¯ã€‚
        :param kwargs: å…¶ä»–å¯é€‰ç»„ä»¶ï¼Œå¦‚è‡ªå®šä¹‰çš„ subgraph_samplerã€io_extractor ç­‰ã€‚
        """

        super().__init__(adaptoflux_instance)
        self.num_initial_models = num_initial_models
        self.max_refinement_steps = max_refinement_steps
        self.compression_threshold = compression_threshold
        self.max_init_layers = max_init_layers
        self.init_mode = init_mode
        self.init_layers_list = init_layers_list
        self.frozen_nodes = set(frozen_nodes) if frozen_nodes else set()
        self.frozen_methods = set(frozen_methods) if frozen_methods else set()  # <-- ä¿å­˜ä¸ºé›†åˆ
        self.candidate_pool_mode = candidate_pool_mode
        self.fallback_mode = fallback_mode or 'self'  
        self.refinement_strategy = refinement_strategy
        self.enable_compression = enable_compression
        self.enable_evolution = enable_evolution
        self.evolution_sampling_frequency = evolution_sampling_frequency
        self.evolution_trigger_count = evolution_trigger_count
        self.evolution_cleanup_mode = evolution_cleanup_mode
        self.consensus_threshold = consensus_threshold
        self.methods_per_evolution = methods_per_evolution    
        self.verbose = verbose

        self.subgraph_sampler = kwargs.get('subgraph_sampler') or BFSSubgraphSampler(max_nodes=4)
        self.io_extractor = kwargs.get('io_extractor') or SubgraphIOExtractor()
        self.replacer = kwargs.get('replacer') or SubgraphReplacer()
        self.equivalence_checker = kwargs.get('equivalence_checker') or MSEEquivalenceChecker(
            threshold=self.compression_threshold
        )

        self.min_subgraph_size_for_evolution = min_subgraph_size_for_evolution
        if self.min_subgraph_size_for_evolution < 1:
            raise ValueError("min_subgraph_size_for_evolution must be at least 1")

        # æ ¡éªŒå‚æ•°
        if self.methods_per_evolution < 1:
            raise ValueError("methods_per_evolution must be >= 1")
        if self.init_mode == "list":
            if self.init_layers_list is None:
                raise ValueError("init_mode='list' requires init_layers_list to be provided.")
            if len(self.init_layers_list) < self.num_initial_models:
                raise ValueError(f"init_layers_list length ({len(self.init_layers_list)}) must be >= num_initial_models ({self.num_initial_models})")
        if self.consensus_threshold is not None:
            if not (0.0 <= self.consensus_threshold <= 1.0):
                raise ValueError("consensus_threshold must be in [0.0, 1.0] or None")

        # ç”¨äºè®°å½•å®Œæ•´å›¾ç»“æ„å¿«ç…§ï¼ˆç”¨äºæ–¹æ³•æ± è¿›åŒ–ï¼‰
        self.graph_snapshots: List[Any] = []
        
        self._strategy_map = {
            "random_single": self._refine_random_single_step,
            "full_sweep": self._refine_full_sweep_step,
            # æœªæ¥å¯åŠ ï¼š"weighted_sample": self._refine_weighted_sample_step,
        }
        if self.refinement_strategy not in self._strategy_map:
            raise ValueError(f"Unknown refinement_strategy: {self.refinement_strategy}. "
                            f"Available: {list(self._strategy_map.keys())}")

        # æ ¡éªŒ
        valid_pool_modes = {"all", "group", "self"}
        valid_fallback_modes = {"all", "group_first", "self", "error"}
        if self.candidate_pool_mode not in valid_pool_modes:
            raise ValueError(f"Invalid candidate_pool_mode: {self.candidate_pool_mode}")
        if self.fallback_mode not in valid_fallback_modes:
            raise ValueError(f"Invalid fallback_mode: {self.fallback_mode}")

        if self.enable_compression == True and self.verbose:
            logger.warning("æ¨¡å—åŒ–å‹ç¼©ä¸ºæœªå®Œæˆçš„å®éªŒæ€§åŠŸèƒ½ï¼Œä½¿ç”¨æ—¶å¤§æ¦‚ç‡æœ‰ä¸¥é‡bugï¼Œå¦‚æ— è‡ªè¡Œä¿®æ”¹å»ºè®®ä¸è¦ä½¿ç”¨ï¼")

    def _phase_diverse_initialization(self, input_data: np.ndarray, target: np.ndarray) -> Dict[str, Any]:
        """
        é˜¶æ®µä¸€ï¼šå¤šæ ·åˆå§‹åŒ– (Diverse Initialization)
        éšæœºç”Ÿæˆå¤šç»„åˆå§‹æ¨¡å‹ï¼Œé€šè¿‡å¿«é€Ÿè¯„ä¼°ç­›é€‰å‡ºæœ€ä¼˜è€…ä½œä¸ºä¼˜åŒ–èµ·ç‚¹ã€‚

        :param input_data: ç”¨äºè¯„ä¼°çš„è¾“å…¥æ•°æ®
        :param target: å¯¹åº”çš„æ ‡ç­¾
        :return: åŒ…å«æœ€ä¼˜æ¨¡å‹ä¿¡æ¯çš„å­—å…¸ {'best_model': AdaptoFluxå®ä¾‹, 'best_loss': float, 'best_acc': float}
        """
        if self.verbose:
            logger.info(f"[Phase 1] Diverse Initialization: Generating {self.num_initial_models} candidate models...")

        candidates = []
        for i in range(self.num_initial_models):
            # åˆ›å»ºä¸€ä¸ªæ–°çš„ã€ç‹¬ç«‹çš„ AdaptoFlux å®ä¾‹å‰¯æœ¬
            # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ AdaptoFlux ç±»æœ‰ä¸€ä¸ª `clone()` æ–¹æ³•æˆ–ç±»ä¼¼çš„æœºåˆ¶
            # å¦‚æœæ²¡æœ‰ï¼Œæ‚¨éœ€è¦å®ç°ä¸€ä¸ªæ·±æ‹·è´é€»è¾‘ï¼Œç¡®ä¿ graph å’Œ methods éƒ½è¢«å¤åˆ¶
            try:
                candidate_af = self.adaptoflux.clone()
            except AttributeError:
                # å¦‚æœæ²¡æœ‰ clone æ–¹æ³•ï¼Œåˆ™è¿›è¡Œæ·±æ‹·è´
                candidate_af = copy.deepcopy(self.adaptoflux)
                # é‡ç½®å…¶å†…éƒ¨çŠ¶æ€ï¼Œç¡®ä¿ç‹¬ç«‹æ€§
                candidate_af.graph_processor.graph = copy.deepcopy(self.adaptoflux.graph_processor.graph)
                candidate_af.methods = copy.deepcopy(self.adaptoflux.methods)

            # æ ¹æ®åˆå§‹åŒ–æ¨¡å¼å†³å®šæ·»åŠ å±‚æ•°
            if self.init_mode == "fixed":
                layers_to_add = self.max_init_layers
            elif self.init_mode == "list":
                layers_to_add = self.init_layers_list[i]  # ç¬¬ i ä¸ªå€™é€‰æ¨¡å‹ä½¿ç”¨åˆ—è¡¨ä¸­ç¬¬ i é¡¹
            else:
                raise ValueError(f"Unsupported init_mode: {self.init_mode}")

            # å¯¹å€™é€‰æ¨¡å‹è¿›è¡Œéšæœºåˆå§‹åŒ–
            self._randomly_initialize_graph(candidate_af, num_layers_to_add=layers_to_add)

            # è¯„ä¼°å€™é€‰æ¨¡å‹
            loss = self._evaluate_loss(input_data, target, adaptoflux_instance=candidate_af)
            acc = self._evaluate_accuracy(input_data, target, adaptoflux_instance=candidate_af)

            candidates.append({
                'model': candidate_af,
                'loss': loss,
                'accuracy': acc,
                'id': i
            })

            if self.verbose:
                logger.info(f"  Candidate {i+1}/{self.num_initial_models}: Loss={loss:.6f}, Acc={acc:.4f}")

        # é€‰æ‹©æŸå¤±æœ€ä½çš„æ¨¡å‹ä½œä¸ºæœ€ä¼˜æ¨¡å‹
        best_candidate = min(candidates, key=lambda x: x['loss'])

        if self.verbose:
            logger.info(f"[Phase 1] Selected best initial model (ID: {best_candidate['id']}) with Loss={best_candidate['loss']:.6f}, Acc={best_candidate['accuracy']:.4f}")

        return {
            'best_model': best_candidate['model'],
            'best_loss': best_candidate['loss'],
            'best_accuracy': best_candidate['accuracy']
        }

    def _randomly_initialize_graph(self, adaptoflux_instance, num_layers_to_add: int = 3):
        """
        ä¸€ä¸ªè¾…åŠ©æ–¹æ³•ï¼Œç”¨äºå¯¹ç»™å®šçš„ AdaptoFlux å®ä¾‹è¿›è¡Œéšæœºçš„å›¾ç»“æ„åˆå§‹åŒ–ã€‚
        é€šè¿‡è°ƒç”¨å…¶ `append_nx_layer` æ–¹æ³•éšæœºæ·»åŠ å‡ å±‚ã€‚

        :param adaptoflux_instance: è¦åˆå§‹åŒ–çš„ AdaptoFlux å®ä¾‹
        :param num_layers_to_add: è¦æ·»åŠ çš„å±‚æ•°
        """
        while adaptoflux_instance.graph_processor.layer > 0:
            adaptoflux_instance.remove_last_nx_layer()
        for _ in range(num_layers_to_add):
            candidate_plan = adaptoflux_instance.process_random_method()
            if candidate_plan["valid_groups"]:
                try:
                    adaptoflux_instance.append_nx_layer(
                        candidate_plan,
                        discard_unmatched='to_discard',
                        discard_node_method_name="null"
                    )
                except Exception as e:
                    logger.warning(f"Failed to add random layer during initialization: {e}", exc_info=True)
                    # å¦‚æœæ·»åŠ å¤±è´¥ï¼Œè·³è¿‡ï¼Œä¸å½±å“æ•´ä½“æµç¨‹

    def _phase_node_refinement(self, adaptoflux_instance, input_data: np.ndarray, target: np.ndarray) -> Dict[str, Any]:
        """
        é˜¶æ®µäºŒï¼šé€èŠ‚ç‚¹ç²¾ç‚¼ (Node-wise Refinement)
        
        ç›®æ ‡ï¼š
            å¯¹å›¾ä¸­æ¯ä¸ªâ€œå¯å¤„ç†èŠ‚ç‚¹â€ï¼ˆprocessing nodeï¼‰ï¼Œå°è¯•ç”¨å…¼å®¹çš„æ›¿ä»£æ–¹æ³•è¿›è¡Œæ›¿æ¢ï¼Œ
            ä»¥é™ä½æŸå¤±ï¼ˆæˆ–æå‡å‡†ç¡®ç‡ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ª**å±€éƒ¨æœç´¢ä¼˜åŒ–è¿‡ç¨‹**ã€‚
        
        ç­–ç•¥ï¼š
            æ”¯æŒå¤šç§ä¼˜åŒ–ç­–ç•¥ï¼ˆå¦‚éšæœºå•ç‚¹ã€å®Œæ•´éå†ï¼‰ï¼Œç”± `self.refinement_strategy` æ§åˆ¶ã€‚
            æ¯æ¬¡æ›¿æ¢éƒ½åŸºäºå°æ‰¹é‡æ•°æ®å¿«é€Ÿè¯„ä¼°ï¼Œé¿å…å…¨é‡è®¡ç®—å¼€é”€ã€‚
        
        å†»ç»“æœºåˆ¶ï¼š
            - `frozen_nodes`: æ˜¾å¼å†»ç»“çš„èŠ‚ç‚¹ååˆ—è¡¨ï¼ˆå¦‚ "root", "collapse"ï¼‰
            - `frozen_methods`: æ˜¾å¼å†»ç»“çš„æ–¹æ³•ååˆ—è¡¨ï¼ˆå¦‚ ["return_value"]ï¼‰ï¼Œè‡ªåŠ¨å†»ç»“ä½¿ç”¨è¿™äº›æ–¹æ³•çš„èŠ‚ç‚¹
        
        è¿”å›ï¼š
            åŒ…å«æœ€ç»ˆæ¨¡å‹ã€æ€§èƒ½æŒ‡æ ‡ã€æ­¥æ•°ç­‰ä¿¡æ¯çš„å­—å…¸ã€‚
        """
        # æ‰“å°æ—¥å¿—ï¼šå¼€å§‹ç²¾ç‚¼é˜¶æ®µ
        if self.verbose:
            logger.info(f"[Phase 2] Node-wise Refinement: Starting refinement with strategy '{self.refinement_strategy}'...")


        # è·å–å›¾å¤„ç†å™¨å’Œå½“å‰æ€§èƒ½æŒ‡æ ‡
        gp = adaptoflux_instance.graph_processor
        current_loss = self._evaluate_loss(input_data, target, adaptoflux_instance=adaptoflux_instance)
        current_acc = self._evaluate_accuracy(input_data, target, adaptoflux_instance=adaptoflux_instance)

        # åˆå§‹åŒ–çŠ¶æ€å˜é‡
        improvement_made = False  # æ ‡è®°æœ¬è½®æ˜¯å¦å‘ç”Ÿè¿‡æ”¹è¿›
        steps_taken = 0           # å®é™…æ‰§è¡Œçš„â€œä¼˜åŒ–æ­¥æ•°â€ï¼ˆæ³¨æ„ï¼šä¸åŒç­–ç•¥æ­¥æ•°å®šä¹‰ä¸åŒï¼‰

        # === 1. è·å–æ‰€æœ‰â€œå¯å¤„ç†èŠ‚ç‚¹â€å¹¶åº”ç”¨å†»ç»“è§„åˆ™ ===
        # `_is_processing_node(node)` æ˜¯ GraphProcessor çš„æ–¹æ³•ï¼Œç”¨äºåˆ¤æ–­èŠ‚ç‚¹æ˜¯å¦æ˜¯â€œä¸­é—´è®¡ç®—èŠ‚ç‚¹â€
        # ï¼ˆé€šå¸¸æ’é™¤ "root"ã€"collapse" ç­‰ç‰¹æ®ŠèŠ‚ç‚¹ï¼‰
        processing_nodes = [node for node in gp.graph.nodes() if gp._is_processing_node(node)]

        # åˆå¹¶æ˜¾å¼å†»ç»“çš„èŠ‚ç‚¹å’ŒåŸºäºæ–¹æ³•åå†»ç»“çš„èŠ‚ç‚¹
        final_frozen_nodes = set(self.frozen_nodes)  # æ˜¾å¼å†»ç»“çš„èŠ‚ç‚¹åé›†åˆ

        # å¦‚æœç”¨æˆ·æŒ‡å®šäº†å†»ç»“çš„æ–¹æ³•åï¼ˆå¦‚ ["return_value"]ï¼‰ï¼Œåˆ™è‡ªåŠ¨å†»ç»“æ‰€æœ‰ä½¿ç”¨è¿™äº›æ–¹æ³•çš„èŠ‚ç‚¹
        if self.frozen_methods:
            method_frozen_nodes = {
                node for node in processing_nodes
                if gp.graph.nodes[node].get('method_name') in self.frozen_methods
            }
            final_frozen_nodes.update(method_frozen_nodes)  # åˆå¹¶åˆ°å†»ç»“é›†åˆ

        # ä»å¯å¤„ç†èŠ‚ç‚¹ä¸­ç§»é™¤æ‰€æœ‰å†»ç»“èŠ‚ç‚¹
        if final_frozen_nodes:
            processing_nodes = [node for node in processing_nodes if node not in final_frozen_nodes]

        # æ‰“å°æ—¥å¿—ï¼šæŠ¥å‘Šå¯ä¼˜åŒ–èŠ‚ç‚¹æ•°é‡
        if self.verbose:
            logger.info(f"  Found {len(processing_nodes)} processing nodes to refine "
                        f"(excluding {len(final_frozen_nodes)} frozen nodes).")

        # === 2. è·å–ä¼˜åŒ–ç­–ç•¥å‡½æ•° ===
        # `_strategy_map` å°†å­—ç¬¦ä¸²ç­–ç•¥åæ˜ å°„åˆ°å…·ä½“å®ç°å‡½æ•°
        strategy_func = self._strategy_map[self.refinement_strategy]

        # === 3. ä¸»ä¼˜åŒ–å¾ªç¯ ===
        for step in range(self.max_refinement_steps):

            if (self.max_total_refinement_attempts is not None and
                self._total_refinement_attempts >= self.max_total_refinement_attempts):
                if self.verbose:
                    logger.info(
                        f"[Phase 2] Stopped early: reached max_total_refinement_attempts="
                        f"{self.max_total_refinement_attempts} (at step {step})."
                    )
                break

            # å®‰å…¨é€€å‡ºï¼šé˜²æ­¢è¶…è¿‡æœ€å¤§æ­¥æ•°ï¼ˆè™½ç„¶ç­–ç•¥å‡½æ•°å¯èƒ½ä¸€æ¬¡æ‰§è¡Œå¤šæ­¥ï¼‰
            if steps_taken >= self.max_refinement_steps:
                break

            # è‹¥æ— å¯ä¼˜åŒ–èŠ‚ç‚¹ï¼Œæå‰é€€å‡º
            if not processing_nodes:
                break

            # è°ƒç”¨å…·ä½“ç­–ç•¥å‡½æ•°æ‰§è¡Œä¸€æ¬¡ä¼˜åŒ–å°è¯•
            # è¿”å›å€¼è¯´æ˜ï¼ˆè§ç­–ç•¥å‡½æ•°æ–‡æ¡£ï¼‰ï¼š
            #   imp: bool â†’ æ˜¯å¦å‘ç”Ÿæ”¹è¿›
            #   new_loss, new_acc: æ”¹è¿›åçš„æŒ‡æ ‡
            #   step_inc: int â†’ æœ¬æ¬¡å®é™…æ¶ˆè€—çš„â€œæ­¥æ•°â€ï¼ˆå¦‚ full_sweep ä¸€æ¬¡å¯èƒ½æ›¿æ¢å¤šä¸ªèŠ‚ç‚¹ï¼‰
            #   updated_nodes: List[str] â†’ æ›´æ–°åçš„å¯å¤„ç†èŠ‚ç‚¹åˆ—è¡¨ï¼ˆå›¾ç»“æ„å¯èƒ½å˜åŒ–ï¼‰
            imp, new_loss, new_acc, step_inc, updated_nodes = strategy_func(
                adaptoflux_instance, input_data, target, processing_nodes, current_loss, gp
            )

            if imp:
                # å¦‚æœå‘ç”Ÿæ”¹è¿›ï¼Œæ›´æ–°å…¨å±€çŠ¶æ€
                improvement_made = True
                current_loss = new_loss
                current_acc = new_acc
                steps_taken += step_inc
                processing_nodes = updated_nodes  # ä½¿ç”¨æœ€æ–°èŠ‚ç‚¹åˆ—è¡¨

                # æ³¨æ„ï¼šæ—¥å¿—å·²åœ¨ç­–ç•¥å‡½æ•°å†…éƒ¨æ‰“å°ï¼ˆé¿å…é‡å¤ï¼‰ï¼Œæ‰€ä»¥è¿™é‡Œä¸åšé¢å¤–è¾“å‡º
            else:
                # æ— æ”¹è¿›ï¼šç»§ç»­ä¸‹ä¸€è½®å°è¯•ï¼ˆä¸æå‰ç»ˆæ­¢ï¼Œå› ä¸ºåç»­å¯èƒ½æœ‰æ”¹è¿›ï¼‰
                # ä¹Ÿå¯åœ¨æ­¤å¤„åŠ å…¥â€œæ—©åœâ€é€»è¾‘ï¼ˆå¦‚è¿ç»­ N æ¬¡æ— æ”¹è¿›åˆ™é€€å‡ºï¼‰
                pass

        # === 4. æ‰“å°æœ€ç»ˆç»“æœæ—¥å¿— ===
        if self.verbose:
            if improvement_made:
                logger.info(f"[Phase 2] Refinement completed in {steps_taken} steps. Final Loss: {current_loss:.6f}, Acc: {current_acc:.4f}")
            else:
                logger.info(f"[Phase 2] Refinement completed. No improvements found within {self.max_refinement_steps} steps.")

        # === 5. è¿”å›ç»“æœ ===
        return {
            'final_model': adaptoflux_instance,      # ä¼˜åŒ–åçš„æ¨¡å‹ï¼ˆåŸåœ°ä¿®æ”¹ï¼‰
            'final_loss': current_loss,              # æœ€ç»ˆæŸå¤±
            'final_accuracy': current_acc,           # æœ€ç»ˆå‡†ç¡®ç‡
            'steps_taken': steps_taken,              # å®é™…æ‰§è¡Œæ­¥æ•°
            'improvement_made': improvement_made,     # æ˜¯å¦æœ‰æ”¹è¿›
            'total_refinement_attempts': self._total_refinement_attempts,  # å‰å‘æ¨ç†æ¬¡æ•°
        }

    def _get_compatible_methods_for_node(
        self, 
        adaptoflux_instance, 
        node_name: str, 
    ) -> List[str]:
        """
        è·å–ä¸å›¾ä¸­æŒ‡å®šèŠ‚ç‚¹å…¼å®¹çš„å€™é€‰æ–¹æ³•åˆ—è¡¨ã€‚

        æ–°å¢å‚æ•°:
            allow_fallback_on_empty (bool): 
                å½“ç±»å‹å…¼å®¹æ–¹æ³•ä¸ºç©ºæ—¶ï¼Œæ˜¯å¦å…è®¸å›é€€åˆ°éç±»å‹å®‰å…¨çš„å…œåº•ç­–ç•¥ã€‚
                - Trueï¼ˆé»˜è®¤ï¼‰ï¼šå¯ç”¨å…œåº•ï¼Œä¿è¯è¿”å›éç©ºåˆ—è¡¨ï¼›
                - Falseï¼šè‹¥æ— ç±»å‹å…¼å®¹æ–¹æ³•ï¼Œç›´æ¥æŠ›å‡º RuntimeErrorã€‚

        å…¶ä½™å‚æ•°è¯´æ˜è§åŸæ³¨é‡Šã€‚
        """
        
        gp = adaptoflux_instance.graph_processor
        methods = adaptoflux_instance.methods
        all_method_names = list(methods.keys())

        if not all_method_names:
            return []

        # === 1. è·å–åŸå§‹æ–¹æ³• ===
        node_data = gp.graph.nodes[node_name]
        original_method_name = node_data.get("method_name")
        if original_method_name is None or original_method_name not in methods:
            error_msg = (
                f"Node '{node_name}' has no valid 'method_name'.\n"
                f"  - Current method_name: {original_method_name}\n"
                f"  - Available methods: {sorted(methods.keys())}\n"
                f"  - Node data: {node_data}"
            )
            logger.error("CRITICAL GRAPH ERROR:\n%s", error_msg)
            # æ‰“å°å®Œæ•´å †æ ˆï¼ˆä¾¿äºå®šä½æ˜¯å“ªä¸ªè°ƒç”¨é“¾å¯¼è‡´çš„ï¼‰
            logger.error("Full traceback:\n%s", traceback.format_exc())
            # æŠ›å‡ºå¼‚å¸¸ï¼Œç»ˆæ­¢ç¨‹åºï¼ˆé™¤éå¤–å±‚æœ‰ç‰¹æ®Šå¤„ç†ï¼‰
            raise RuntimeError(error_msg)

        # === 2. æå–åŸå§‹ç±»å‹ ===
        orig_info = methods[original_method_name]
        orig_input_types = orig_info.get("input_types", []) or []
        orig_output_types = orig_info.get("output_types", []) or []
        def is_type_compatible(method_name: str) -> bool:
            info = methods[method_name]
            m_input = info.get("input_types", []) or []
            m_output = info.get("output_types", []) or []
            return m_input == orig_input_types and m_output == orig_output_types

        # === 3. æ„å»ºå€™é€‰æ±  ===
        if self.candidate_pool_mode == "all":
            candidate_pool = all_method_names
        elif self.candidate_pool_mode == "self":
            candidate_pool = [original_method_name]
        else:  # "group"
            node_group = methods[original_method_name].get("group", "default")
            candidate_pool = [
                name for name, info in methods.items()
                if info.get("group", "default") == node_group
            ]

        # === 4. ç­›é€‰ç±»å‹å…¼å®¹æ–¹æ³• ===
        compatible_methods = [name for name in candidate_pool if is_type_compatible(name)]
        # print(f"Node '{node_name}' compatible methods: {compatible_methods}")

        # === 5. å¤„ç†ç©ºç»“æœ ===
        if not compatible_methods:
            log_msg = (f"Node '{node_name}' has no compatible methods.")
            logger.debug(log_msg)

            if self.fallback_mode == "error":
                raise RuntimeError(f"Strict mode: no type-compatible methods for node '{node_name}'...")

            # === æ‰§è¡Œå…œåº•å›é€€ ===
            if self.fallback_mode == "all":
                result = all_method_names
            elif self.fallback_mode == "group_first":
                node_group = methods[original_method_name].get("group", "default")
                group_methods = [name for name, info in methods.items() if info.get("group") == node_group]
                result = group_methods[:1] if group_methods else all_method_names[:1]
            elif self.fallback_mode == "self":
                result = [original_method_name]  # ğŸ‘ˆ å…³é”®ï¼šåªè¿”å›è‡ªå·±
            else:
                result = all_method_names  # fallback

            logger.warning("Falling back to non-type-safe methods for node '%s'...", node_name)
            return result

        return compatible_methods

    def _phase_modular_compression(self, adaptoflux_instance, input_data: np.ndarray, target: np.ndarray) -> Dict[str, Any]:
        """
        é˜¶æ®µå››ï¼šæ¨¡å—åŒ–å‹ç¼© (Modular Compression)
        è¯†åˆ«å›¾ä¸­å¯è¢«æ›¿æ¢çš„é«˜æ•ˆå­å›¾ï¼Œç”¨æ›´å°æˆ–æ›´å¿«çš„ç­‰æ•ˆç»“æ„è¿›è¡Œæ›¿ä»£ã€‚

        æœ¬é˜¶æ®µæ‰§è¡Œä»¥ä¸‹æµç¨‹ï¼š
        1. **å­å›¾é‡‡æ ·**ï¼šä½¿ç”¨é…ç½®çš„é‡‡æ ·å™¨ï¼ˆå¦‚ BFSï¼‰éšæœºé€‰å–ä¸€ä¸ªè¿é€šå­å›¾ï¼›
        2. **I/O æå–**ï¼šæ‰§è¡ŒåŸå›¾ï¼Œè®°å½•è¯¥å­å›¾çš„è¾“å…¥ä¸è¾“å‡ºæ•°æ®ï¼›
        3. **æ›¿ä»£ç”Ÿæˆ**ï¼šï¼ˆå½“å‰ç®€åŒ–ä¸ºï¼‰é€‰æ‹©ä¸€ä¸ªå€™é€‰æ–¹æ³•ï¼ˆå¦‚ "add"ï¼‰ä½œä¸ºæ›¿ä»£ç»“æ„ï¼›
        4. **ç­‰æ•ˆæ€§éªŒè¯**ï¼šæ¯”è¾ƒæ›¿ä»£ç»“æ„ä¸åŸå­å›¾åœ¨ç›¸åŒè¾“å…¥ä¸‹çš„è¾“å‡ºæ˜¯å¦è¶³å¤Ÿç›¸ä¼¼ï¼›
        5. **å›¾ç»“æ„æ›¿æ¢**ï¼šè‹¥éªŒè¯é€šè¿‡ï¼Œåˆ™ç”¨æ›¿ä»£ç»“æ„ï¼ˆå½“å‰ä¸ºå•èŠ‚ç‚¹ï¼‰æ›¿æ¢åŸå­å›¾ï¼›
        6. **è®°å½•é«˜æ€§èƒ½å­å›¾**ï¼šå°†è¢«æ›¿æ¢çš„åŸå­å›¾ä¿å­˜è‡³ `high_performance_subgraphs`ï¼Œä¾›è¿›åŒ–é˜¶æ®µä½¿ç”¨ã€‚

        æ³¨æ„ï¼šå½“å‰å®ç°ä¸­ï¼Œæ›¿ä»£ç»“æ„ä¸º**å•ä¸ªèŠ‚ç‚¹**ï¼Œä¸”å€™é€‰æ–¹æ³•å›ºå®šä¸º "add"ã€‚
        åç»­å¯æ‰©å±•ä¸ºè®­ç»ƒä¸€ä¸ªå°å‹æ›¿ä»£å­å›¾ä»¥å®ç°æ›´ä¼˜å‹ç¼©ã€‚

        :param adaptoflux_instance: å¾…ä¼˜åŒ–çš„ AdaptoFlux å®ä¾‹
        :param input_data: ç”¨äºè¯„ä¼°ç­‰æ•ˆæ€§çš„è¾“å…¥æ•°æ®ï¼ˆå°æ‰¹é‡ï¼‰
        :param target: å¯¹åº”çš„æ ‡ç­¾ï¼ˆç”¨äºæœ€ç»ˆæ€§èƒ½è¯„ä¼°ï¼Œéç­‰æ•ˆæ€§éªŒè¯å¿…éœ€ï¼‰
        :return: åŒ…å«å‹ç¼©åæ¨¡å‹ä¿¡æ¯å’Œå‹ç¼©æƒ…å†µçš„å­—å…¸ï¼Œå­—æ®µåŒ…æ‹¬ï¼š
                 - 'final_model': å‹ç¼©åçš„ AdaptoFlux å®ä¾‹ï¼ˆå¯èƒ½æœªå˜ï¼‰
                 - 'final_loss': å‹ç¼©åçš„æŸå¤±å€¼
                 - 'final_accuracy': å‹ç¼©åçš„å‡†ç¡®ç‡
                 - 'compression_applied': boolï¼Œæ˜¯å¦æˆåŠŸæ‰§è¡Œäº†å‹ç¼©
                 - 'compressed_subgraphs': intï¼Œæœ¬æ¬¡å‹ç¼©çš„å­å›¾æ•°é‡ï¼ˆ0 æˆ– 1ï¼‰
        """
        if not self.enable_compression:
            return self._return_original_result(adaptoflux_instance, input_data, target)

        gp = adaptoflux_instance.graph_processor

        # 1. é‡‡æ ·å­å›¾
        subgraph = self.subgraph_sampler.sample(gp.graph)
        if subgraph is None:
            return self._return_original_result(adaptoflux_instance, input_data, target)

        # 2. æå– I/O
        try:
            sub_inputs, sub_outputs = self.io_extractor.extract(adaptoflux_instance, subgraph, input_data)
        except Exception as e:
            logger.warning(f"IO extraction failed: {e}")
            return self._return_original_result(adaptoflux_instance, input_data, target)

        # 3. ç®€åŒ–ï¼šç›´æ¥é€‰ä¸€ä¸ªå€™é€‰æ–¹æ³•ï¼ˆå¦‚ "add"ï¼‰ä½œä¸ºæ›¿ä»£ï¼ˆè·³è¿‡è®­ç»ƒï¼‰
        candidate_method = "add"  # åç»­å¯æ›¿æ¢ä¸ºè®­ç»ƒé€»è¾‘
        if candidate_method not in adaptoflux_instance.methods:
            return self._return_original_result(adaptoflux_instance, input_data, target)

        # 4. éªŒè¯ç­‰æ•ˆæ€§
        # æ„å»ºä¸´æ—¶å•èŠ‚ç‚¹å›¾å¹¶æµ‹è¯•
        temp_af = self._create_single_node_graph(adaptoflux_instance, candidate_method)
        rep_output = temp_af.infer_with_graph(input_data)
        orig_output = list(sub_outputs.values())[0]

        if not self.equivalence_checker.is_equivalent(orig_output, rep_output):
            return self._return_original_result(adaptoflux_instance, input_data, target)

        # 5. æ‰§è¡Œæ›¿æ¢
        try:
            new_node_id = self.replacer.replace_with_node(gp, subgraph, candidate_method)
            logger.info(f"Replaced subgraph with node '{new_node_id}' ({candidate_method})")

            # è¿”å›æ–°ç»“æœ
            new_loss = self._evaluate_loss(input_data, target, adaptoflux_instance=adaptoflux_instance)
            new_acc = self._evaluate_accuracy(input_data, target, adaptoflux_instance=adaptoflux_instance)
            return {
                'final_model': adaptoflux_instance,
                'final_loss': new_loss,
                'final_accuracy': new_acc,
                'compression_applied': True,
                'compressed_subgraphs': 1
            }

        except Exception as e:
            logger.error(f"Replacement failed: {e}")
            return self._return_original_result(adaptoflux_instance, input_data, target)

    def _return_original_result(self, adaptoflux_instance, input_data, target):
        """
        è¾…åŠ©æ–¹æ³•ï¼šè¿”å›æœªè¿›è¡Œä»»ä½•å‹ç¼©çš„åŸå§‹æ¨¡å‹è¯„ä¼°ç»“æœã€‚

        å½“æ¨¡å—åŒ–å‹ç¼©å› ä»¥ä¸‹åŸå› è·³è¿‡æ—¶è°ƒç”¨ï¼š
        - å‹ç¼©åŠŸèƒ½è¢«ç¦ç”¨ï¼ˆenable_compression=Falseï¼‰
        - æœªèƒ½é‡‡æ ·åˆ°æœ‰æ•ˆå­å›¾
        - I/O æå–å¤±è´¥
        - å€™é€‰æ›¿ä»£æ–¹æ³•ä¸å¯ç”¨
        - ç­‰æ•ˆæ€§éªŒè¯æœªé€šè¿‡
        - å›¾æ›¿æ¢è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸

        è¯¥æ–¹æ³•ç¡®ä¿å‹ç¼©é˜¶æ®µå§‹ç»ˆè¿”å›ç»“æ„ä¸€è‡´çš„ç»“æœå­—å…¸ï¼Œä¾¿äºä¸»è®­ç»ƒå¾ªç¯ç»Ÿä¸€å¤„ç†ã€‚

        :param adaptoflux_instance: å½“å‰çš„ AdaptoFlux å®ä¾‹ï¼ˆæœªä¿®æ”¹ï¼‰
        :param input_data: ç”¨äºè¯„ä¼°çš„è¾“å…¥æ•°æ®
        :param target: å¯¹åº”çš„æ ‡ç­¾
        :return: è¡¨ç¤ºâ€œæ— å‹ç¼©â€çš„æ ‡å‡†ç»“æœå­—å…¸
        """
        loss = self._evaluate_loss(input_data, target, adaptoflux_instance=adaptoflux_instance)
        acc = self._evaluate_accuracy(input_data, target, adaptoflux_instance=adaptoflux_instance)
        return {
            'final_model': adaptoflux_instance,
            'final_loss': loss,
            'final_accuracy': acc,
            'compression_applied': False,
            'compressed_subgraphs': 0
        }

    def _extract_node_signature(self, graph, node_id: str) -> Tuple[int, Tuple[int, ...], Tuple[int, ...]]:
        """
        ä»å›¾ä¸­æå–èŠ‚ç‚¹çš„æ‹“æ‰‘ç­¾åï¼š(layer, sorted_in_coords, sorted_out_coords)
        
        :param graph: NetworkX å›¾
        :param node_id: èŠ‚ç‚¹ IDï¼Œæ ¼å¼å¦‚ "L_I_method"
        :return: (layer, in_coords_tuple, out_coords_tuple)
        """
        # 1. æå– layer
        if node_id in ("root", "collapse"):
            raise ValueError("Skip special nodes")
        try:
            layer = int(node_id.split('_', 1)[0])
        except (ValueError, IndexError):
            raise ValueError(f"Cannot parse layer from node ID: {node_id}")

        # 2. æ”¶é›†æ‰€æœ‰è¾“å…¥è¾¹çš„ data_coordï¼ˆæŒ‡å‘è¯¥èŠ‚ç‚¹çš„è¾¹ï¼‰
        in_coords = []
        for src, _, edge_data in graph.in_edges(node_id, data=True):
            coord = edge_data.get('data_coord')
            if coord is not None:
                in_coords.append(coord)
        in_coords = tuple(sorted(in_coords))

        # 3. æ”¶é›†æ‰€æœ‰è¾“å‡ºè¾¹çš„ data_coordï¼ˆä»è¯¥èŠ‚ç‚¹å‡ºå‘çš„è¾¹ï¼‰
        out_coords = []
        for _, dst, edge_data in graph.out_edges(node_id, data=True):
            coord = edge_data.get('data_coord')
            if coord is not None:
                out_coords.append(coord)
        out_coords = tuple(sorted(out_coords))

        return (layer, in_coords, out_coords)
    
    def _phase_method_pool_evolution(
        self,
        adaptoflux_instance,
        snapshots: List[Any],
        max_methods: int = 1,
        enable_graph_isomorphism_clustering: bool = True,
        evolved_methods_save_dir: Optional[str] = None,
        subgraph_selection_policy: str = "largest"
    ) -> Dict[str, Any]:
        # åç»­å¯èƒ½æ·»åŠ å‚æ•°å…è®¸é€‰æ‹©åˆå§‹åŒ–çš„å¤šä¸ªæ¨¡å‹è¿›å…¥åé¢é˜¶æ®µå•ä¸ªä»»åŠ¡æ„å»ºå¤šä¸ªå®ä¾‹ï¼Œå®ç°ä¸€ç§åœ¨å•ä»»åŠ¡ä¸Šå®ç°ç±»å¤šä»»åŠ¡çš„çŸ¥è¯†æå–ï¼Œè¿™ç§æ–¹æ³•æ•ˆæœä¼šæ›´å¥½ä½†æ˜¯æ¶ˆè€—æ€§èƒ½æ›´å¤š
        # åç»­å¯èƒ½æ·»åŠ å…¶ä»–ç§ç±»çš„é€‰å–è¿å›¾å­å›¾çš„ç­–ç•¥
        # æ‰“å°æ—¥å¿—ï¼šå¼€å§‹æ–¹æ³•æ± è¿›åŒ–é˜¶æ®µï¼Œè¯´æ˜å°†åŸºäºæ‹“æ‰‘ç­¾åå¯¹é½èŠ‚ç‚¹
        # å¯¹é½ä¾æ®ï¼š(å±‚å·, è¾“å…¥æ•°æ®åæ ‡é›†åˆ, è¾“å‡ºæ•°æ®åæ ‡é›†åˆ)
        # è¿™ç§ç­¾åèƒ½å”¯ä¸€æ ‡è¯†èŠ‚ç‚¹åœ¨æ•°æ®æµå›¾ä¸­çš„æ‹“æ‰‘è§’è‰²ï¼Œä¸èŠ‚ç‚¹IDæˆ–æ–¹æ³•åæ— å…³
        if self.verbose:
            logger.warning(f"è¯¥å‡½æ•°ï¼ˆ_phase_method_pool_evolutionï¼‰ä»åœ¨å¼€å‘ä¸­ï¼Œå¦‚å‡ºç°ä¸é¢„æœŸä¸ç¬¦è¯·è”ç³»å¼€å‘è€…")
            logger.info(f"[Phase 3] Method Pool Evolution: Aligning nodes via (layer, in_coords, out_coords) "
                        f"across {len(snapshots)} snapshots...")

        # å®‰å…¨æ£€æŸ¥ï¼šè‹¥æ— å¿«ç…§å¯ä¾›åˆ†æï¼Œç›´æ¥è¿”å›ç©ºç»“æœ
        if not snapshots:
            return {'methods_added': 0, 'new_method_names': []}

        # Step 1: æ„å»ºæ‹“æ‰‘ç­¾ååˆ°æ–¹æ³•é¢‘æ¬¡çš„æ˜ å°„
        # ç»“æ„ï¼šsignature_freq[signature][method_name] = count
        # å…¶ä¸­ signature = (layer, (in_coord1, in_coord2, ...), (out_coord1, out_coord2, ...))
        signature_freq = defaultdict(lambda: defaultdict(int))

        # éå†æ¯ä¸ªå¿«ç…§ï¼ˆå³æ¯æ¬¡ä¿å­˜çš„å›¾ç»“æ„ï¼‰
        for snap in snapshots:
            graph = snap.graph_processor.graph
            # éå†å›¾ä¸­æ¯ä¸ªèŠ‚ç‚¹
            for node_id in graph.nodes():
                try:
                    # æå–è¯¥èŠ‚ç‚¹çš„æ‹“æ‰‘ç­¾åï¼ˆåŸºäºå±‚å·å’Œè¾¹çš„ data_coordï¼‰
                    sig = self._extract_node_signature(graph, node_id)
                    # è·å–è¯¥èŠ‚ç‚¹å½“å‰ä½¿ç”¨çš„æ–¹æ³•åï¼Œè‹¥ç¼ºå¤±åˆ™æ ‡è®°ä¸º 'unknown'
                    method = graph.nodes[node_id].get('method_name', 'unknown')
                    # ç´¯åŠ è¯¥æ–¹æ³•åœ¨è¯¥æ‹“æ‰‘ä½ç½®å‡ºç°çš„æ¬¡æ•°
                    signature_freq[sig][method] += 1
                except ValueError:
                    # è·³è¿‡æ— æ³•è§£æçš„èŠ‚ç‚¹ï¼ˆå¦‚ "root"ã€"collapse" ç­‰ç‰¹æ®ŠèŠ‚ç‚¹ï¼‰
                    continue

        # Step 2: ä¸ºæ¯ä¸ªæ‹“æ‰‘ä½ç½®é€‰æ‹©é«˜é¢‘æ–¹æ³•ï¼ˆæ„å»ºå…±è¯†ï¼‰ï¼ˆå¯ä»¥æŒ‰ç…§ä¸åŒèŠ‚ç‚¹æ–¹æ³•æ„å»ºå¤šä¸ªå…±è¯†å›¾ï¼Œä½†ç›®å‰æ¥è¯´æ²¡æœ‰å¿…è¦ï¼‰
        consensus_map = {}
        for sig, method_counts in signature_freq.items():
            total = sum(method_counts.values())
            max_method, max_count = max(method_counts.items(), key=lambda x: x[1])
            if self.consensus_threshold is None or (max_count / total) >= self.consensus_threshold:
                consensus_map[sig] = max_method

        if self.verbose:
            logger.debug(f"Selected consensus methods for {len(consensus_map)} node roles.")

        # Step 3: é‡å»ºå…±è¯†å›¾ï¼ˆèŠ‚ç‚¹ä¸º signatureï¼‰
        template_graph = snapshots[0].graph_processor.graph
        consensus_graph = nx.MultiDiGraph()
        node_id_to_sig = {}

        # æ·»åŠ èŠ‚ç‚¹
        for node_id in template_graph.nodes():
            if node_id in ("root", "collapse"):
                continue
            try:
                sig = self._extract_node_signature(template_graph, node_id)
                if sig in consensus_map:
                    # âœ… è·å–åŸå§‹èŠ‚ç‚¹çš„å®Œæ•´å±æ€§
                    orig_attrs = template_graph.nodes[node_id]
                    
                    # âœ… æ„å»ºæ–°èŠ‚ç‚¹å±æ€§ï¼šè‡³å°‘åŒ…å« method_name å’Œ is_passthrough
                    new_attrs = {
                        'method_name': consensus_map[sig],
                        'is_passthrough': orig_attrs.get('is_passthrough', False),  # â† å…³é”®ä¿®å¤ï¼
                    }
                    
                    # å¯é€‰ï¼šä¿ç•™å…¶ä»–ä½ è®¤ä¸ºé‡è¦çš„å±æ€§
                    # ä¾‹å¦‚ï¼š
                    # if 'output_count' in orig_attrs:
                    #     new_attrs['output_count'] = orig_attrs['output_count']
                    
                    consensus_graph.add_node(sig, **new_attrs)
                    node_id_to_sig[node_id] = sig
            except ValueError:
                continue

        # æ·»åŠ è¾¹
        for src_id, dst_id, edge_data in template_graph.edges(data=True):
            if src_id in node_id_to_sig and dst_id in node_id_to_sig:
                src_sig = node_id_to_sig[src_id]
                dst_sig = node_id_to_sig[dst_id]
                consensus_graph.add_edge(src_sig, dst_sig, **edge_data)

        # Step 4: åˆ†å‰²ä¸ºè¿é€šåˆ†é‡
        connected_components = list(nx.weakly_connected_components(consensus_graph))

        comp_sizes = [len(comp) for comp in connected_components]
        if self.verbose:
            logger.debug(
                f"[Connectivity Analysis] Found {len(connected_components)} weakly connected components. "
                f"è¿é€šåˆ†é‡æ•°é‡ï¼š{len(connected_components)}ï¼Œå„åˆ†é‡å¤§å°ï¼š{comp_sizes}"
            )

        # è¿‡æ»¤ï¼šåªä¿ç•™èŠ‚ç‚¹æ•° >= é˜ˆå€¼çš„è¿é€šå­å›¾
        connected_components = [
            comp for comp in connected_components 
            if len(comp) >= self.min_subgraph_size_for_evolution
        ]
        
        num_components = len(connected_components)

        if self.verbose:
            filtered_sizes = [len(comp) for comp in connected_components]
            logger.info(
                f"[Component Filtering] Kept {num_components} components (min size â‰¥ {self.min_subgraph_size_for_evolution}). "
                f"ä¿ç•™è¿é€šåˆ†é‡æ•°é‡ï¼š{num_components}ï¼ˆæœ€å°å°ºå¯¸ â‰¥ {self.min_subgraph_size_for_evolution}ï¼‰ï¼Œå°ºå¯¸åˆ—è¡¨ï¼š{filtered_sizes}"
            )


        if self.verbose:
            logger.debug(f"Consensus graph split into {num_components} connected components.")

        if num_components == 0:
            return {'methods_added': 0, 'new_method_names': []}

        # Step 5: å†³å®šæ˜¯å¦è¿›è¡Œå›¾åŒæ„èšç±»
        if enable_graph_isomorphism_clustering:
            # ===== å¯ç”¨å›¾åŒ¹é…èšç±» =====
            from networkx.algorithms.isomorphism import DiGraphMatcher

            unique_graphs = []      # å­˜å‚¨å”¯ä¸€å­å›¾
            graph_counts = []       # å¯¹åº”å‡ºç°æ¬¡æ•°

            for comp in connected_components:
                g = consensus_graph.subgraph(comp).copy()
                matched = False
                for i, rep_g in enumerate(unique_graphs):
                    nm = lambda n1, n2: n1.get('method_name') == n2.get('method_name')
                    em = None  # æˆ–å¯ç”¨è¾¹å±æ€§åŒ¹é…
                    if DiGraphMatcher(rep_g, g, node_match=nm, edge_match=em).is_isomorphic():
                        graph_counts[i] += 1
                        matched = True
                        break
                if not matched:
                    unique_graphs.append(g)
                    graph_counts.append(1)

            # æŒ‰é¢‘æ¬¡æ’åºï¼Œå– top-k
            # ===== æ ¹æ®ç­–ç•¥é€‰æ‹©æ’åºé”® =====
            if subgraph_selection_policy == "most_frequent":
                # ä»…æŒ‰é¢‘æ¬¡ï¼ˆåŸå§‹è¡Œä¸ºï¼‰
                sort_key = lambda x: (x[1],)
            elif subgraph_selection_policy == "largest":
                # é¢‘æ¬¡ä¼˜å…ˆï¼ŒèŠ‚ç‚¹æ•°æ¬¡ä¹‹ï¼ˆé»˜è®¤æ¨èï¼‰
                sort_key = lambda x: (x[1], x[0].number_of_nodes(), x[0].number_of_edges())
            elif subgraph_selection_policy == "smallest":
                # é¢‘æ¬¡ä¼˜å…ˆï¼Œä½†èŠ‚ç‚¹æ•°è¶Šå°è¶Šå¥½
                sort_key = lambda x: (x[1], -x[0].number_of_nodes(), -x[0].number_of_edges())
            elif subgraph_selection_policy == "balanced":
                # é¢‘æ¬¡ä¸ºä¸»ï¼ŒèŠ‚ç‚¹æ•°ä¸ºè¾…ï¼ˆåŒ largestï¼Œä½†æ˜¾å¼å‘½åï¼‰
                sort_key = lambda x: (x[1], x[0].number_of_nodes(), x[0].number_of_edges())
            else:
                raise ValueError(f"Unknown subgraph_selection_policy: {subgraph_selection_policy}")

            # æ‰§è¡Œæ’åºï¼ˆé¢‘æ¬¡é«˜çš„åœ¨å‰ï¼ŒåŒé¢‘æ¬¡æŒ‰ç­–ç•¥æ’ï¼‰
            sorted_pairs = sorted(zip(unique_graphs, graph_counts), key=sort_key, reverse=True)

            top_k = sorted_pairs[:max_methods]
            selected_graphs = [g for g, _ in top_k]
            selected_counts = [c for _, c in top_k]

            if self.verbose:
                logger.debug(
                    f"Graph isomorphism clustering: {len(sorted_pairs)} unique structures; "
                    f"selected top-{len(selected_graphs)}."
                )
                logger.info(
                    f"[Graph Clustering] Identified {len(unique_graphs)} unique graph structures via isomorphism. "
                    f"å›¾åŒæ„èšç±»ç»“æœï¼šå…±å‘ç° {len(unique_graphs)} ç§å”¯ä¸€å­å›¾ç»“æ„ã€‚"
                )
                for idx, (g, cnt) in enumerate(sorted_pairs):
                    logger.debug(
                        f"  Structure {idx+1}: {g.number_of_nodes()} nodes, {g.number_of_edges()} edges, frequency = {cnt}. "
                        f"  ç»“æ„ {idx+1}ï¼š{g.number_of_nodes()} ä¸ªèŠ‚ç‚¹ï¼Œ{g.number_of_edges()} æ¡è¾¹ï¼Œå‡ºç°é¢‘æ¬¡ = {cnt}"
                    )
                logger.info(
                    f"[Subgraph Selection] Using policy '{subgraph_selection_policy}'. "
                    f"å­å›¾é€‰æ‹©ç­–ç•¥ï¼š'{subgraph_selection_policy}'"
                )
                logger.info(
                    f"[Top-K Selection] Selected top-{len(selected_graphs)} most frequent structures for evolution. "
                    f"æœ€ç»ˆé€‰å–ï¼šé¢‘æ¬¡æœ€é«˜çš„å‰ {len(selected_graphs)} ä¸ªç»“æ„ç”¨äºæ–¹æ³•è¿›åŒ–ã€‚"
                )

        else:
            # ===== ç¦ç”¨èšç±»ï¼šæ¯ä¸ªè¿é€šåˆ†é‡ç‹¬ç«‹å¤„ç† =====
            selected_graphs = [consensus_graph.subgraph(comp).copy() for comp in connected_components]
            selected_counts = [1] * len(selected_graphs)  # æ— é¢‘æ¬¡æ„ä¹‰ï¼Œç»Ÿä¸€ä¸º1
            # ä½†ä»å— max_methods é™åˆ¶
            if len(selected_graphs) > max_methods:
                selected_graphs = selected_graphs[:max_methods]
                selected_counts = selected_counts[:max_methods]
                if self.verbose:
                    logger.info(
                        f"[No Clustering] Clustering disabled; using first {max_methods} components out of {len(connected_components)}. "
                        f"æœªå¯ç”¨èšç±»ï¼šä» {len(connected_components)} ä¸ªåˆ†é‡ä¸­ç›´æ¥é€‰å–å‰ {max_methods} ä¸ªã€‚"
                    )

        # Step 6: å¯¹ selected_graphs è¿˜åŸï¼ˆæ·»åŠ  root/collapse + é‡å‘½åï¼‰ï¼ˆæœªå®Œæˆï¼‰
        reconstructed_graphs = []

        # æ„å»ºåå‘æ˜ å°„ï¼šsignature -> åŸå§‹ node_idï¼ˆä»»é€‰ä¸€ä¸ªå³å¯ï¼Œå› ç­¾åç›¸åŒï¼‰
        sig_to_original_node = {}
        for node_id, sig in node_id_to_sig.items():
            if sig not in sig_to_original_node:
                sig_to_original_node[sig] = node_id

        for g in selected_graphs:
            g_full = g.copy()
            g_full.add_node("root")
            g_full.add_node("collapse")
            
            internal_nodes = [n for n in g_full.nodes() if n not in ("root", "collapse")]
            entry_nodes = [n for n in internal_nodes if g_full.in_degree(n) == 0]
            exit_nodes = [n for n in internal_nodes if g_full.out_degree(n) == 0]
            
            # === è¿æ¥ root åˆ° entry_nodesï¼Œä½¿ç”¨åŸå§‹è¾¹å±æ€§ ===
            edge_counter = 0

            for sig in entry_nodes:
                orig_node = sig_to_original_node.get(sig)
                
                if orig_node is None or not list(template_graph.in_edges(orig_node, data=True)):
                    logger.warning("ç†è®ºä¸Šä¸è¯¥è¿›å…¥è¯¥åˆ†æ”¯ï¼Œè¯·è”ç³»å¼€å‘è€…ã€‚")
                    g_full.add_edge("root", sig,
                                    data_type='scalar',
                                    output_index=edge_counter,
                                    data_coord=edge_counter)
                    edge_counter += 1
                    continue

                # æ„å»ºå¹¶æ’åºå…¥è¾¹åˆ—è¡¨
                edge_info_list = []
                for u, v, attrs in template_graph.in_edges(orig_node, data=True):
                    if 'data_coord' not in attrs or 'data_type' not in attrs:
                        raise ValueError(f"Edge {u}->{v} missing required attrs: {attrs}")
                    layer_u = self.get_node_layer(u)
                    edge_info_list.append((layer_u, attrs['data_coord'], u, v, attrs))
                
                edge_info_list.sort(key=lambda x: (x[0], x[1]))

                # ä¸ºæ¯æ¡åŸå§‹å…¥è¾¹åˆ›å»ºä¸€æ¡ root -> sig çš„è¾¹
                for (_, _, u, v, orig_attrs) in edge_info_list:
                    g_full.add_edge("root", sig,
                                    data_type=orig_attrs['data_type'],
                                    output_index=edge_counter,
                                    data_coord=edge_counter)
                    edge_counter += 1
            
            # === è¿æ¥ exit_nodes åˆ° collapseï¼Œä½¿ç”¨åŸå§‹è¾¹å±æ€§å¹¶é‡æ–°æ’åº ===
            # === å…¨å±€æ”¶é›†æ‰€æœ‰ exit -> collapse çš„å€™é€‰è¾¹ ===
            all_exit_edges = []  # (sig, orig_output_index, data_type, layer_u, orig_data_coord)

            for sig in exit_nodes:
                orig_node = sig_to_original_node.get(sig)
                if orig_node is None:
                    logger.warning("ç†è®ºä¸Šä¸è¯¥è¿›å…¥è¯¥åˆ†æ”¯ï¼Œè¯·è”ç³»å¼€å‘è€…ã€‚")
                    raise ValueError(f"No original node found for exit signature: {sig}")

                out_edges = list(template_graph.out_edges(orig_node, data=True))
                if not out_edges:
                    logger.warning("ç†è®ºä¸Šä¸è¯¥è¿›å…¥è¯¥åˆ†æ”¯ï¼Œè¯·è”ç³»å¼€å‘è€…ã€‚")
                    # æ·»åŠ é»˜è®¤è¾“å‡ºï¼ˆæ³¨æ„ï¼šè¿™ä¹Ÿåº”çº³å…¥å…¨å±€ç¼–å·ï¼ï¼‰
                    all_exit_edges.append((sig, 0, 'scalar', self.get_node_layer(orig_node), 0))
                    continue

                for u, v, attrs in out_edges:
                    if not {'data_type', 'data_coord', 'output_index'} <= attrs.keys():
                        raise ValueError(f"Edge {u}->{v} missing required attrs: {attrs}")
                    layer_u = self.get_node_layer(u)
                    all_exit_edges.append((
                        sig,
                        attrs['output_index'],
                        attrs['data_type'],
                        layer_u,
                        attrs['data_coord']
                    ))

            # === å…¨å±€æ’åºï¼šé«˜å±‚çº§ä¼˜å…ˆï¼ŒåŒå±‚æŒ‰åŸ data_coord å‡åº ===
            all_exit_edges.sort(key=lambda x: (-x[3], x[4]))  # x[3]=layer_u, x[4]=orig_data_coord

            # === å…¨å±€åˆ†é… data_coordï¼ˆ0,1,2,...ï¼‰å¹¶æ·»åŠ è¾¹ ===
            for data_coord, (sig, orig_output_index, data_type, _, _) in enumerate(all_exit_edges):
                g_full.add_edge(sig, "collapse",
                                output_index=orig_output_index,  # âœ… ä¿ç•™åŸå§‹è¾“å‡ºç´¢å¼•
                                data_coord=data_coord,           # âœ… å…¨å±€å”¯ä¸€
                                data_type=data_type)
            
            # === é‡å‘½åèŠ‚ç‚¹ï¼šä½¿ç”¨ä¸ append_nx_layer ä¸€è‡´çš„å‘½åæ ¼å¼ ===
            if self.verbose:
                logger.info("Renaming nodes...")
                logger.warning("å›¾çš„é‡å‘½åç³»ç»Ÿæ ¹æ®èŠ‚ç‚¹åˆ°rootçš„è·¯å¾„å±‚æ•°è¿›è¡Œå‘½åï¼Œç†è®ºä¸Šä¸è¯¥å‡ºç°é—®é¢˜ï¼Œå¦‚å‡ºç°é—®é¢˜ï¼Œè¯·è”ç³»å¼€å‘è€…")

            # åœ¨ g_full ä¸­ï¼ˆå·²æ·»åŠ  rootï¼‰ï¼Œè®¡ç®—æ¯ä¸ªèŠ‚ç‚¹åˆ° root çš„æœ€çŸ­è·¯å¾„é•¿åº¦
            # æ³¨æ„ï¼šg_full æ˜¯ DiGraphï¼Œè¾¹æ–¹å‘ root â†’ node
            try:
                lengths = nx.single_source_shortest_path_length(g_full, "root")
            except nx.NetworkXError:
                # å¦‚æœ root ä¸è¿é€šï¼ˆç†è®ºä¸Šä¸åº”å‘ç”Ÿï¼‰
                lengths = {node: 0 for node in g_full.nodes()}
                lengths["root"] = 0

            mapping = {}

            # æŒ‰æ–¹æ³•ååˆ†ç»„ï¼Œä¸ºæ¯ä¸ªæ–¹æ³•åˆ†é…å±€éƒ¨ç´¢å¼•
            layer_method_counter = defaultdict(lambda: defaultdict(int))

            for node in internal_nodes:
                method = g_full.nodes[node].get('method_name', 'unknown')
                local_layer = lengths.get(node, 1)
                
                # æŒ‰ (layer, method) è®¡æ•°
                local_index = layer_method_counter[local_layer][method]
                layer_method_counter[local_layer][method] += 1
                
                new_name = f"{local_layer}_{local_index}_{method}"
                mapping[node] = new_name

            g_full = nx.relabel_nodes(g_full, mapping)

            # === ä¿®æ­£ï¼šæŒ‰è¾¹èµ·ç‚¹æ‰€åœ¨å±‚é‡æ–°åˆ†é… data_coordï¼Œç¡®ä¿æ¯å±‚å†…å”¯ä¸€ä¸”ä»0å¼€å§‹ ===
            # æå–æ‰€æœ‰é root/collapse çš„è¾¹ï¼ˆåŒ…æ‹¬å†…éƒ¨è¾¹å’Œåˆ° collapse çš„è¾¹ï¼‰
            edges_to_update = []
            layer_edges = defaultdict(list)  # layer -> list of (src, dst, edge_attrs)

            for src, dst, key, attrs in list(g_full.edges(keys=True, data=True)):
                # è·³è¿‡ root çš„å‡ºè¾¹ï¼ˆå®ƒä»¬çš„ data_coord å·²åœ¨å‰é¢æ­£ç¡®è®¾ç½®ä¸º input_slotï¼‰
                if src == "root":
                    continue
                # è·å–èµ·ç‚¹çš„ layerï¼ˆä»èŠ‚ç‚¹åè§£æï¼‰
                if src == "collapse":
                    continue  # collapse ä¸åº”æœ‰å‡ºè¾¹
                try:
                    layer = int(src.split('_')[0])
                except (ValueError, IndexError):
                    logger.warning(f"Cannot parse layer from node name: {src}. Skipping edge {src}->{dst}.")
                    continue
                layer_edges[layer].append((src, dst, key, attrs))

            # å¯¹æ¯ä¸€å±‚çš„è¾¹é‡æ–°åˆ†é… data_coord
            for layer, edge_list in layer_edges.items():
                # å¯é€‰ï¼šæŒ‰ç›®æ ‡èŠ‚ç‚¹åæ’åºä»¥ä¿è¯ç¡®å®šæ€§
                edge_list.sort(key=lambda x: x[1])  # æŒ‰ dst èŠ‚ç‚¹åæ’åº
                for new_coord, (src, dst, key, attrs) in enumerate(edge_list):
                    # æ›´æ–°è¾¹çš„ data_coord
                    g_full.edges[src, dst, key]['data_coord'] = new_coord

            internal_nodes = [n for n in g_full.nodes() if n not in ("root", "collapse")]

            if self.verbose:
                logger.debug(
                    f"[Node Renaming] Renamed {len(internal_nodes)} internal nodes using layer-method-index scheme. "
                    f"èŠ‚ç‚¹é‡å‘½åï¼šå·²æŒ‰ (å±‚_å±€éƒ¨ç´¢å¼•_æ–¹æ³•å) æ ¼å¼é‡å‘½å {len(internal_nodes)} ä¸ªå†…éƒ¨èŠ‚ç‚¹ã€‚"
                )

            reconstructed_graphs.append(g_full)


        # Step 7: ç”Ÿæˆæ–°æ–¹æ³•å¹¶å¯é€‰ä¿å­˜å­å›¾ï¼ˆæœªå®Œæˆï¼‰
        new_method_names = []

        # ç¡®å®šä¿å­˜ç›®å½•
        save_dir = evolved_methods_save_dir
        if save_dir is not None:
            os.makedirs(save_dir, exist_ok=True)

        for i, (g_orig, g_full, count) in enumerate(zip(selected_graphs, reconstructed_graphs, selected_counts)):
            name = f"evolved_method_{len(adaptoflux_instance.methods) + i + 1}"

            # æ¨æ–­è¾“å…¥/è¾“å‡ºç±»å‹ï¼ˆä» root/collapse è¾¹ï¼‰
            input_types = [edge['data_type'] for _, _, edge in g_full.out_edges("root", data=True)]
            output_types = [edge['data_type'] for _, _, edge in g_full.in_edges("collapse", data=True)]

            meta = {
                'name': name,
                'occurrence_count': count,
                'subgraph_node_count': g_full.number_of_nodes(),
                'is_clustered': enable_graph_isomorphism_clustering,
                'selection_policy': subgraph_selection_policy,
                'evolved_at': datetime.now().isoformat(),
                'input_count': len(input_types),
                'input_types': input_types,
                'output_types': output_types,
                'output_count': len(output_types),
                'group': 'evolved',
                'weight': 1.0,
                'vectorized': True,
                'is_evolved': True
            }

            # åˆ›å»ºå¯è°ƒç”¨å¯¹è±¡
            evolved_method = EvolvedMethod(
                name=name,
                graph=g_full,
                methods_ref=adaptoflux_instance.methods,
                metadata=meta
            )

            # æ³¨å†Œåˆ°ä¸»ç±» methodsï¼ˆå…³é”®ï¼ï¼‰
            adaptoflux_instance.methods[name] = {
                'function': evolved_method,           # â† å¯è°ƒç”¨å¯¹è±¡
                'input_count': meta['input_count'],
                'output_count': meta['output_count'],
                'input_types': meta['input_types'],
                'output_types': meta['output_types'],
                'group': meta['group'],
                'weight': meta['weight'],
                'vectorized': meta['vectorized'],
                'is_evolved': True
            }

            # 6. ã€å…³é”®ã€‘é‡å»º graph_processor ä»¥è¯†åˆ«æ–°æ–¹æ³•ï¼Œå¹¶ä¿ç•™å½“å‰ layer çŠ¶æ€
            adaptoflux_instance.path_generator.methods = adaptoflux_instance.methods # ç¡®ä¿ methods å·²æ›´æ–°
            current_layer = adaptoflux_instance.graph_processor.layer
            adaptoflux_instance.graph_processor = adaptoflux_instance._create_graph_processor(
                graph=adaptoflux_instance.graph
            )
            adaptoflux_instance.graph_processor.layer = current_layer

            new_method_names.append(name)

            # ä¿å­˜åˆ°ç£ç›˜
            if save_dir is not None:
                evolved_method.save(save_dir)
                if self.verbose:
                    logger.info(f"[Saved] Evolved method {name} to {save_dir}")

        return {
            'methods_added': len(new_method_names),
            'new_method_names': new_method_names
        }

    def train(
        self,
        input_data: np.ndarray,
        target: np.ndarray,
        max_evo_cycles: int = 5,
        enable_early_stop: bool = True,      # â† æ–°å¢å¼€å…³
        early_stop_eps: float = 1e-6,        # â† å»ºè®®æ”¹åé¿å…å’Œæ•°å€¼è®¡ç®—åº“çš„ eps å†²çª
        save_model: bool = True,
        model_save_path: Optional[str] = None,
        save_best_model: bool = True,
        best_model_subfolder: str = "best",
        final_model_subfolder: str = "final",
        subgraph_selection_policy: str = "largest",
        skip_initialization: bool = False,
        max_total_refinement_attempts: Optional[int] = None,  # <-- æ–°å¢å‚æ•°
        **kwargs
    ) -> dict:
        """
        å®ç°åŸºç±»çš„ train æ–¹æ³•ã€‚
        æ‰§è¡Œå®Œæ•´çš„â€œå›¾æ¼”â€ä¼˜åŒ–å¾ªç¯ã€‚

        :param input_data: ç”¨äºå¿«é€Ÿè¯„ä¼°çš„è¾“å…¥æ•°æ®ï¼ˆå°æ‰¹é‡ï¼‰
        :param target: å¯¹åº”çš„æ ‡ç­¾
        :param max_evo_cycles: æœ€å¤šæ‰§è¡Œçš„è®­ç»ƒè½®æ¬¡æ•°ã€‚
        :param save_model: æ˜¯å¦åœ¨è®­ç»ƒç»“æŸåä¿å­˜æ¨¡å‹
        :param model_save_path: æ¨¡å‹ä¿å­˜çš„æ–‡ä»¶å¤¹è·¯å¾„
        :param save_best_model: æ˜¯å¦ä¿å­˜è¿‡ç¨‹ä¸­é‡åˆ°çš„æœ€ä½³æ¨¡å‹
        :param best_model_subfolder: æœ€ä½³æ¨¡å‹ä¿å­˜çš„å­ç›®å½•
        :param final_model_subfolder: æœ€ç»ˆæ¨¡å‹ä¿å­˜çš„å­ç›®å½•
        :param skip_initialization: è‹¥ä¸º Trueï¼Œåˆ™è·³è¿‡å¤šæ ·åˆå§‹åŒ–é˜¶æ®µï¼Œç›´æ¥ä½¿ç”¨å½“å‰ adaptoflux å®ä¾‹çš„å›¾ç»“æ„ä½œä¸ºä¼˜åŒ–èµ·ç‚¹ã€‚
                            é€‚ç”¨äº CombinedTrainer æˆ–äººå·¥é¢„è®¾æ¨¡å‹åœºæ™¯ã€‚
        :param kwargs: å…¶ä»–å¯é€‰å‚æ•°
        :return: ä¸€ä¸ªåŒ…å«è®­ç»ƒè¿‡ç¨‹ä¿¡æ¯çš„å­—å…¸
        """

        self.max_total_refinement_attempts = max_total_refinement_attempts
        self._total_refinement_attempts = 0  # <-- æ–°å¢è®¡æ•°å™¨

        # åé¢å¯èƒ½ç¼–å†™å•ä»»åŠ¡ä½¿ç”¨å¤šä¸ªå®ä¾‹çš„çŸ¥è¯†æå–ï¼ˆæ¶ˆè€—æ€§èƒ½æ›´é«˜ï¼Œä½†æå–å‡ºæ¥çš„çŸ¥è¯†åº”è¯¥æ•ˆæœæ›´å¥½ï¼‰ï¼Œä»¥åŠå¤šä»»åŠ¡é€‚é…
        # åé¢æ·»åŠ å¯é€‰å‚æ•°ï¼Œæ§åˆ¶è¯¥è½®è®­ç»ƒå¾—åˆ°çš„æ–°çŸ¥è¯†æ˜¯å¦ç›´æ¥åŠ å…¥æ–¹æ³•æ± ï¼Œæˆ–ä¿å­˜ä¸ºå›¾ä½¿ç”¨ã€‚

        if self.verbose:
            init_msg = "Skipping diverse initialization" if skip_initialization else "Starting diverse initialization"
            logger.info(f"Starting GraphEvoTrainer. {init_msg}. Max evolution cycles: {max_evo_cycles}")

        results = {
            "evo_cycles_completed": 0,
            "phase_results": [],
            "total_refinement_steps": 0,
            "total_compressions": 0,
            "total_methods_evolved": 0,
            "best_accuracy": -1.0,
            "best_accuracy_cycle": -1,
            "best_model_snapshot": None,
            "best_model_cycle": -1
        }

        # === é˜¶æ®µä¸€ï¼šå¤šæ ·åˆå§‹åŒ–ï¼ˆå¯é€‰ï¼‰===
        if not skip_initialization:
            init_result = self._phase_diverse_initialization(input_data, target)
            current_af = init_result['best_model']
            current_loss = init_result['best_loss']
            current_acc = init_result['best_accuracy']

            # æ›´æ–°å…¨å±€çŠ¶æ€ï¼ˆå³ä½¿è·³è¿‡ï¼Œåç»­ä¹Ÿä¼šç”¨ current_af è¦†ç›–ï¼‰
            self.adaptoflux = current_af

            # è®°å½•åˆå§‹åŒ–ç»“æœ
            if current_acc > results['best_accuracy']:
                results['best_accuracy'] = current_acc
                results['best_accuracy_cycle'] = 0
                results['best_model_snapshot'] = copy.deepcopy(current_af)
                results['best_model_cycle'] = 0

            results['phase_results'].append({
                'cycle': 0,
                'phase': 'initialization',
                'result': init_result
            })
        else:
            # ç›´æ¥ä½¿ç”¨å½“å‰å®ä¾‹çš„æ¨¡å‹
            current_af = copy.deepcopy(self.adaptoflux)
            current_loss = self._evaluate_loss(input_data, target)
            current_acc = self._evaluate_accuracy(input_data, target)

            # åˆå§‹åŒ–å³ä¸ºå½“å‰çŠ¶æ€
            if current_acc > results['best_accuracy']:
                results['best_accuracy'] = current_acc
                results['best_accuracy_cycle'] = 0
                results['best_model_snapshot'] = copy.deepcopy(current_af)
                results['best_model_cycle'] = 0

            results['phase_results'].append({
                'cycle': 0,
                'phase': 'initialization_skipped',
                'result': {
                    'loss': current_loss,
                    'accuracy': current_acc,
                    'model_used': 'current_adaptoflux'
                }
            })

        # å¼€å§‹è¿›åŒ–å¾ªç¯
        for cycle in range(1, max_evo_cycles + 1):
            if self.verbose:
                logger.info(f"--- Starting Evolution Cycle {cycle}/{max_evo_cycles} ---")

            cycle_results = {'cycle': cycle}

            # é˜¶æ®µäºŒï¼šé€èŠ‚ç‚¹ç²¾ç‚¼
            refinement_result = self._phase_node_refinement(current_af, input_data, target)
            current_af = refinement_result['final_model']
            current_loss = refinement_result['final_loss']
            current_acc = refinement_result['final_accuracy']

            # ã€âœ… æ–°å¢æ—©åœé€»è¾‘ã€‘
            if enable_early_stop and current_acc >= 1.0 - early_stop_eps:
                if self.verbose:
                    logger.info(f"ğŸ¯ Early stopping triggered at cycle {cycle}: accuracy={current_acc:.6f} >= {1.0 - eps}")
                results['evo_cycles_completed'] = cycle
                break  # ç«‹å³ç»ˆæ­¢åç»­è¿›åŒ–è½®æ¬¡

            results['total_refinement_steps'] += refinement_result['steps_taken']

            cycle_results['refinement'] = refinement_result

            # é˜¶æ®µä¸‰ï¼šæ–¹æ³•æ± è¿›åŒ–ï¼ˆåŸºäºå¿«ç…§è§¦å‘ï¼‰
            if self.enable_evolution:
                # 1. æŒ‰é¢‘ç‡è®°å½•å›¾å¿«ç…§
                if cycle % self.evolution_sampling_frequency == 0:
                    snapshot = copy.deepcopy(current_af)
                    self.graph_snapshots.append(snapshot)
                    if self.verbose:
                        logger.debug(f"Saved graph snapshot #{len(self.graph_snapshots)} at cycle {cycle}")

                # 2. æ£€æŸ¥æ˜¯å¦è§¦å‘è¿›åŒ–
                if len(self.graph_snapshots) >= self.evolution_trigger_count:

                    evolved_dir = os.path.join(model_save_path, "evolved_methods")

                    # 3. æ‰§è¡Œè¿›åŒ–
                    evolution_result = self._phase_method_pool_evolution(
                        current_af,
                        snapshots=self.graph_snapshots,
                        max_methods=self.methods_per_evolution,
                        evolved_methods_save_dir=evolved_dir,
                        subgraph_selection_policy=subgraph_selection_policy
                    )
                    results['total_methods_evolved'] += evolution_result['methods_added']
                    cycle_results['evolution'] = evolution_result

                    # 4. æ¸…ç†å¿«ç…§
                    if self.evolution_cleanup_mode == "full":
                        self.graph_snapshots.clear()
                    elif self.evolution_cleanup_mode == "oldest":
                        if self.graph_snapshots:
                            self.graph_snapshots.pop(0)
                else:
                    cycle_results['evolution'] = {
                        'skipped': True,
                        'reason': 'insufficient_snapshots'
                    }
            else:
                cycle_results['evolution'] = {
                    'skipped': True,
                    'reason': 'disabled'
                }

            # æ›´æ–°å…¨å±€çŠ¶æ€
            self.adaptoflux = current_af

            # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°çš„æœ€ä½³æ¨¡å‹
            if current_acc > results['best_accuracy']:
                results['best_accuracy'] = current_acc
                results['best_accuracy_cycle'] = cycle
                results['best_model_snapshot'] = copy.deepcopy(current_af)
                results['best_model_cycle'] = cycle

            results['evo_cycles_completed'] += 1
            results['phase_results'].append(cycle_results)

            if self.verbose:
                logger.info(f"--- Cycle {cycle} completed. Current Acc: {current_acc:.4f}, Best Acc: {results['best_accuracy']:.4f} ---")

        if self.verbose:
            logger.info(f"GraphEvoTrainer finished after {results['evo_cycles_completed']} cycles.")


        # === é˜¶æ®µå››ï¼ˆåå¤„ç†ï¼‰ï¼šæ¨¡å—åŒ–å‹ç¼©ï¼ˆä»…æ‰§è¡Œä¸€æ¬¡ï¼‰===
        final_compression_result = None
        if self.enable_compression:
            if self.verbose:
                logger.info("[Post-Training] Applying Modular Compression once on final model...")
                logger.warning(f"è¯¥å‡½æ•°ï¼ˆ_phase_modular_compressionï¼‰ä»åœ¨å¼€å‘ä¸­ï¼Œå¤§æ¦‚ç‡æ— æ³•è¾¾åˆ°é¢„æœŸæ•ˆæœï¼Œä¸å»ºè®®å¼€å¯ï¼Œå¦‚éœ€ä½¿ç”¨å»ºè®®åœ¨ATF.ModelTrainer.GraphEvoTrainerä¸­ä¿®æ”¹æˆ–ä½¿ç”¨è§„åˆ™åŒ–æ–¹æ³•æ›¿ä»£")
            final_compression_result = self._phase_modular_compression(self.adaptoflux, input_data, target)
            # æ›´æ–°æœ€ç»ˆæ¨¡å‹
            self.adaptoflux = final_compression_result['final_model']
            results['total_compressions'] = final_compression_result['compressed_subgraphs']
        else:
            if self.verbose:
                logger.info("[Post-Training] Modular Compression: SKIPPED (disabled by enable_compression=False)")
            results['total_compressions'] = 0
        
        if final_compression_result:
            results['final_compression_applied'] = final_compression_result['compression_applied']
        else:
            results['final_compression_applied'] = False

        # ä¿å­˜æ¨¡å‹
        if save_model:
            try:
                base_save_path = model_save_path or "models"
                os.makedirs(base_save_path, exist_ok=True)

                # ä¿å­˜æœ€ç»ˆæ¨¡å‹
                final_path = os.path.join(base_save_path, final_model_subfolder)
                self.adaptoflux.save_model(folder=final_path)
                if self.verbose:
                    logger.info(f"Final model saved to '{final_path}'")
                results["final_model_saved"] = final_path

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if save_best_model and results['best_model_snapshot'] is not None:
                    best_path = os.path.join(base_save_path, best_model_subfolder)

                    # ä¸´æ—¶åˆ‡æ¢
                    original_af = self.adaptoflux
                    self.adaptoflux = results['best_model_snapshot']
                    try:
                        self.adaptoflux.save_model(folder=best_path)
                        if self.verbose:
                            logger.info(f"Best model (Cycle {results['best_model_cycle']}, Acc={results['best_accuracy']:.4f}) saved to '{best_path}'")
                    finally:
                        self.adaptoflux = original_af

                    results["best_model_saved"] = best_path

                # ä¿å­˜è®­ç»ƒæ—¥å¿—
                log_filename = kwargs.get("log_filename", "graph_evo_training_log.json")
                log_path = os.path.join(base_save_path, log_filename)
                with open(log_path, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=4, default=str)
                results["training_log_saved"] = log_path

            except Exception as e:
                logger.error(f"Failed to save model(s): {e}")
                import traceback
                logger.error(traceback.format_exc())
                
        # è®°å½•æ€»ç²¾ç‚¼å°è¯•æ¬¡æ•°ï¼ˆæ— è®ºæ˜¯å¦æˆåŠŸï¼‰
        results['total_refinement_attempts'] = self._total_refinement_attempts

        return results
    
    def _refine_random_single_step(
        self,
        adaptoflux_instance,
        input_data: np.ndarray,
        target: np.ndarray,
        processing_nodes: List[str],
        current_loss: float,
        gp: Any  # GraphProcessor å®ä¾‹
    ) -> Tuple[bool, float, float, int, List[str]]:
        """
        ã€ç­–ç•¥ï¼šéšæœºå•ç‚¹ä¼˜åŒ–ã€‘
        åœ¨å½“å‰å›¾ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªå¯ä¼˜åŒ–çš„å¤„ç†èŠ‚ç‚¹ï¼Œå°è¯•å°†å…¶æ–¹æ³•æ›¿æ¢ä¸ºæ‰€æœ‰å…¼å®¹æ–¹æ³•ä¸­çš„æœ€ä¼˜è€…ï¼ˆåŸºäºæŸå¤±ä¸‹é™ï¼‰ã€‚
        ä»…æ‰§è¡Œä¸€æ¬¡æ›¿æ¢å°è¯•ï¼ˆå³ä¸€ä¸ªèŠ‚ç‚¹çš„ä¸€æ¬¡ä¼˜åŒ–ï¼‰ï¼Œé€‚ç”¨äºè½»é‡çº§ã€ä½å¼€é”€çš„å±€éƒ¨æœç´¢ã€‚
        æ³¨æ„ï¼šæœ¬å‡½æ•°ä¼š in-place ä¿®æ”¹ adaptoflux_instance çš„å›¾ç»“æ„ï¼

        è¿”å›å€¼è¯´æ˜ï¼š
        - bool: æœ¬è½®æ˜¯å¦æˆåŠŸæ”¹è¿›ï¼ˆå³æ‰¾åˆ°æ›´ä¼˜æ–¹æ³•ï¼‰
        - float: æ”¹è¿›åçš„æŸå¤±å€¼ï¼ˆè‹¥æœªæ”¹è¿›åˆ™è¿”å›åŸæŸå¤±ï¼‰
        - float: æ”¹è¿›åçš„å‡†ç¡®ç‡ï¼ˆè‹¥æœªæ”¹è¿›åˆ™è¿”å› 0.0ï¼Œè°ƒç”¨æ–¹åº”å¿½ç•¥ï¼‰
        - int: æœ¬æ¬¡å®é™…æ‰§è¡Œçš„ä¼˜åŒ–æ­¥æ•°ï¼ˆ0 æˆ– 1ï¼‰
        - List[str]: æ›´æ–°åçš„å¯å¤„ç†èŠ‚ç‚¹åˆ—è¡¨ï¼ˆå› å›¾ç»“æ„å¯èƒ½å˜åŒ–ï¼Œéœ€é‡æ–°è·å–ï¼‰

        :param adaptoflux_instance: å½“å‰å¾…ä¼˜åŒ–çš„ AdaptoFlux å®ä¾‹
        :param input_data: ç”¨äºè¯„ä¼°çš„å°æ‰¹é‡è¾“å…¥æ•°æ®
        :param target: å¯¹åº”çš„çœŸå®æ ‡ç­¾
        :param processing_nodes: å½“å‰å›¾ä¸­æ‰€æœ‰å¯è¢«ä¼˜åŒ–çš„å¤„ç†èŠ‚ç‚¹åç§°åˆ—è¡¨ï¼ˆå·²æ’é™¤å†»ç»“èŠ‚ç‚¹ï¼‰
        :param current_loss: å½“å‰æ¨¡å‹çš„æŸå¤±å€¼ï¼ˆç”¨äºæ¯”è¾ƒï¼‰
        :param gp: å›¾å¤„ç†å™¨å®ä¾‹ï¼Œç”¨äºè®¿é—®å’Œä¿®æ”¹å›¾ç»“æ„
        :return: (æ˜¯å¦æ”¹è¿›, æ–°æŸå¤±, æ–°å‡†ç¡®ç‡, æ­¥æ•°å¢é‡, æ›´æ–°åçš„èŠ‚ç‚¹åˆ—è¡¨)
        """
        # è‹¥æ— å¯ä¼˜åŒ–èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›æ— æ”¹è¿›
        if not processing_nodes:
            return False, current_loss, 0.0, 0, processing_nodes

        # éšæœºé€‰æ‹©ä¸€ä¸ªå¾…ä¼˜åŒ–èŠ‚ç‚¹
        target_node = random.choice(processing_nodes)
        original_method_name = gp.graph.nodes[target_node]['method_name']
        
        # è·å–ä¸è¯¥èŠ‚ç‚¹å…¼å®¹çš„å€™é€‰æ–¹æ³•åˆ—è¡¨ï¼ˆåŸºäºç»„åˆ«æˆ–ç±»å‹åŒ¹é…ï¼‰
        candidate_methods = self._get_compatible_methods_for_node(
            adaptoflux_instance, 
            target_node
        )

        best_candidate = None
        best_loss = current_loss  # åˆå§‹åŒ–ä¸ºå½“å‰æŸå¤±ï¼Œç”¨äºæ¯”è¾ƒ

        attempts_in_step = 0
        
        # éå†æ‰€æœ‰å€™é€‰æ–¹æ³•ï¼ˆè·³è¿‡å½“å‰æ–¹æ³•ï¼‰
        for candidate_method_name in candidate_methods:
            if candidate_method_name == original_method_name:
                continue

            # --- æ£€æŸ¥æ˜¯å¦å·²è¾¾æ€»å°è¯•ä¸Šé™ ---
            if (self.max_total_refinement_attempts is not None and
                self._total_refinement_attempts >= self.max_total_refinement_attempts):
                break

            # --- è®¡æ•° +1 ---
            self._total_refinement_attempts += 1
            attempts_in_step += 1

            # åˆ›å»ºä¸´æ—¶å‰¯æœ¬ï¼Œé¿å…æ±¡æŸ“åŸæ¨¡å‹
            temp_af = copy.deepcopy(adaptoflux_instance)
            temp_gp = temp_af.graph_processor
            # å°è¯•æ›¿æ¢èŠ‚ç‚¹æ–¹æ³•
            temp_gp.graph.nodes[target_node]['method_name'] = candidate_method_name

            # è¯„ä¼°æ›¿æ¢åçš„æŸå¤±
            new_loss = self._evaluate_loss(input_data, target, adaptoflux_instance=temp_af)


            # è®°å½•æŸå¤±æ›´ä½çš„æœ€ä¼˜å€™é€‰
            if new_loss < best_loss:
                best_loss = new_loss
                best_candidate = candidate_method_name

        # å¦‚æœæ‰¾åˆ°æ›´ä¼˜æ–¹æ³•ï¼Œåˆ™åº”ç”¨åˆ°åŸå›¾
        if best_candidate and best_loss < current_loss:
            # === æ›¿æ¢èŠ‚ç‚¹ï¼ˆè‡ªåŠ¨æ›´æ–° ID å’Œè¾¹ï¼‰ ===
            new_node_id = gp.replace_node_method(target_node, best_candidate)
            
            # æ³¨æ„ï¼štarget_node å·²è¢«åˆ é™¤ï¼Œåç»­æ“ä½œåº”ä½¿ç”¨ new_node_idï¼ˆä½†æœ¬ç­–ç•¥ä¸éœ€è¦ï¼‰
            # å¦‚æœä½œè€…æœ‰ç©ºè€Œä¸”æ²¡å¿˜è®°å¯èƒ½ä¼šåœ¨gpé‡Œé¢åŠ ä¸€ä¸ªåˆ·æ–°å›¾èŠ‚ç‚¹idçš„æ–¹æ³•æå‡å¯è¯»æ€§

            # é‡æ–°è¯„ä¼°å‡†ç¡®ç‡
            new_acc = self._evaluate_accuracy(input_data, target, adaptoflux_instance=adaptoflux_instance)

            # é‡æ–°è·å–å¤„ç†èŠ‚ç‚¹åˆ—è¡¨ï¼ˆå› ä¸ºèŠ‚ç‚¹ ID å·²å˜ï¼‰
            updated_nodes = [node for node in gp.graph.nodes() if gp._is_processing_node(node)]
            
            return True, best_loss, new_acc, 1, updated_nodes
        
        else:
            # æ— æ”¹è¿›ï¼Œè¿”å›åŸå§‹çŠ¶æ€
            return False, current_loss, 0.0, 0, processing_nodes

    def _refine_full_sweep_step(
        self,
        adaptoflux_instance,
        input_data: np.ndarray,
        target: np.ndarray,
        processing_nodes: List[str],
        current_loss: float,
        gp: Any
    ) -> Tuple[bool, float, float, int, List[str]]:
        """
        ã€ç­–ç•¥ï¼šå®Œæ•´éå†ä¼˜åŒ–ã€‘
        å¯¹å½“å‰å›¾ä¸­æ‰€æœ‰å¯ä¼˜åŒ–çš„å¤„ç†èŠ‚ç‚¹è¿›è¡Œä¸€è½®å®Œæ•´éå†ã€‚
        å¯¹æ¯ä¸ªèŠ‚ç‚¹ï¼Œå°è¯•æ‰€æœ‰å…¼å®¹æ–¹æ³•ï¼Œè‹¥å‘ç°èƒ½é™ä½æŸå¤±çš„æ›¿æ¢ï¼Œåˆ™ç«‹å³åº”ç”¨ï¼ˆè´ªå¿ƒç­–ç•¥ï¼‰ã€‚
        ä¸€è½®ä¸­å¯èƒ½å¤šæ¬¡ä¿®æ”¹å›¾ç»“æ„ï¼Œé€‚ç”¨äºæ›´å½»åº•çš„å±€éƒ¨ä¼˜åŒ–ï¼Œä½†è®¡ç®—å¼€é”€è¾ƒå¤§ã€‚
        æ³¨æ„ï¼šæœ¬å‡½æ•°ä¼š in-place ä¿®æ”¹ adaptoflux_instance çš„å›¾ç»“æ„ï¼

        è¿”å›å€¼è¯´æ˜ï¼š
        - bool: æœ¬è½®æ˜¯å¦è‡³å°‘æœ‰ä¸€æ¬¡æˆåŠŸæ”¹è¿›
        - float: æœ¬è½®ç»“æŸåçš„æœ€ç»ˆæŸå¤±å€¼
        - float: æœ¬è½®ç»“æŸåçš„æœ€ç»ˆå‡†ç¡®ç‡
        - int: æœ¬è½®æ€»å…±æ‰§è¡Œçš„æ–¹æ³•æ›¿æ¢æ¬¡æ•°
        - List[str]: æœ€ç»ˆçš„å¯å¤„ç†èŠ‚ç‚¹åˆ—è¡¨ï¼ˆå¯èƒ½å› æ–¹æ³•å˜æ›´è€ŒåŠ¨æ€å˜åŒ–ï¼‰

        :param adaptoflux_instance: å½“å‰å¾…ä¼˜åŒ–çš„ AdaptoFlux å®ä¾‹
        :param input_data: ç”¨äºè¯„ä¼°çš„å°æ‰¹é‡è¾“å…¥æ•°æ®
        :param target: å¯¹åº”çš„çœŸå®æ ‡ç­¾
        :param processing_nodes: åˆå§‹çš„å¯ä¼˜åŒ–èŠ‚ç‚¹åˆ—è¡¨
        :param current_loss: å½“å‰æŸå¤±ï¼ˆä½œä¸ºèµ·ç‚¹ï¼‰
        :param gp: å›¾å¤„ç†å™¨å®ä¾‹
        :return: (æ˜¯å¦æ”¹è¿›, æœ€ç»ˆæŸå¤±, æœ€ç»ˆå‡†ç¡®ç‡, æ›¿æ¢æ¬¡æ•°, æœ€ç»ˆèŠ‚ç‚¹åˆ—è¡¨)
        """
        if not processing_nodes:
            return False, current_loss, 0.0, 0, processing_nodes

        # éšæœºæ‰“ä¹±èŠ‚ç‚¹é¡ºåºï¼Œé¿å…é¡ºåºåå·®ï¼ˆä¾‹å¦‚æ€»æ˜¯å…ˆä¼˜åŒ–é å‰çš„èŠ‚ç‚¹ï¼‰
        nodes_to_try = random.sample(processing_nodes, len(processing_nodes))
        improvement_made = False
        total_replacements = 0
        current_acc = self._evaluate_accuracy(input_data, target, adaptoflux_instance=adaptoflux_instance)

        # éå†æ¯ä¸€ä¸ªå¾…ä¼˜åŒ–èŠ‚ç‚¹
        for target_node in nodes_to_try:
            # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦ä»ç„¶å­˜åœ¨äºå›¾ä¸­ï¼ˆå¯èƒ½è¢«ä¹‹å‰çš„æ›¿æ¢æ“ä½œåˆ é™¤æˆ–é‡å‘½åï¼‰
            if target_node not in gp.graph.nodes:
                if self.verbose:
                    logger.debug(f"Node '{target_node}' no longer exists in graph; skipping.")
                continue

            original_method_name = gp.graph.nodes[target_node].get('method_name')
            if original_method_name is None:
                if self.verbose:
                    logger.warning(f"Node '{target_node}' has no 'method_name'; skipping.")
                continue

            # è·å–ä¸è¯¥èŠ‚ç‚¹å…¼å®¹çš„å€™é€‰æ–¹æ³•åˆ—è¡¨ï¼ˆè€ƒè™‘ç»„åˆ«ã€ç±»å‹å…¼å®¹æ€§åŠå†»ç»“è§„åˆ™ï¼‰
            candidate_methods = self._get_compatible_methods_for_node(
                adaptoflux_instance,
                target_node
            )

            best_candidate = None
            best_loss = current_loss  # ä»¥å½“å‰å…¨å±€æŸå¤±ä¸ºåŸºå‡†è¿›è¡Œæ¯”è¾ƒ

            # å°è¯•æ‰€æœ‰å…¼å®¹æ–¹æ³•ï¼ˆè·³è¿‡å½“å‰æ–¹æ³•ï¼‰
            for candidate_method_name in candidate_methods:
                if candidate_method_name == original_method_name:
                    continue

                # --- æ£€æŸ¥å¹¶è®¡æ•° ---
                if (self.max_total_refinement_attempts is not None and
                    self._total_refinement_attempts >= self.max_total_refinement_attempts):
                    # æå‰é€€å‡ºåŒå±‚å¾ªç¯
                    return improvement_made, current_loss, current_acc, total_replacements, processing_nodes

                self._total_refinement_attempts += 1

                # åˆ›å»ºä¸´æ—¶å‰¯æœ¬è¿›è¡Œå®‰å…¨è¯„ä¼°ï¼Œé¿å…æ±¡æŸ“åŸæ¨¡å‹
                try:
                    temp_af = copy.deepcopy(adaptoflux_instance)
                    temp_gp = temp_af.graph_processor
                    # å®‰å…¨æ›¿æ¢ï¼šä½¿ç”¨å›¾å¤„ç†å™¨çš„æ ‡å‡†æ–¹æ³•ï¼ˆä¼šå¤„ç†è¾“å…¥/è¾“å‡ºç±»å‹å˜åŒ–ï¼‰
                    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿæ›¿æ¢ï¼Œä½†ä¸å®é™…è°ƒç”¨ replace_node_methodï¼ˆå› ä¸ºåªæ˜¯è¯„ä¼°ï¼‰
                    # æ‰€ä»¥ç›´æ¥ä¿®æ”¹ method_name æ˜¯å®‰å…¨çš„ï¼Œå‰ææ˜¯ä¸ä¾èµ–è¾¹ç»“æ„å˜åŒ–
                    temp_gp.graph.nodes[target_node]['method_name'] = candidate_method_name

                    new_loss = self._evaluate_loss(input_data, target, adaptoflux_instance=temp_af)

                    if new_loss < best_loss:
                        best_loss = new_loss
                        best_candidate = candidate_method_name
                except Exception as e:
                    logger.warning(f"Failed to evaluate candidate method '{candidate_method_name}' "
                                   f"for node '{target_node}': {e}")
                    continue

            # å¦‚æœæ‰¾åˆ°æ›´ä¼˜æ–¹æ³•ï¼Œç«‹å³åº”ç”¨åˆ°åŸå›¾ï¼ˆè´ªå¿ƒç­–ç•¥ï¼‰
            if best_candidate and best_loss < current_loss:
                try:
                    # ä½¿ç”¨å›¾å¤„ç†å™¨çš„æ ‡å‡†æ›¿æ¢æ–¹æ³•ï¼Œç¡®ä¿å›¾ç»“æ„ä¸€è‡´æ€§ï¼ˆå¦‚è¾¹æ›´æ–°ã€IDåˆ·æ–°ç­‰ï¼‰
                    new_node_id = gp.replace_node_method(target_node, best_candidate)
                    # æ›´æ–°å½“å‰æŸå¤±å’Œå‡†ç¡®ç‡
                    current_loss = best_loss
                    current_acc = self._evaluate_accuracy(input_data, target, adaptoflux_instance=adaptoflux_instance)
                    improvement_made = True
                    total_replacements += 1

                    if self.verbose:
                        logger.info(f"  Replacement {total_replacements}: Node '{target_node}' "
                                    f"{original_method_name} â†’ {best_candidate}, Loss: {current_loss:.6f}")

                    # é‡è¦ï¼šèŠ‚ç‚¹æ›¿æ¢åï¼ŒåŸ target_node å¯èƒ½å·²è¢«åˆ é™¤æˆ–é‡å‘½åï¼ˆnew_node_idï¼‰
                    # å› æ­¤éœ€è¦é‡æ–°è·å–å½“å‰æ‰€æœ‰å¤„ç†èŠ‚ç‚¹ï¼Œç¡®ä¿åç»­æ“ä½œåŸºäºæœ€æ–°å›¾çŠ¶æ€
                    processing_nodes = [
                        node for node in gp.graph.nodes()
                        if gp._is_processing_node(node)
                    ]
                    # æ³¨æ„ï¼šnodes_to_try æ˜¯æ—§åˆ—è¡¨ï¼Œä½†ä»…åŒ…å«èŠ‚ç‚¹åï¼Œåç»­èŠ‚ç‚¹è‹¥ä»å­˜åœ¨ä»å¯å¤„ç†ï¼›
                    # è‹¥éœ€æ›´ä¸¥æ ¼çš„ä¸€è‡´æ€§ï¼Œå¯è€ƒè™‘ break å¹¶é‡å¯æœ¬è½®ï¼Œä½†ä¼šå¢åŠ å¼€é”€ï¼Œæ­¤å¤„æš‚ä¸å¤„ç†ã€‚

                except Exception as e:
                    logger.error(f"Failed to apply method replacement for node '{target_node}': {e}")
                    # æ›¿æ¢å¤±è´¥ï¼Œè·³è¿‡è¯¥èŠ‚ç‚¹ï¼Œç»§ç»­ä¼˜åŒ–å…¶ä»–èŠ‚ç‚¹
                    continue

        return improvement_made, current_loss, current_acc, total_replacements, processing_nodes
        
    def build_candidate_pool(compatibility_mode, methods, original_method_name, all_method_names):
        if compatibility_mode == "all":
            return all_method_names
        else:
            node_group = methods[original_method_name].get("group", "default")
            group_methods = [
                name for name, info in methods.items()
                if info.get("group", "default") == node_group
            ]
            if compatibility_mode == "group_only":
                return group_methods if group_methods else all_method_names[:1]
            else:  # group_with_fallback
                return group_methods if len(group_methods) >= 2 else all_method_names

    def get_node_layer(self, node_id: str) -> int:
        if node_id in ("root", "collapse"):
            return -1  # ç‰¹æ®ŠèŠ‚ç‚¹è®¾ä¸º -1ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        try:
            return int(node_id.split('_')[0]) # æ„Ÿè§‰è·å–èŠ‚ç‚¹å†è·å–å…ƒå±æ€§å¯èƒ½æ›´æ…¢ï¼Œå…ˆè¿™æ ·å§
        except (ValueError, IndexError):
            logger.warning(f"Cannot parse layer from node_id: {node_id}. Using layer=0.")
            return 0