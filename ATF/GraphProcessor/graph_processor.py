import networkx as nx
import numpy as np
from collections import Counter
import math
from ..CollapseManager.collapse_functions import CollapseFunctionManager, CollapseMethod
import logging
from typing import List, Dict, Set, Optional, Tuple  # add Set here if missing

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
_ALREADY_WARNED_OLD_MODEL = False

class GraphProcessor:
    def __init__(self, graph: nx.MultiDiGraph, methods: dict, collapse_method=CollapseMethod.SUM):
        """
        åˆå§‹åŒ–å›¾å¤„ç†å™¨ã€‚
        
        å‚æ•°:
            graph (nx.MultiDiGraph): åˆå§‹å›¾ç»“æ„
            methods (dict): å¯ç”¨æ–¹æ³•å­—å…¸ {method_name: {"function": ..., "input_count": int, "output_count": int}}
            collapse_method (callable): èšåˆå‡½æ•°ï¼Œé»˜è®¤ä¸º sum
        """
        self.discard_node_method_name = "null"
        self.graph = graph.copy() if graph else nx.MultiDiGraph()
        self.methods = methods
        self.layer = self.get_max_layer_from_graph()  # è®°å½•å½“å‰å±‚æ•°ï¼ˆå¯é€‰ï¼‰

        self.collapse_manager = CollapseFunctionManager(method=collapse_method)

    def set_graph(self, new_graph):
        if not hasattr(new_graph, 'nodes') or not hasattr(new_graph, 'edges'):
            raise ValueError("new_graph ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å›¾å¯¹è±¡")
        self.graph = new_graph
        # è‡ªåŠ¨åŒæ­¥ layer çŠ¶æ€
        self.layer = self.get_max_layer_from_graph()
        print(f"å›¾ç»“æ„å·²æ›´æ–°ï¼Œå½“å‰æœ€å¤§å±‚æ•°ï¼š{self.layer}ã€‚")

    def set_methods(self, new_methods):
        """æ›´æ–° methodsï¼ˆå‡½æ•°å­—å…¸æˆ–æ¨¡å—ï¼‰"""
        if not isinstance(new_methods, dict) and not callable(new_methods):
            raise TypeError("methods åº”è¯¥æ˜¯ä¸€ä¸ªå‡½æ•°å­—å…¸æˆ–å¯è°ƒç”¨å¯¹è±¡")
        self.methods = new_methods
        print("Methods å·²æ›´æ–°ã€‚")

    def append_nx_layer(self, result, discard_unmatched='to_discard', discard_node_method_name="null"):
        """
        å‘å›¾ä¸­æ·»åŠ ä¸€å±‚æ–°èŠ‚ç‚¹ã€‚
        """
        import logging

        self.discard_node_method_name = discard_node_method_name
        self.layer += 1
        new_index_edge = 0
        method_counts = {method_name: 0 for method_name in self.methods}
        if discard_node_method_name not in method_counts:
            method_counts[discard_node_method_name] = 0

        v = "collapse"
        collapse_edges = list(self.graph.in_edges(v, data=True))

        # è½¬æ¢ collapse_edges ä¸ºä¾¿äºæŸ¥æ‰¾çš„æ˜ å°„ï¼šdata_coord -> (u, data)
        coord_to_edge = {}
        for u, _, data in collapse_edges:
            coord = data.get('data_coord')
            if coord is None:
                raise ValueError(f"Edge to 'collapse' missing 'data_coord': {data}")
            if coord in coord_to_edge:
                raise ValueError(f"Duplicate data_coord {coord} found in collapse edges!")
            coord_to_edge[coord] = (u, data)

        # === 1. å¤„ç† valid_groups ===
        for method_name, groups in result['valid_groups'].items():
            if method_name == discard_node_method_name:
                continue
            for group in groups:
                new_target_node = f"{self.layer}_{method_counts[method_name]}_{method_name}"
                self.graph.add_node(
                    new_target_node,
                    method_name=method_name,
                    layer=self.layer,
                    is_passthrough=False
                )

                # æŒ‰ group çš„åŸå§‹é¡ºåºåˆ†é… input_slot
                for input_slot, coord in enumerate(group):
                    if coord not in coord_to_edge:
                        raise ValueError(f"Coord {coord} in group not found in collapse edges!")
                    u, orig_data = coord_to_edge[coord]
                    self.graph.remove_edge(u, v)
                    # æ–°å¢ input_slot å­—æ®µï¼Œä¿ç•™å…¶ä»–å±æ€§
                    new_edge_data = {**orig_data, 'input_slot': input_slot}
                    self.graph.add_edge(u, new_target_node, **new_edge_data)

                # æ·»åŠ è¾“å‡ºè¾¹ï¼ˆåˆ° collapseï¼‰
                output_count = self.methods[method_name]["output_count"]
                for local_output_index in range(output_count):
                    self.graph.add_edge(
                        new_target_node, v,
                        output_index=local_output_index,
                        data_coord=new_index_edge,
                        data_type=self.methods[method_name]["output_types"][local_output_index]
                    )
                    new_index_edge += 1

                method_counts[method_name] += 1

        # === 2. å¤„ç† unmatched (discard) ===
        unmatched_groups = result.get('unmatched', [])
        if unmatched_groups and discard_unmatched == 'to_discard':
            for group in unmatched_groups:
                node_name = f"{self.layer}_{method_counts[discard_node_method_name]}_{discard_node_method_name}"
                self.graph.add_node(
                    node_name,
                    method_name=discard_node_method_name,
                    layer=self.layer,
                    is_passthrough=True
                )

                # æŒ‰ group é¡ºåºåˆ†é… input_slotï¼ˆé€ä¼ èŠ‚ç‚¹ä¹Ÿéœ€è¦ input_slotï¼ï¼‰
                input_data_types = []
                for input_slot, coord in enumerate(group):
                    if coord not in coord_to_edge:
                        raise ValueError(f"Coord {coord} in unmatched group not found in collapse edges!")
                    u, orig_data = coord_to_edge[coord]
                    input_data_types.append(orig_data.get('data_type'))
                    self.graph.remove_edge(u, v)
                    new_edge_data = {**orig_data, 'input_slot': input_slot}
                    self.graph.add_edge(u, node_name, **new_edge_data)

                # é€ä¼ èŠ‚ç‚¹ï¼šè¾“å‡ºè¾¹æ•°é‡ = è¾“å…¥è¾¹æ•°é‡ï¼Œä¸€ä¸€å¯¹åº”
                for local_output_index, used_data_type in enumerate(input_data_types):
                    self.graph.add_edge(
                        node_name, v,
                        output_index=local_output_index,
                        data_coord=new_index_edge,
                        data_type=used_data_type
                    )
                    new_index_edge += 1

                method_counts[discard_node_method_name] += 1

        elif unmatched_groups and discard_unmatched == 'ignore':
            logging.warning(
                "discard_unmatched='ignore' ç­–ç•¥æœªç»å……åˆ†æµ‹è¯•ï¼Œå¯èƒ½å­˜åœ¨æœªé¢„è§è¡Œä¸ºã€‚"
                "å¦‚æœ‰é—®é¢˜ï¼Œè¯·è”ç³»å¼€å‘è€…ã€‚"
            )
            logging.warning(
                "âš ï¸  UNTESTED PATH: discard_unmatched='ignore' is experimental. "
                "Data will be DROPPED (edges removed from 'collapse'). "
                "Use at your own risk."
            )
            # æ‰å¹³åŒ– unmatched_coords
            unmatched_coords = set()
            for group in unmatched_groups:
                unmatched_coords.update(group)
            # åˆ é™¤å¯¹åº”è¾¹ï¼ˆä¸åˆ›å»ºæ–°èŠ‚ç‚¹ï¼‰
            for u, _, data in collapse_edges:
                if data.get('data_coord') in unmatched_coords:
                    if self.graph.has_edge(u, v):
                        self.graph.remove_edge(u, v)

        elif unmatched_groups:
            raise ValueError(f"æœªçŸ¥çš„ discard_unmatched å€¼ï¼š{discard_unmatched}")

        return self.graph

    def remove_last_nx_layer(self):
        """
        åˆ é™¤å›¾ä¸­çš„æœ€åä¸€å±‚èŠ‚ç‚¹ã€‚
        """
        collapse_node = "collapse"
        incoming_edges = list(self.graph.in_edges(collapse_node, data=True))
        nodes_to_remove = set(u for u, v, d in incoming_edges)

        if not nodes_to_remove:
            print("æ²¡æœ‰å¯åˆ é™¤çš„å±‚ã€‚")
            return

        input_edges_map = {
            node: list(self.graph.in_edges(node, data=True)) for node in nodes_to_remove
        }

        for node in nodes_to_remove:
            for prev_node, _, edge_data in input_edges_map[node]:
                self.graph.add_edge(prev_node, collapse_node, **edge_data)

        for node in nodes_to_remove:
            if node != "root":
                self.graph.remove_node(node)
            else:
                print(f"è·³è¿‡åˆ é™¤èŠ‚ç‚¹ï¼š{node}ï¼ˆè¿™æ˜¯ä¿ç•™çš„æ ¹èŠ‚ç‚¹ï¼‰")

        self.layer -= 1
        return self.graph

    def infer_with_graph(self, values):
        """
        ä½¿ç”¨å›¾ç»“æ„å¯¹è¾“å…¥æ•°æ®è¿›è¡Œæ¨ç†ï¼Œæ”¯æŒä»»æ„å¯¹è±¡ï¼ˆéä»…æ•°å€¼ï¼‰ã€‚
        
        å‚æ•°:
            values: 
                - è‹¥ä¸º numpy array: å¿…é¡»æ˜¯ (N, D) shapeï¼Œdtype å¯ä¸º object
                - è‹¥ä¸º list: å¿…é¡»æ˜¯ [[feat0, feat1, ...], ...] çš„äºŒç»´ç»“æ„
        
        è¿”å›:
            collapsed_output: 1D numpy array of scalars (shape [N,])
        """
        import numpy as np

        # === 1. æ ‡å‡†åŒ–è¾“å…¥ä¸º list of listsï¼ˆæ ·æœ¬ Ã— è¾“å…¥ç‰¹å¾ï¼‰===
        if isinstance(values, np.ndarray):
            if values.ndim != 2:
                raise ValueError(f"Input values must be 2D, got shape {values.shape}")
            # è½¬ä¸º list of listsï¼Œä¿ç•™å¯¹è±¡å¼•ç”¨
            input_samples = [list(row) for row in values]
        elif isinstance(values, list):
            if not all(isinstance(row, (list, tuple)) for row in values):
                raise ValueError("Each sample in values must be a list/tuple of features.")
            input_samples = [list(row) for row in values]
        else:
            raise TypeError("values must be a 2D numpy array or list of lists.")

        num_samples = len(input_samples)
        if num_samples == 0:
            return np.array([])

        # === 2. åˆå§‹åŒ–èŠ‚ç‚¹è¾“å‡ºå­—å…¸ ===
        node_outputs = {}
        node_outputs["root"] = input_samples  # list of lists

        # === 3. æ‹“æ‰‘æ’åºï¼ˆæ’é™¤ root å’Œ collapseï¼‰===
        nodes_in_order = list(nx.topological_sort(self.graph))
        nodes_in_order = [n for n in nodes_in_order if n not in {"root", "collapse"}]

        # === 4. é€èŠ‚ç‚¹æ‰§è¡Œ ===
        for node in nodes_in_order:
            node_data = self.graph.nodes[node]
            method_name = node_data.get("method_name")

            # === å¤„ç† is_passthroughï¼ˆå…¼å®¹è€æ¨¡å‹ï¼‰===
            if "is_passthrough" not in node_data:
                is_passthrough = (
                    method_name is None or 
                    (isinstance(method_name, str) and method_name.lower() == 'null') or
                    (isinstance(method_name, str) and method_name.lower() == 'unmatched')
                )
                global _ALREADY_WARNED_OLD_MODEL
                if not _ALREADY_WARNED_OLD_MODEL:
                    logger.warning(
                        f'å½“å‰ä½¿ç”¨çš„ä¸ºè€æ¨¡å‹ï¼ŒèŠ‚ç‚¹ "{node}" æœªæ˜¾å¼æ ‡è®° "is_passthrough"ã€‚'
                        f'æ¨æ–­å…¶ä¸º passthrough={is_passthrough}ã€‚å»ºè®®é‡è®­ç»ƒæˆ–å›é€€ç‰ˆæœ¬ã€‚'
                        f'ä¹‹åçš„ç‰ˆæœ¬å¯èƒ½å–æ¶ˆå¯¹è€ç‰ˆæœ¬çš„å…¼å®¹'
                    )
                    _ALREADY_WARNED_OLD_MODEL = True
            else:
                is_passthrough = bool(node_data.get("is_passthrough", False))

            # === æ”¶é›†æ‰€æœ‰è¾“å…¥ç‰¹å¾ï¼ˆæŒ‰æ ·æœ¬å¯¹é½ï¼‰===
            predecessors = list(self.graph.predecessors(node))
            if not predecessors:
                raise ValueError(f"Node '{node}' has no predecessors.")

            # === æ”¶é›†æ‰€æœ‰è¾“å…¥ç‰¹å¾ï¼ˆæŒ‰ input_slot å¯¹é½ï¼‰===
            # === æ”¶é›†æ‰€æœ‰è¾“å…¥ç‰¹å¾ ===
            input_pairs = []  # ç”¨äºæ–°é€»è¾‘ï¼š(input_slot, extracted)
            input_feature_lists_old = []  # ç”¨äºè€é€»è¾‘ï¼šç›´æ¥ append

            has_input_slot = None  # None=æœªç¡®å®š, True=æœ‰, False=æ— 

            for src in predecessors:
                edges_from_src = self.graph[src][node]  # MultiEdge dict
                for edge_key, edge_data in edges_from_src.items():
                    output_idx = edge_data.get("output_index")
                    input_slot = edge_data.get("input_slot")  # å¯èƒ½ä¸º None
                    src_output = node_outputs[src]

                    if output_idx is None:
                        extracted = [sample_output for sample_output in src_output]
                    else:
                        extracted = [sample_output[output_idx] for sample_output in src_output]

                    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ input_slot
                    current_has_slot = input_slot is not None

                    if has_input_slot is None:
                        has_input_slot = current_has_slot
                    elif has_input_slot != current_has_slot:
                        raise ValueError(
                            f"Node '{node}': mixed use of 'input_slot' in incoming edges. "
                            "All edges must either have 'input_slot' or all must omit it."
                        )

                    if has_input_slot:
                        if not isinstance(input_slot, int):
                            raise ValueError(f"input_slot must be an integer, got {input_slot} (type {type(input_slot)})")
                        input_pairs.append((input_slot, extracted))
                    else:
                        input_feature_lists_old.append(extracted)

            # === æ ¹æ®æ˜¯å¦æœ‰ input_slot é€‰æ‹©è·¯å¾„ ===
            if has_input_slot:
                # --- æ–°é€»è¾‘ï¼šæŒ‰ input_slot æ’åº ---
                input_pairs.sort(key=lambda x: x[0])
                input_slots = [slot for slot, _ in input_pairs]
                expected_slots = list(range(len(input_pairs)))
                if input_slots != expected_slots:
                    raise ValueError(
                        f"Node '{node}' has inconsistent input_slot indices. "
                        f"Expected {expected_slots}, got {input_slots}."
                    )
                input_feature_lists = [data for _, data in input_pairs]
            else:
                # --- è€é€»è¾‘ï¼šä¿æŒè¾¹éå†é¡ºåº ---
                input_feature_lists = input_feature_lists_old

            if is_passthrough:
                if len(input_feature_lists) != 1:
                    raise ValueError(f"Passthrough node '{node}' must have exactly one input, got {len(input_feature_lists)}")
                
                # input_feature_lists[0] is a list of N elements: [feat_sample0, feat_sample1, ..., feat_sampleN-1]
                extracted_per_sample = input_feature_lists[0]  # length = N
                
                # âœ… æ­£ç¡®æ ¼å¼ï¼šæ¯ä¸ªæ ·æœ¬ä¸€ä¸ª listï¼ˆå³ä½¿åªæœ‰ä¸€ä¸ªè¾“å‡ºï¼‰
                node_outputs[node] = [[feat] for feat in extracted_per_sample]
                continue

            # === æ­£å¸¸æ–¹æ³•æ‰§è¡Œ ===
            if method_name not in self.methods:
                raise ValueError(f"Unknown method: {method_name}")

            method_info = self.methods[method_name]
            func = method_info["function"]
            expected_input_count = method_info["input_count"]
            expected_output_count = method_info["output_count"]
            is_vectorized = method_info.get("vectorized", False)

            if len(input_feature_lists) != expected_input_count:
                raise ValueError(
                    f"Node '{node}' method '{method_name}' expects {expected_input_count} inputs, "
                    f"but got {len(input_feature_lists)} from edges."
                )

            # å°è¯•å‘é‡åŒ–æ‰§è¡Œï¼ˆä»…å½“æ–¹æ³•æ ‡è®°ä¸º vectorized=Trueï¼‰
            node_output_samples = None
            if is_vectorized and num_samples > 1:
                try:
                    # === å°è¯•æ„å»ºæ‰¹é‡è¾“å…¥ ===
                    batched_inputs = []
                    for input_list in input_feature_lists:
                        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å…ƒç´ ç±»å‹ä¸€è‡´ä¸”å¯å †å 
                        first = input_list[0]
                        
                        # æƒ…å†µ1: å…¨æ˜¯æ ‡é‡ï¼ˆint/float/np.numberï¼‰
                        if all(isinstance(x, (int, float, np.number)) for x in input_list):
                            batched = np.array(input_list)
                        # æƒ…å†µ2: å…¨æ˜¯ numpy æ•°ç»„ä¸” shape ä¸€è‡´
                        elif all(isinstance(x, np.ndarray) for x in input_list):
                            shapes = [x.shape for x in input_list]
                            if all(s == shapes[0] for s in shapes):
                                batched = np.stack(input_list, axis=0)  # (N, ...)
                            else:
                                raise ValueError("Array shapes mismatch, cannot vectorize")
                        # æƒ…å†µ3: å…¶ä»–ç±»å‹ï¼ˆstr, dict ç­‰ï¼‰â†’ æ— æ³•å‘é‡åŒ–
                        else:
                            raise ValueError("Non-numeric or mixed types, cannot vectorize")
                            
                        batched_inputs.append(batched)
                    
                    batched_outputs = func(*batched_inputs)  # åº”è¿”å› (N, output_count) æˆ– tuple of (N,)
                    
                    # === æ ‡å‡†åŒ–è¾“å‡ºä¸º list of lists ===
                    if isinstance(batched_outputs, tuple):
                        # å¤šè¾“å‡ºï¼šæ¯ä¸ªæ˜¯ (N,) æˆ– (N, ...)
                        if len(batched_outputs) != expected_output_count:
                            raise ValueError(f"Expected {expected_output_count} outputs, got {len(batched_outputs)}")
                        # è½¬ç½®: [(N,), (N,)] â†’ [(out0_s0, out1_s0), ...]
                        node_output_samples = []
                        for i in range(num_samples):
                            sample_outs = [batched_outputs[j][i] for j in range(expected_output_count)]
                            # å¦‚æœè¾“å‡ºæ˜¯æ•°ç»„ï¼Œä¿ç•™ä¸ºæ•°ç»„ï¼ˆä¸å¼ºåˆ¶æ ‡é‡ï¼‰
                            node_output_samples.append(sample_outs)
                    else:
                        # å•è¾“å‡ºæˆ– (N, output_count)
                        if batched_outputs.ndim == 1:
                            # (N,) â†’ æ¯ä¸ªæ ·æœ¬ä¸€ä¸ªæ ‡é‡
                            if expected_output_count != 1:
                                raise ValueError(f"Expected {expected_output_count} outputs, but got 1D array")
                            node_output_samples = [[x] for x in batched_outputs]
                        elif batched_outputs.ndim == 2:
                            # (N, output_count)
                            if batched_outputs.shape[1] != expected_output_count:
                                raise ValueError(f"Output shape {batched_outputs.shape} mismatches output_count={expected_output_count}")
                            node_output_samples = batched_outputs.tolist()
                        else:
                            raise ValueError(f"Unsupported output ndim: {batched_outputs.ndim}")
                            
                except Exception as e:
                    # å›é€€åˆ°é€æ ·æœ¬æ¨¡å¼ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
                    logger.warning(
                        f"Vectorized execution failed for method '{method_name}' (inputs: {[type(x[0]) for x in input_feature_lists]}), "
                        f"fallback to sample-by-sample. Error: {e}"
                    )
                    is_vectorized = False  # è§¦å‘ä¸‹æ–¹é€æ ·æœ¬é€»è¾‘

            # === å›é€€ï¼šé€æ ·æœ¬æ‰§è¡Œï¼ˆåŸé€»è¾‘ï¼‰===
            else:
                node_output_samples = []
                for sample_idx in range(num_samples):
                    sample_inputs = [input_list[sample_idx] for input_list in input_feature_lists]
                    try:
                        result = func(*sample_inputs)
                    except Exception as e:
                        raise RuntimeError(
                            f"Error in method '{method_name}' at sample {sample_idx}:\n"
                            f"  Inputs: {sample_inputs}\n"
                            f"  Error: {e}"
                        ) from e

                    if not isinstance(result, (list, tuple)):
                        result = [result]
                    result = list(result)

                    if len(result) != expected_output_count:
                        raise ValueError(
                            f"Method '{method_name}' returned {len(result)} outputs, "
                            f"but expected {expected_output_count} (output_count)."
                        )

                    node_output_samples.append(result)

            # ä¿å­˜ç»“æœ
            node_outputs[node] = node_output_samples

        # === 5. èšåˆåˆ° collapse èŠ‚ç‚¹ ===
        collapse_inputs = []  # List[Tuple[global_coord, List[Any]]]
        for u, v, data in self.graph.in_edges("collapse", data=True):
            local_idx = data.get("output_index")
            global_coord = data.get("data_coord")
            if local_idx is None or global_coord is None:
                raise ValueError(f"Edge from {u} to collapse missing output_index or data_coord")

            src_output = node_outputs[u]  # list of lists
            feature_values = [sample_output[local_idx] for sample_output in src_output]
            collapse_inputs.append((global_coord, feature_values))

        if not collapse_inputs:
            raise ValueError("No inputs connected to 'collapse' node.")

        # æŒ‰ global_coord æ’åºä»¥ä¿è¯é¡ºåºä¸€è‡´
        collapse_inputs.sort(key=lambda x: x[0])
        all_features_per_sample = list(zip(*[feat_list for _, feat_list in collapse_inputs]))  # transpose

        # === 6. åº”ç”¨ collapse å‡½æ•° ===
        collapsed_results = []
        for sample_features in all_features_per_sample:
            try:
                # collapse æ¥æ”¶ listï¼Œè¿”å›ä»»æ„å¯¹è±¡ï¼ˆæ ‡é‡ã€å‘é‡ã€å­—ç¬¦ä¸²ç­‰ï¼‰
                collapsed_val = self.collapse_manager.collapse(list(sample_features))
                collapsed_results.append(collapsed_val)
            except Exception as e:
                raise RuntimeError(
                    f"Error in collapse function with inputs {list(sample_features)}:\n{e}"
                ) from e

        # è¿”å›ç»“æœåˆ—è¡¨ï¼ˆä¸å¼ºåˆ¶è½¬ä¸º np.arrayï¼‰
        return np.array(collapsed_results)
    
    def infer_with_task_parallel(self, values, num_workers=4):
        from concurrent.futures import ThreadPoolExecutor
        import threading
        import queue
        import numpy as np
        import traceback

        # === 1. æ ‡å‡†åŒ–è¾“å…¥ä¸º list of listsï¼ˆä¸ infer_with_graph ä¸€è‡´ï¼‰===
        if isinstance(values, np.ndarray):
            if values.ndim != 2:
                raise ValueError(f"Input values must be 2D, got shape {values.shape}")
            input_samples = [list(row) for row in values]
        elif isinstance(values, list):
            if not all(isinstance(row, (list, tuple)) for row in values):
                raise ValueError("Each sample in values must be a list/tuple of features.")
            input_samples = [list(row) for row in values]
        else:
            raise TypeError("values must be a 2D numpy array or list of lists.")

        # åˆå§‹åŒ–
        node_outputs = {"root": input_samples}
        lock = threading.Lock()
        in_degree_remaining = {}
        ready_queue = queue.Queue()

        # æ„å»ºæ‹“æ‰‘ä¾èµ–å›¾ï¼Œå¹¶åˆå§‹åŒ–æ¯ä¸ªèŠ‚ç‚¹çš„å¾…å®Œæˆå‰é©±æ•°
        for node in self.graph.nodes:
            if node in ["root", "collapse"]:
                continue
            preds = list(self.graph.predecessors(node))
            in_degree_remaining[node] = len(preds)
            if len(preds) == 0:
                ready_queue.put(node)

        # å¤„ç† root çš„ç›´æ¥åç»§
        for succ in self.graph.successors("root"):
            if succ in in_degree_remaining:
                in_degree_remaining[succ] -= 1
                if in_degree_remaining[succ] == 0:
                    ready_queue.put(succ)

        # æ£€æŸ¥ collapse æ˜¯å¦æœ‰è¾“å…¥
        collapse_in_edges = list(self.graph.in_edges("collapse"))
        if not collapse_in_edges:
            return []

        # å·¥ä½œå‡½æ•°
        def process_node(node):
            try:
                with lock:
                    # === æ”¶é›†æ‰€æœ‰è¾“å…¥ç‰¹å¾ï¼ˆæŒ‰è¾“å…¥æ§½ç»„ç»‡ï¼‰===
                    predecessors = list(self.graph.predecessors(node))
                    if not predecessors:
                        raise ValueError(f"Node '{node}' has no predecessors.")

                    # === æ”¶é›†æ‰€æœ‰è¾“å…¥ç‰¹å¾ï¼ˆæŒ‰ input_slot å¯¹é½ï¼‰===
                    # === æ”¶é›†æ‰€æœ‰è¾“å…¥ç‰¹å¾ ===
                    input_pairs = []  # ç”¨äºæ–°é€»è¾‘ï¼š(input_slot, extracted)
                    input_feature_lists_old = []  # ç”¨äºè€é€»è¾‘ï¼šç›´æ¥ append

                    has_input_slot = None  # None=æœªç¡®å®š, True=æœ‰, False=æ— 

                    for src in predecessors:
                        edges_from_src = self.graph[src][node]  # MultiEdge dict
                        for edge_key, edge_data in edges_from_src.items():
                            output_idx = edge_data.get("output_index")
                            input_slot = edge_data.get("input_slot")  # å¯èƒ½ä¸º None
                            src_output = node_outputs[src]

                            if output_idx is None:
                                extracted = [sample_output for sample_output in src_output]
                            else:
                                extracted = [sample_output[output_idx] for sample_output in src_output]

                            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ input_slot
                            current_has_slot = input_slot is not None

                            if has_input_slot is None:
                                has_input_slot = current_has_slot
                            elif has_input_slot != current_has_slot:
                                raise ValueError(
                                    f"Node '{node}': mixed use of 'input_slot' in incoming edges. "
                                    "All edges must either have 'input_slot' or all must omit it."
                                )

                            if has_input_slot:
                                if not isinstance(input_slot, int):
                                    raise ValueError(f"input_slot must be an integer, got {input_slot} (type {type(input_slot)})")
                                input_pairs.append((input_slot, extracted))
                            else:
                                input_feature_lists_old.append(extracted)

                    # === æ ¹æ®æ˜¯å¦æœ‰ input_slot é€‰æ‹©è·¯å¾„ ===
                    if has_input_slot:
                        # --- æ–°é€»è¾‘ï¼šæŒ‰ input_slot æ’åº ---
                        input_pairs.sort(key=lambda x: x[0])
                        input_slots = [slot for slot, _ in input_pairs]
                        expected_slots = list(range(len(input_pairs)))
                        if input_slots != expected_slots:
                            raise ValueError(
                                f"Node '{node}' has inconsistent input_slot indices. "
                                f"Expected {expected_slots}, got {input_slots}."
                            )
                        input_feature_lists = [data for _, data in input_pairs]
                    else:
                        # --- è€é€»è¾‘ï¼šä¿æŒè¾¹éå†é¡ºåº ---
                        input_feature_lists = input_feature_lists_old

                # === è·å–èŠ‚ç‚¹å…ƒä¿¡æ¯ ===
                node_data = self.graph.nodes[node]
                method_name = node_data.get("method_name")

                # å…¼å®¹è€æ¨¡å‹ï¼šæ¨æ–­ is_passthrough
                if "is_passthrough" not in node_data:
                    is_passthrough = (
                        method_name is None or 
                        (isinstance(method_name, str) and method_name.lower() == 'null')
                    )
                else:
                    is_passthrough = bool(node_data.get("is_passthrough", False))

                # === å¤„ç† passthrough èŠ‚ç‚¹ ===
                if is_passthrough:
                    if len(input_feature_lists) != 1:
                        raise ValueError(f"Passthrough node '{node}' must have exactly one input, got {len(input_feature_lists)}")
                    # è¾“å…¥æ˜¯ [obj, obj, ...]ï¼Œè¾“å‡ºéœ€ä¸º [[obj], [obj], ...]
                    node_output_samples = [[x] for x in input_feature_lists[0]]
                else:
                    # === æ­£å¸¸æ–¹æ³•æ‰§è¡Œ ===
                    if not method_name:
                        raise ValueError(f"Node '{node}'æœªæŒ‡å®š method_name ä¸”é passthrough")

                    if method_name not in self.methods:
                        raise KeyError(f"Method '{method_name}' not registered in self.methods")

                    method_info = self.methods[method_name]
                    func = method_info["function"]
                    expected_input_count = method_info["input_count"]
                    expected_output_count = method_info["output_count"]
                    is_vectorized = method_info.get("vectorized", False)

                    if len(input_feature_lists) != expected_input_count:
                        raise ValueError(
                            f"Node '{node}' method '{method_name}' expects {expected_input_count} inputs, "
                            f"but got {len(input_feature_lists)}."
                        )

                    num_samples = len(input_feature_lists[0]) if input_feature_lists else 0
                    node_output_samples = None

                    # --- å°è¯•å‘é‡åŒ–æ‰§è¡Œ ---
                    if is_vectorized and num_samples > 1:
                        try:
                            batched_inputs = []
                            for input_list in input_feature_lists:
                                first = input_list[0]
                                # æƒ…å†µ1: å…¨æ˜¯æ ‡é‡ï¼ˆint/float/np.numberï¼‰
                                if all(isinstance(x, (int, float, np.number)) for x in input_list):
                                    batched = np.array(input_list)
                                # æƒ…å†µ2: å…¨æ˜¯ numpy æ•°ç»„ä¸” shape ä¸€è‡´
                                elif all(isinstance(x, np.ndarray) for x in input_list):
                                    shapes = [x.shape for x in input_list]
                                    if all(s == shapes[0] for s in shapes):
                                        batched = np.stack(input_list, axis=0)
                                    else:
                                        raise ValueError("Array shapes mismatch, cannot vectorize")
                                # æƒ…å†µ3: å…¶ä»–ç±»å‹ï¼ˆstr, dict, list, objectï¼‰â†’ æ— æ³•å‘é‡åŒ–
                                else:
                                    raise ValueError("Non-numeric or mixed types, cannot vectorize")
                                batched_inputs.append(batched)

                            batched_outputs = func(*batched_inputs)

                            # æ ‡å‡†åŒ–è¾“å‡ºä¸º list of lists
                            if isinstance(batched_outputs, tuple):
                                if len(batched_outputs) != expected_output_count:
                                    raise ValueError(f"Expected {expected_output_count} outputs, got {len(batched_outputs)}")
                                node_output_samples = [
                                    [batched_outputs[j][i] for j in range(expected_output_count)]
                                    for i in range(num_samples)
                                ]
                            else:
                                if batched_outputs.ndim == 1:
                                    if expected_output_count != 1:
                                        raise ValueError(f"Expected {expected_output_count} outputs, but got 1D array")
                                    node_output_samples = [[x] for x in batched_outputs]
                                elif batched_outputs.ndim == 2:
                                    if batched_outputs.shape[1] != expected_output_count:
                                        raise ValueError(f"Output shape {batched_outputs.shape} mismatches output_count={expected_output_count}")
                                    node_output_samples = batched_outputs.tolist()
                                else:
                                    raise ValueError(f"Unsupported output ndim: {batched_outputs.ndim}")

                        except Exception as e:
                            logger.warning(
                                f"Vectorized execution failed for method '{method_name}' (inputs: {[type(x[0]) for x in input_feature_lists]}), "
                                f"fallback to sample-by-sample. Error: {e}"
                            )
                            is_vectorized = False

                    # --- å›é€€ï¼šé€æ ·æœ¬æ‰§è¡Œ ---
                    else:
                        node_output_samples = []
                        for sample_idx in range(num_samples):
                            sample_inputs = [slot[sample_idx] for slot in input_feature_lists]
                            try:
                                result = func(*sample_inputs)
                            except Exception as e:
                                raise RuntimeError(
                                    f"Error in method '{method_name}' at sample {sample_idx}:\n"
                                    f"  Inputs: {sample_inputs}\n"
                                    f"  Error: {e}"
                                ) from e

                            if not isinstance(result, (list, tuple)):
                                result = [result]
                            result = list(result)

                            if len(result) != expected_output_count:
                                raise ValueError(
                                    f"Method '{method_name}' returned {len(result)} outputs, "
                                    f"but expected {expected_output_count}."
                                )
                            node_output_samples.append(result)

                # === ä¿å­˜ç»“æœå¹¶è§¦å‘åç»§èŠ‚ç‚¹ ===
                with lock:
                    node_outputs[node] = node_output_samples
                    for succ in self.graph.successors(node):
                        if succ == "collapse":
                            continue
                        if succ in in_degree_remaining:
                            in_degree_remaining[succ] -= 1
                            if in_degree_remaining[succ] == 0:
                                ready_queue.put(succ)

            except Exception as e:
                error_msg = f"[ğŸ”¥ CRITICAL ERROR in process_node] Node '{node}' failed: {e}"
                print(error_msg)
                traceback.print_exc()
                raise

        # å¯åŠ¨çº¿ç¨‹æ± 
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = []
            total_nodes_to_execute = len([n for n in self.graph.nodes if n not in {"root", "collapse"}])
            submitted_nodes = set()

            while len(submitted_nodes) < total_nodes_to_execute:
                try:
                    node = ready_queue.get(timeout=1)
                    if node in submitted_nodes:
                        continue
                    submitted_nodes.add(node)
                    futures.append(executor.submit(process_node, node))
                except queue.Empty:
                    continue

            for f in futures:
                f.result()

        # === 5. èšåˆåˆ° collapse èŠ‚ç‚¹ï¼ˆä¸ infer_with_graph ä¸€è‡´ï¼‰===
        collapse_inputs = []  # List[Tuple[global_coord, List[Any]]]
        for u, v, data in self.graph.in_edges("collapse", data=True):
            local_idx = data.get("output_index")
            global_coord = data.get("data_coord")
            if local_idx is None or global_coord is None:
                raise ValueError(f"Edge from {u} to collapse missing output_index or data_coord")

            src_output = node_outputs[u]  # list of lists
            feature_values = [sample[local_idx] for sample in src_output]
            collapse_inputs.append((global_coord, feature_values))

        if not collapse_inputs:
            return []

        # æŒ‰ global_coord æ’åºä»¥ä¿è¯é¡ºåºä¸€è‡´
        collapse_inputs.sort(key=lambda x: x[0])
        all_features_per_sample = list(zip(*[feat_list for _, feat_list in collapse_inputs]))

        # === 6. åº”ç”¨ collapse å‡½æ•° ===
        collapsed_results = []
        for sample_features in all_features_per_sample:
            try:
                collapsed_val = self.collapse_manager.collapse(list(sample_features))
                collapsed_results.append(collapsed_val)
            except Exception as e:
                raise RuntimeError(
                    f"Error in collapse function with inputs {list(sample_features)}:\n{e}"
                ) from e

        return np.array(collapsed_results)  # è¿”å› listï¼Œä¸ infer_with_graph ä¸€è‡´

    def infer_with_graph_single(self, sample, use_pipeline=False, num_workers=4):
        # Step 1: ç¡®ä¿ sample æ˜¯ 1D å¯è¿­ä»£ï¼ˆlist/tuple/arrayï¼‰
        if isinstance(sample, np.ndarray):
            if sample.ndim == 0:
                sample = [sample.item()]
            elif sample.ndim == 1:
                sample = sample.tolist()  # è½¬ä¸º Python list
            else:
                raise ValueError("Single sample must be 0D or 1D")
        elif not isinstance(sample, (list, tuple)):
            sample = [sample]

        # Step 2: åŒ…è£…æˆ batch: [[x1, x2, ...]]
        batch_input = [sample]  # âœ… å…³é”®ï¼šäºŒç»´ç»“æ„

        # Step 3: è°ƒç”¨æ‰¹å¤„ç†æ¥å£
        if use_pipeline:
            result = self.infer_with_task_parallel(batch_input, num_workers=num_workers)
        else:
            result = self.infer_with_graph(batch_input)

        return result[0]
    
    # åœ¨ GraphProcessor ç±»ä¸­
    def replace_node_method(
        self,
        old_node_id: str,
        new_method_name: str
    ) -> str:
        """
        æ›¿æ¢å›¾ä¸­ä¸€ä¸ªèŠ‚ç‚¹çš„æ–¹æ³•ï¼Œå¹¶æ›´æ–°å…¶ ID å’Œæ‰€æœ‰ç›¸è¿çš„è¾¹ã€‚
        ä¸åšå›¾å…¨èŠ‚ç‚¹åˆ·æ–°ï¼ˆå…¨èŠ‚ç‚¹åˆ·æ–°è€—èƒ½é«˜å¹¶ä¸”æ¨ç†ä¸ä¾èµ–å…·ä½“idï¼Œå¯èƒ½åç»­åšä¸ªå•ç‹¬æ–¹æ³•ï¼‰
        è¯¥æ–¹æ³•ä¸åšç±»å‹æ£€æµ‹
        
        :param old_node_id: è¦æ›¿æ¢çš„æ—§èŠ‚ç‚¹ IDï¼ˆå¦‚ "2_3_return_value"ï¼‰
        :param new_method_name: æ–°çš„æ–¹æ³•åï¼ˆå¦‚ "add_values"ï¼‰
        :return: æ–°èŠ‚ç‚¹çš„ IDï¼ˆå¦‚ "2_0_add_values"ï¼‰
        """
        if new_method_name not in self.methods:
            raise ValueError(f"Method '{new_method_name}' not registered in methods.")
        graph = self.graph

        # === 1. è·å–æ—§èŠ‚ç‚¹ä¿¡æ¯ ===
        if old_node_id not in graph:
            raise ValueError(f"Node '{old_node_id}' not found in graph.")
        
        old_data = graph.nodes[old_node_id]
        old_method = old_data.get("method_name")
        if old_method is None:
            raise ValueError(f"Node '{old_node_id}' has no 'method_name' attribute.")

        # === 2. è§£ææ—§ ID è·å– layer å’Œ index å‰ç¼€ ===
        # æ—§ ID æ ¼å¼: {layer}_{index}_{method_name} æˆ– {layer}_{index}_unmatched
        id_parts = old_node_id.split('_', 2)  # æœ€å¤š split æˆ 3 éƒ¨åˆ†
        if len(id_parts) < 3:
            raise ValueError(f"Invalid node ID format: '{old_node_id}'")
        
        layer_str, index_str, _ = id_parts
        try:
            layer = int(layer_str)
        except ValueError:
            raise ValueError(f"Invalid layer in node ID: '{old_node_id}'")

        # === 3. ç”Ÿæˆæ–° ID ===
        new_base_name = new_method_name  # âœ… å…³é”®ï¼šå®šä¹‰ new_base_name
        
        new_node_id = self._generate_unique_node_id(layer, new_method_name)

        if new_node_id in graph:
            raise RuntimeError(f"Node ID collision: {new_node_id} already exists!")

        # === 4. ä¿å­˜æ—§èŠ‚ç‚¹çš„å…¥è¾¹å’Œå‡ºè¾¹ ===
        in_edges = list(graph.in_edges(old_node_id, keys=True, data=True))
        out_edges = list(graph.out_edges(old_node_id, keys=True, data=True))

        # === 5. åˆ é™¤æ—§èŠ‚ç‚¹ ===
        graph.remove_node(old_node_id)

        # === 6. æ·»åŠ æ–°èŠ‚ç‚¹ ===
        is_passthrough = (new_method_name == self.discard_node_method_name)
        graph.add_node(new_node_id, method_name=new_method_name, is_passthrough=is_passthrough)

        # === 7. é‡è¿å…¥è¾¹ï¼ˆsource -> new_node_idï¼‰===
        for src, _, key, data in in_edges:
            graph.add_edge(src, new_node_id, key=key, **data)

        # === 8. é‡è¿å‡ºè¾¹ï¼ˆnew_node_id -> targetï¼‰===
        for _, dst, key, data in out_edges:
            graph.add_edge(new_node_id, dst, key=key, **data)

        logger.debug(
            "Replaced node '%s' (%s) with '%s' (%s)",
            old_node_id, old_method, new_node_id, new_method_name
        )
        
        return new_node_id

    def _is_processing_node(self, node):
        """
        åˆ¤æ–­ä¸€ä¸ªèŠ‚ç‚¹æ˜¯å¦æ˜¯éœ€è¦æ‰§è¡Œå‡½æ•°çš„â€œå¤„ç†èŠ‚ç‚¹â€ã€‚
        æ’é™¤ rootã€collapse å’Œ passthroughï¼ˆå¦‚ discard/unmatchedï¼‰èŠ‚ç‚¹ã€‚
        """
        if node in {"root", "collapse"}:
            return False
        
        node_data = self.graph.nodes.get(node, {})

        # å¦‚æœæ˜¯ passthrough èŠ‚ç‚¹ï¼ˆå¦‚ discard_node_method_name å¯¹åº”çš„èŠ‚ç‚¹ï¼‰ï¼Œä¸è§†ä¸ºå¤„ç†èŠ‚ç‚¹
        if node_data.get("is_passthrough", False):
            return False
        
        return True

    def get_graph_entropy(self):
        """
        è®¡ç®—å›¾ç»“æ„çš„ç†µå€¼ï¼ŒåŸºäºèŠ‚ç‚¹å’Œæ–¹æ³•ç±»å‹çš„åˆ†å¸ƒã€‚
        ç¤ºä¾‹è®¡ç®—æ–¹æ³•ï¼Œå¯æ ¹æ®å®é™…éœ€æ±‚æ›¿æ¢ã€‚
        :return: è®¡ç®—å¾—åˆ°çš„å›¾ç»“æ„ç†µå€¼
        """
        method_counter = Counter()

        # ç»Ÿè®¡æ¯ç§æ–¹æ³•çš„å‡ºç°æ¬¡æ•°
        for node in self.graph.nodes:
            data = self.graph.nodes[node]
            method_name = data.get("method_name")
            if method_name and method_name != "null":  # å¿½ç•¥ null èŠ‚ç‚¹
                method_counter[method_name] += 1

        if not method_counter:
            return 0.0

        # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        total = sum(method_counter.values())
        probabilities = [count / total for count in method_counter.values()]

        # è®¡ç®—é¦™å†œç†µ
        entropy = -sum(p * math.log2(p) for p in probabilities if p > 0)

        return entropy

    def get_method_counter(self):
        """
        ç»Ÿè®¡å›¾ä¸­å„ method_name çš„å‡ºç°æ¬¡æ•°
        """
        from collections import Counter
        method_counter = Counter()

        for node in self.graph.nodes:
            data = self.graph.nodes[node]
            method_name = data.get("method_name")
            if method_name and method_name != "null":  # å¿½ç•¥ null èŠ‚ç‚¹
                method_counter[method_name] += 1

        return method_counter

    def get_max_layer_from_graph(self):
        max_layer = 0
        for node in self.graph.nodes:
            if node == 'root' or node == 'collapse':
                continue
            if isinstance(node, str) and '_' in node:
                layer = int(node.split('_')[0])
                if layer > max_layer:
                    max_layer = layer
        return max_layer

    def replace_subgraph_with_graph(
        self,
        subgraph_nodes: Set[str],
        replacement_graph: nx.DiGraph,
        input_port_bindings: Dict[str, Tuple[str, str, dict]],   # port_name â†’ (src, key, data)
        output_port_bindings: Dict[str, Tuple[str, str, dict]],  # port_name â†’ (dst, key, data)
        root_placeholder: str = "root",
        collapse_placeholder: str = "collapse"
    ) -> List[str]:
        if not subgraph_nodes:
            raise ValueError("subgraph_nodes cannot be empty")
        if root_placeholder not in replacement_graph or collapse_placeholder not in replacement_graph:
            raise ValueError("replacement_graph must contain 'root' and 'collapse' nodes.")

        graph = self.graph

        # 1. åˆ é™¤å­å›¾
        for node in subgraph_nodes:
            if node in graph:
                graph.remove_node(node)

        # 2. è®¡ç®—æ–° layerï¼ˆåŸºäº replacement_graph ä¸­ç¦» root çš„è·ç¦»ï¼‰
        internal_nodes = [n for n in replacement_graph.nodes() if n not in (root_placeholder, collapse_placeholder)]
        
        try:
            dist_from_root = nx.single_source_shortest_path_length(replacement_graph, root_placeholder)
        except Exception as e:
            raise ValueError(f"Cannot compute layers from root: {e}")

        # æå–æ‰€æœ‰åˆæ³•çš„ layer ç¼–å·
        valid_layers = []
        for node in subgraph_nodes:
            if node in ("root", "collapse"):
                continue
            parts = node.split('_', 1)
            if len(parts) >= 1:
                try:
                    layer = int(parts[0])
                    valid_layers.append(layer)
                except ValueError:
                    continue  # å¿½ç•¥æ— æ³•è§£æ layer çš„èŠ‚ç‚¹

        # è®¾å®šå…¨å±€åç§»ï¼šå¦‚æœæœ‰åˆæ³• layerï¼Œå–æœ€å°å€¼ï¼›å¦åˆ™é»˜è®¤ä¸º 0
        global_offset = min(valid_layers) if valid_layers else 0

        # 3. é‡å‘½åèŠ‚ç‚¹
        node_mapping = {}
        for node in internal_nodes:
            method = replacement_graph.nodes[node].get('method_name')
            if not method:
                raise ValueError(f"Node '{node}' missing 'method_name'")
            layer = global_offset + dist_from_root[node] - 1
            new_name = self._generate_unique_node_id(layer, method)
            node_mapping[node] = new_name

        renamed_replacement = nx.relabel_nodes(replacement_graph, node_mapping, copy=True)

        # 4. æ·»åŠ å†…éƒ¨èŠ‚ç‚¹å’Œè¾¹
        for node in internal_nodes:
            new_node = node_mapping[node]
            graph.add_node(new_node, **renamed_replacement.nodes[new_node])
        
        for u, v, key, data in renamed_replacement.edges(keys=True, data=True):
            if u != root_placeholder and v != collapse_placeholder:
                graph.add_edge(u, v, key=key, **data)

        # 5. é‡è¿è¾“å…¥ï¼šç›´æ¥æŒ‰ port_name æŸ¥æ‰¾
        for _, internal_node, key, data in renamed_replacement.out_edges(root_placeholder, keys=True, data=True):
            port_name = data.get('port_name')
            if not port_name:
                raise ValueError("Replacement input edge missing 'port_name'")
            if port_name not in input_port_bindings:
                raise ValueError(f"No input binding for port: {port_name}")
            src, key_in, orig_data = input_port_bindings[port_name]
            merged = {
                'data_type': orig_data.get('data_type', data.get('data_type', 'scalar')),
                'output_index': orig_data.get('output_index', 0),
                'input_slot': data.get('input_slot', orig_data.get('input_slot', 0)),
                'data_coord': data.get('data_coord', 0),
                'port_name': port_name
            }
            graph.add_edge(src, internal_node, key=key_in, **merged)

        # 6. é‡è¿è¾“å‡º
        for internal_node, _, key, data in renamed_replacement.in_edges(collapse_placeholder, keys=True, data=True):
            port_name = data.get('port_name')
            if not port_name:
                raise ValueError("Replacement output edge missing 'port_name'")
            if port_name not in output_port_bindings:
                raise ValueError(f"No output binding for port: {port_name}")
            dst, key_out, orig_data = output_port_bindings[port_name]
            merged = {
                'data_type': orig_data.get('data_type', data.get('data_type', 'scalar')),
                'output_index': data.get('output_index', orig_data.get('output_index', 0)),
                'input_slot': orig_data.get('input_slot', data.get('input_slot', 0)),
                'data_coord': data.get('data_coord', 0),
                'port_name': port_name
            }
            graph.add_edge(internal_node, dst, key=key_out, **merged)

        return list(node_mapping.values())

    def _generate_unique_node_id(self, layer: int, method_name: str) -> str:
        """
        ç”Ÿæˆç¬¦åˆ {layer}_{index}_{method_name} æ ¼å¼çš„å”¯ä¸€èŠ‚ç‚¹ IDã€‚
        å¤ç”¨ replace_node_method ä¸­çš„å‘½åé€»è¾‘ï¼Œä¾›å†…éƒ¨ä½¿ç”¨ã€‚
        """
        existing_indices = set()
        prefix = f"{layer}_"
        suffix = f"_{method_name}"
        for nid in self.graph.nodes():
            if nid.startswith(prefix) and nid.endswith(suffix):
                idx_part = nid[len(prefix): -len(suffix)]
                if idx_part.isdigit():
                    existing_indices.add(int(idx_part))
        new_index = 0
        while new_index in existing_indices:
            new_index += 1
        return f"{layer}_{new_index}_{method_name}"