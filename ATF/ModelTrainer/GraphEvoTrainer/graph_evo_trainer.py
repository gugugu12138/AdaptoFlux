# graph_evo_trainer.py
from ..model_trainer import ModelTrainer
import numpy as np
import logging
import random
import copy
from typing import Optional, List, Dict, Any, Tuple
import os
import json
import traceback
from ...PathGenerator.path_generator import PathGenerator
from ...GraphManager.graph_processor import GraphProcessor

from .Components import (
    BFSSubgraphSampler,
    SubgraphIOExtractor,
    SubgraphReplacer,
    MSEEquivalenceChecker
)

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class GraphEvoTrainer(ModelTrainer):
    """
    ä¸€ä¸ªç»§æ‰¿è‡ª ModelTrainer çš„å…·ä½“è®­ç»ƒå™¨ã€‚
    è¯¥è®­ç»ƒå™¨å®ç°äº† AdaptoFlux çš„å›¾æ¼”ï¼ˆGraphEvoï¼‰ä¼˜åŒ–æ¡†æ¶ã€‚
    é€šè¿‡â€œå¤šæ ·åˆå§‹åŒ– â†’ é€èŠ‚ç‚¹ç²¾ç‚¼ â†’ æ¨¡å—åŒ–å‹ç¼© â†’ æ–¹æ³•æ± è¿›åŒ–â€çš„é—­ç¯æµç¨‹ï¼Œå®ç°å›¾ç»“æ„çš„è‡ªè¿›åŒ–ä¼˜åŒ–ã€‚
    """

    def __init__(
        self,
        adaptoflux_instance,
        num_initial_models: int = 5,
        max_refinement_steps: int = 100,
        compression_threshold: float = 0.95,
        max_init_layers: int = 3,
        init_mode: str = "fixed",
        init_layers_list: Optional[List[int]] = None,
        frozen_nodes: Optional[List[str]] = None,
        frozen_methods: Optional[List[str]] = None,
        refinement_strategy: str = "random_single",
        candidate_pool_mode: str = "group",
        fallback_mode: Optional[str] = None,
        enable_compression: bool = True,
        enable_evolution: bool = True,
        evolution_sampling_frequency: int = 1,
        evolution_trigger_count: int = 3,
        evolution_cleanup_mode: str = "full",
        methods_per_evolution: int = 1,
        verbose: bool = True,
        **kwargs
    ):
        """
        åˆå§‹åŒ– GraphEvoTrainerï¼Œç”¨äºæ‰§è¡Œ AdaptoFlux çš„å›¾ç»“æ„è‡ªè¿›åŒ–ä¼˜åŒ–æµç¨‹ã€‚

        æœ¬è®­ç»ƒå™¨é€šè¿‡å››ä¸ªæ ¸å¿ƒé˜¶æ®µå®ç°å›¾ç»“æ„çš„è¿­ä»£ä¼˜åŒ–ï¼š
        1. **å¤šæ ·åˆå§‹åŒ–**ï¼šç”Ÿæˆå¤šä¸ªéšæœºåˆå§‹å›¾ï¼Œé€‰æ‹©æ€§èƒ½æœ€ä¼˜è€…ä½œä¸ºèµ·ç‚¹ï¼›
        2. **é€èŠ‚ç‚¹ç²¾ç‚¼**ï¼šå¯¹å›¾ä¸­å¯ä¼˜åŒ–èŠ‚ç‚¹å°è¯•æ–¹æ³•æ›¿æ¢ï¼Œå±€éƒ¨æœç´¢æ›´ä¼˜ç»“æ„ï¼›
        3. **æ–¹æ³•æ± è¿›åŒ–**ï¼ˆå¯é€‰ï¼‰ï¼šåŸºäºè®°å½•çš„é«˜æ€§èƒ½å›¾ç»“æ„ï¼ŒæŠ½è±¡æ–°æ–¹æ³•æ³¨å…¥æ–¹æ³•æ± ã€‚
        4. **æ¨¡å—åŒ–å‹ç¼©**ï¼ˆå¯é€‰ï¼‰ï¼šè¯†åˆ«å¹¶æ›¿æ¢ç­‰æ•ˆçš„é«˜æ•ˆå­å›¾ï¼Œå®ç°ç»“æ„ç®€åŒ–ï¼›
        

        å‚æ•°æ§åˆ¶è¯´æ˜ï¼š
        - **åˆå§‹åŒ–æ§åˆ¶**ï¼šé€šè¿‡ `num_initial_models`ã€`init_mode` ç­‰æ§åˆ¶åˆå§‹å¤šæ ·æ€§ï¼›
        - **ç²¾ç‚¼æ§åˆ¶**ï¼šé€šè¿‡ `refinement_strategy`ã€`frozen_nodes` ç­‰æ§åˆ¶å±€éƒ¨æœç´¢è¡Œä¸ºï¼›
        - **è¿›åŒ–æ§åˆ¶**ï¼šé€šè¿‡ `evolution_sampling_frequency`ã€`evolution_trigger_count` ç­‰æ§åˆ¶è¿›åŒ–è§¦å‘æœºåˆ¶ã€‚
        - **å‹ç¼©æ§åˆ¶**ï¼šé€šè¿‡ `enable_compression`ã€`compression_threshold` æ§åˆ¶æ˜¯å¦å¯ç”¨åŠç­‰æ•ˆæ€§æ ‡å‡†ï¼›
        :param adaptoflux_instance: å·²åˆå§‹åŒ–çš„ AdaptoFlux å®ä¾‹ï¼Œä½œä¸ºä¼˜åŒ–çš„åŸºç¡€æ¨¡æ¿ã€‚
        :param num_initial_models: å¤šæ ·åˆå§‹åŒ–é˜¶æ®µç”Ÿæˆçš„å€™é€‰æ¨¡å‹æ•°é‡ã€‚
        :param max_refinement_steps: å•æ¬¡ç²¾ç‚¼é˜¶æ®µå…è®¸çš„æœ€å¤§ä¼˜åŒ–æ­¥æ•°ã€‚
        :param compression_threshold: æ¨¡å—åŒ–å‹ç¼©é˜¶æ®µåˆ¤å®šå­å›¾ç­‰æ•ˆçš„ MSE ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå€¼è¶Šå°è¦æ±‚è¶Šä¸¥æ ¼ï¼‰ã€‚
        :param max_init_layers: åˆå§‹åŒ–æ—¶æ¯ä¸ªå€™é€‰æ¨¡å‹æœ€å¤šæ·»åŠ çš„éšæœºå±‚æ•°ï¼ˆä»…åœ¨ init_mode="fixed" æ—¶ç”Ÿæ•ˆï¼‰ã€‚
        :param init_mode: åˆå§‹åŒ–æ¨¡å¼ï¼Œ"fixed" è¡¨ç¤ºæ‰€æœ‰æ¨¡å‹æ·»åŠ ç›¸åŒå±‚æ•°ï¼Œ"list" è¡¨ç¤ºæŒ‰ init_layers_list æŒ‡å®šã€‚
        :param init_layers_list: å½“ init_mode="list" æ—¶ï¼ŒæŒ‡å®šæ¯ä¸ªå€™é€‰æ¨¡å‹çš„åˆå§‹åŒ–å±‚æ•°ï¼Œé•¿åº¦éœ€ â‰¥ num_initial_modelsã€‚
        :param frozen_nodes: æ˜¾å¼å†»ç»“çš„èŠ‚ç‚¹åç§°åˆ—è¡¨ï¼ˆå¦‚ ["root", "collapse"]ï¼‰ï¼Œè¿™äº›èŠ‚ç‚¹åœ¨ç²¾ç‚¼é˜¶æ®µä¸ä¼šè¢«ä¿®æ”¹ã€‚
        :param frozen_methods: æ˜¾å¼å†»ç»“çš„æ–¹æ³•åç§°åˆ—è¡¨ï¼ˆå¦‚ ["return_value"]ï¼‰ï¼Œä½¿ç”¨è¿™äº›æ–¹æ³•çš„èŠ‚ç‚¹å°†è‡ªåŠ¨è¢«å†»ç»“ã€‚
        :param refinement_strategy: ç²¾ç‚¼ç­–ç•¥ï¼Œ"random_single"ï¼ˆéšæœºå•ç‚¹ä¼˜åŒ–ï¼‰æˆ– "full_sweep"ï¼ˆå…¨å›¾éå†ä¼˜åŒ–ï¼‰ã€‚
        :param candidate_pool_mode: æ„å»ºå€™é€‰æ–¹æ³•æ± çš„ç­–ç•¥ï¼Œ"all"ï¼ˆæ‰€æœ‰æ–¹æ³•ï¼‰ã€"group"ï¼ˆåŒç»„æ–¹æ³•ï¼‰æˆ– "self"ï¼ˆä»…è‡ªèº«ï¼‰ã€‚
        :param fallback_mode: å½“æ— ç±»å‹å…¼å®¹æ–¹æ³•æ—¶çš„å…œåº•ç­–ç•¥ï¼Œ"all"/"group_first"/"self"/"error"ã€‚
        :param enable_compression: æ˜¯å¦å¯ç”¨æ¨¡å—åŒ–å‹ç¼©é˜¶æ®µã€‚
        :param enable_evolution: æ˜¯å¦å¯ç”¨æ–¹æ³•æ± è¿›åŒ–é˜¶æ®µã€‚
        :param evolution_sampling_frequency: æ¯éš”å¤šå°‘ä¸ªè®­ç»ƒè½®æ¬¡ï¼ˆå³ä¸€æ¬¡å®Œæ•´çš„ã€Œç²¾ç‚¼+å‹ç¼©ã€å¾ªç¯ï¼‰è®°å½•ä¸€æ¬¡å½“å‰å›¾ç»“æ„å¿«ç…§ï¼Œç”¨äºåç»­æ–¹æ³•æ± è¿›åŒ–ã€‚ä¾‹å¦‚è®¾ä¸º 2 è¡¨ç¤ºæ¯ 2 è½®ä¿å­˜ä¸€æ¬¡å¿«ç…§ã€‚
        :param evolution_trigger_count: å½“ç´¯è®¡è®°å½•çš„å›¾å¿«ç…§æ•°é‡è¾¾åˆ°æ­¤å€¼æ—¶ï¼Œè§¦å‘ä¸€æ¬¡æ–¹æ³•æ± è¿›åŒ–ã€‚
        :param evolution_cleanup_mode: è¿›åŒ–å®Œæˆåå¦‚ä½•æ¸…ç†å·²è®°å½•çš„å¿«ç…§ï¼Œ"full"ï¼ˆæ¸…ç©ºå…¨éƒ¨ï¼‰æˆ– "oldest"ï¼ˆä»…ç§»é™¤æœ€æ—©çš„ä¸€ä¸ªï¼‰ã€‚
        :param methods_per_evolution: æ¯æ¬¡æ–¹æ³•æ± è¿›åŒ–æ—¶ï¼Œæœ€å¤šä»è®°å½•çš„å›¾ç»“æ„ä¸­æŠ½è±¡å¹¶æ·»åŠ çš„æ–°æ–¹æ³•æ•°é‡ã€‚
        :param verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—ä¿¡æ¯ã€‚
        :param kwargs: å…¶ä»–å¯é€‰ç»„ä»¶ï¼Œå¦‚è‡ªå®šä¹‰çš„ subgraph_samplerã€io_extractor ç­‰ã€‚
        """

        super().__init__(adaptoflux_instance)
        self.num_initial_models = num_initial_models
        self.max_refinement_steps = max_refinement_steps
        self.compression_threshold = compression_threshold
        self.max_init_layers = max_init_layers
        self.init_mode = init_mode
        self.init_layers_list = init_layers_list
        self.frozen_nodes = set(frozen_nodes) if frozen_nodes else set()
        self.frozen_methods = set(frozen_methods) if frozen_methods else set()  # <-- ä¿å­˜ä¸ºé›†åˆ
        self.candidate_pool_mode = candidate_pool_mode
        self.fallback_mode = fallback_mode or candidate_pool_mode  # é»˜è®¤åŒ pool_mode
        self.refinement_strategy = refinement_strategy
        self.enable_compression = enable_compression
        self.enable_evolution = enable_evolution
        self.evolution_sampling_frequency = evolution_sampling_frequency
        self.evolution_trigger_count = evolution_trigger_count
        self.evolution_cleanup_mode = evolution_cleanup_mode
        self.methods_per_evolution = methods_per_evolution    
        self.verbose = verbose

        self.subgraph_sampler = kwargs.get('subgraph_sampler') or BFSSubgraphSampler(max_nodes=4)
        self.io_extractor = kwargs.get('io_extractor') or SubgraphIOExtractor()
        self.replacer = kwargs.get('replacer') or SubgraphReplacer()
        self.equivalence_checker = kwargs.get('equivalence_checker') or MSEEquivalenceChecker(
            threshold=self.compression_threshold
        )
        

        # æ ¡éªŒå‚æ•°
        if self.methods_per_evolution < 1:
            raise ValueError("methods_per_evolution must be >= 1")
        if self.init_mode == "list":
            if self.init_layers_list is None:
                raise ValueError("init_mode='list' requires init_layers_list to be provided.")
            if len(self.init_layers_list) < self.num_initial_models:
                raise ValueError(f"init_layers_list length ({len(self.init_layers_list)}) must be >= num_initial_models ({self.num_initial_models})")

        # ç”¨äºè®°å½•å®Œæ•´å›¾ç»“æ„å¿«ç…§ï¼ˆç”¨äºæ–¹æ³•æ± è¿›åŒ–ï¼‰
        self.graph_snapshots: List[Any] = []

        
        self._strategy_map = {
            "random_single": self._refine_random_single_step,
            "full_sweep": self._refine_full_sweep_step,
            # æœªæ¥å¯åŠ ï¼š"weighted_sample": self._refine_weighted_sample_step,
        }
        if self.refinement_strategy not in self._strategy_map:
            raise ValueError(f"Unknown refinement_strategy: {self.refinement_strategy}. "
                            f"Available: {list(self._strategy_map.keys())}")

        # æ ¡éªŒ
        valid_pool_modes = {"all", "group", "self"}
        valid_fallback_modes = {"all", "group_first", "self", "error"}
        if self.candidate_pool_mode not in valid_pool_modes:
            raise ValueError(f"Invalid candidate_pool_mode: {self.candidate_pool_mode}")
        if self.fallback_mode not in valid_fallback_modes:
            raise ValueError(f"Invalid fallback_mode: {self.fallback_mode}")

    def _phase_diverse_initialization(self, input_data: np.ndarray, target: np.ndarray) -> Dict[str, Any]:
        """
        é˜¶æ®µä¸€ï¼šå¤šæ ·åˆå§‹åŒ– (Diverse Initialization)
        éšæœºç”Ÿæˆå¤šç»„åˆå§‹æ¨¡å‹ï¼Œé€šè¿‡å¿«é€Ÿè¯„ä¼°ç­›é€‰å‡ºæœ€ä¼˜è€…ä½œä¸ºä¼˜åŒ–èµ·ç‚¹ã€‚

        :param input_data: ç”¨äºè¯„ä¼°çš„è¾“å…¥æ•°æ®
        :param target: å¯¹åº”çš„æ ‡ç­¾
        :return: åŒ…å«æœ€ä¼˜æ¨¡å‹ä¿¡æ¯çš„å­—å…¸ {'best_model': AdaptoFluxå®ä¾‹, 'best_loss': float, 'best_acc': float}
        """
        if self.verbose:
            logger.info(f"[Phase 1] Diverse Initialization: Generating {self.num_initial_models} candidate models...")

        candidates = []
        for i in range(self.num_initial_models):
            # åˆ›å»ºä¸€ä¸ªæ–°çš„ã€ç‹¬ç«‹çš„ AdaptoFlux å®ä¾‹å‰¯æœ¬
            # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ AdaptoFlux ç±»æœ‰ä¸€ä¸ª `clone()` æ–¹æ³•æˆ–ç±»ä¼¼çš„æœºåˆ¶
            # å¦‚æœæ²¡æœ‰ï¼Œæ‚¨éœ€è¦å®ç°ä¸€ä¸ªæ·±æ‹·è´é€»è¾‘ï¼Œç¡®ä¿ graph å’Œ methods éƒ½è¢«å¤åˆ¶
            try:
                candidate_af = self.adaptoflux.clone()
            except AttributeError:
                # å¦‚æœæ²¡æœ‰ clone æ–¹æ³•ï¼Œåˆ™è¿›è¡Œæ·±æ‹·è´
                candidate_af = copy.deepcopy(self.adaptoflux)
                # é‡ç½®å…¶å†…éƒ¨çŠ¶æ€ï¼Œç¡®ä¿ç‹¬ç«‹æ€§
                candidate_af.graph_processor.graph = copy.deepcopy(self.adaptoflux.graph_processor.graph)
                candidate_af.methods = copy.deepcopy(self.adaptoflux.methods)

            # æ ¹æ®åˆå§‹åŒ–æ¨¡å¼å†³å®šæ·»åŠ å±‚æ•°
            if self.init_mode == "fixed":
                layers_to_add = self.max_init_layers
            elif self.init_mode == "list":
                layers_to_add = self.init_layers_list[i]  # ç¬¬ i ä¸ªå€™é€‰æ¨¡å‹ä½¿ç”¨åˆ—è¡¨ä¸­ç¬¬ i é¡¹
            else:
                raise ValueError(f"Unsupported init_mode: {self.init_mode}")

            # å¯¹å€™é€‰æ¨¡å‹è¿›è¡Œéšæœºåˆå§‹åŒ–
            self._randomly_initialize_graph(candidate_af, num_layers_to_add=layers_to_add)

            # è¯„ä¼°å€™é€‰æ¨¡å‹
            loss = self._evaluate_loss_with_instance(candidate_af, input_data, target)
            acc = self._evaluate_accuracy_with_instance(candidate_af, input_data, target)

            candidates.append({
                'model': candidate_af,
                'loss': loss,
                'accuracy': acc,
                'id': i
            })

            if self.verbose:
                logger.info(f"  Candidate {i+1}/{self.num_initial_models}: Loss={loss:.6f}, Acc={acc:.4f}")

        # é€‰æ‹©æŸå¤±æœ€ä½çš„æ¨¡å‹ä½œä¸ºæœ€ä¼˜æ¨¡å‹
        best_candidate = min(candidates, key=lambda x: x['loss'])

        if self.verbose:
            logger.info(f"[Phase 1] Selected best initial model (ID: {best_candidate['id']}) with Loss={best_candidate['loss']:.6f}, Acc={best_candidate['accuracy']:.4f}")

        return {
            'best_model': best_candidate['model'],
            'best_loss': best_candidate['loss'],
            'best_accuracy': best_candidate['accuracy']
        }

    def _randomly_initialize_graph(self, adaptoflux_instance, num_layers_to_add: int = 3):
        """
        ä¸€ä¸ªè¾…åŠ©æ–¹æ³•ï¼Œç”¨äºå¯¹ç»™å®šçš„ AdaptoFlux å®ä¾‹è¿›è¡Œéšæœºçš„å›¾ç»“æ„åˆå§‹åŒ–ã€‚
        é€šè¿‡è°ƒç”¨å…¶ `append_nx_layer` æ–¹æ³•éšæœºæ·»åŠ å‡ å±‚ã€‚

        :param adaptoflux_instance: è¦åˆå§‹åŒ–çš„ AdaptoFlux å®ä¾‹
        :param num_layers_to_add: è¦æ·»åŠ çš„å±‚æ•°
        """
        for _ in range(num_layers_to_add):
            candidate_plan = adaptoflux_instance.process_random_method()
            if candidate_plan["valid_groups"]:
                try:
                    adaptoflux_instance.append_nx_layer(
                        candidate_plan,
                        discard_unmatched='to_discard',
                        discard_node_method_name="null"
                    )
                except Exception as e:
                    logger.warning(f"Failed to add random layer during initialization: {e}")
                    # å¦‚æœæ·»åŠ å¤±è´¥ï¼Œè·³è¿‡ï¼Œä¸å½±å“æ•´ä½“æµç¨‹

    def _phase_node_refinement(self, adaptoflux_instance, input_data: np.ndarray, target: np.ndarray) -> Dict[str, Any]:
        """
        é˜¶æ®µäºŒï¼šé€èŠ‚ç‚¹ç²¾ç‚¼ (Node-wise Refinement)
        
        ç›®æ ‡ï¼š
            å¯¹å›¾ä¸­æ¯ä¸ªâ€œå¯å¤„ç†èŠ‚ç‚¹â€ï¼ˆprocessing nodeï¼‰ï¼Œå°è¯•ç”¨å…¼å®¹çš„æ›¿ä»£æ–¹æ³•è¿›è¡Œæ›¿æ¢ï¼Œ
            ä»¥é™ä½æŸå¤±ï¼ˆæˆ–æå‡å‡†ç¡®ç‡ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ª**å±€éƒ¨æœç´¢ä¼˜åŒ–è¿‡ç¨‹**ã€‚
        
        ç­–ç•¥ï¼š
            æ”¯æŒå¤šç§ä¼˜åŒ–ç­–ç•¥ï¼ˆå¦‚éšæœºå•ç‚¹ã€å®Œæ•´éå†ï¼‰ï¼Œç”± `self.refinement_strategy` æ§åˆ¶ã€‚
            æ¯æ¬¡æ›¿æ¢éƒ½åŸºäºå°æ‰¹é‡æ•°æ®å¿«é€Ÿè¯„ä¼°ï¼Œé¿å…å…¨é‡è®¡ç®—å¼€é”€ã€‚
        
        å†»ç»“æœºåˆ¶ï¼š
            - `frozen_nodes`: æ˜¾å¼å†»ç»“çš„èŠ‚ç‚¹ååˆ—è¡¨ï¼ˆå¦‚ "root", "collapse"ï¼‰
            - `frozen_methods`: æ˜¾å¼å†»ç»“çš„æ–¹æ³•ååˆ—è¡¨ï¼ˆå¦‚ ["return_value"]ï¼‰ï¼Œè‡ªåŠ¨å†»ç»“ä½¿ç”¨è¿™äº›æ–¹æ³•çš„èŠ‚ç‚¹
        
        è¿”å›ï¼š
            åŒ…å«æœ€ç»ˆæ¨¡å‹ã€æ€§èƒ½æŒ‡æ ‡ã€æ­¥æ•°ç­‰ä¿¡æ¯çš„å­—å…¸ã€‚
        """
        # æ‰“å°æ—¥å¿—ï¼šå¼€å§‹ç²¾ç‚¼é˜¶æ®µ
        if self.verbose:
            logger.info(f"[Phase 2] Node-wise Refinement: Starting refinement with strategy '{self.refinement_strategy}'...")

        # è·å–å›¾å¤„ç†å™¨å’Œå½“å‰æ€§èƒ½æŒ‡æ ‡
        gp = adaptoflux_instance.graph_processor
        current_loss = self._evaluate_loss_with_instance(adaptoflux_instance, input_data, target)
        current_acc = self._evaluate_accuracy_with_instance(adaptoflux_instance, input_data, target)

        # åˆå§‹åŒ–çŠ¶æ€å˜é‡
        improvement_made = False  # æ ‡è®°æœ¬è½®æ˜¯å¦å‘ç”Ÿè¿‡æ”¹è¿›
        steps_taken = 0           # å®é™…æ‰§è¡Œçš„â€œä¼˜åŒ–æ­¥æ•°â€ï¼ˆæ³¨æ„ï¼šä¸åŒç­–ç•¥æ­¥æ•°å®šä¹‰ä¸åŒï¼‰

        # === 1. è·å–æ‰€æœ‰â€œå¯å¤„ç†èŠ‚ç‚¹â€å¹¶åº”ç”¨å†»ç»“è§„åˆ™ ===
        # `_is_processing_node(node)` æ˜¯ GraphProcessor çš„æ–¹æ³•ï¼Œç”¨äºåˆ¤æ–­èŠ‚ç‚¹æ˜¯å¦æ˜¯â€œä¸­é—´è®¡ç®—èŠ‚ç‚¹â€
        # ï¼ˆé€šå¸¸æ’é™¤ "root"ã€"collapse" ç­‰ç‰¹æ®ŠèŠ‚ç‚¹ï¼‰
        processing_nodes = [node for node in gp.graph.nodes() if gp._is_processing_node(node)]

        # åˆå¹¶æ˜¾å¼å†»ç»“çš„èŠ‚ç‚¹å’ŒåŸºäºæ–¹æ³•åå†»ç»“çš„èŠ‚ç‚¹
        final_frozen_nodes = set(self.frozen_nodes)  # æ˜¾å¼å†»ç»“çš„èŠ‚ç‚¹åé›†åˆ

        # å¦‚æœç”¨æˆ·æŒ‡å®šäº†å†»ç»“çš„æ–¹æ³•åï¼ˆå¦‚ ["return_value"]ï¼‰ï¼Œåˆ™è‡ªåŠ¨å†»ç»“æ‰€æœ‰ä½¿ç”¨è¿™äº›æ–¹æ³•çš„èŠ‚ç‚¹
        if self.frozen_methods:
            method_frozen_nodes = {
                node for node in processing_nodes
                if gp.graph.nodes[node].get('method_name') in self.frozen_methods
            }
            final_frozen_nodes.update(method_frozen_nodes)  # åˆå¹¶åˆ°å†»ç»“é›†åˆ

        # ä»å¯å¤„ç†èŠ‚ç‚¹ä¸­ç§»é™¤æ‰€æœ‰å†»ç»“èŠ‚ç‚¹
        if final_frozen_nodes:
            processing_nodes = [node for node in processing_nodes if node not in final_frozen_nodes]

        # æ‰“å°æ—¥å¿—ï¼šæŠ¥å‘Šå¯ä¼˜åŒ–èŠ‚ç‚¹æ•°é‡
        if self.verbose:
            logger.info(f"  Found {len(processing_nodes)} processing nodes to refine "
                        f"(excluding {len(final_frozen_nodes)} frozen nodes).")

        # === 2. è·å–ä¼˜åŒ–ç­–ç•¥å‡½æ•° ===
        # `_strategy_map` å°†å­—ç¬¦ä¸²ç­–ç•¥åæ˜ å°„åˆ°å…·ä½“å®ç°å‡½æ•°
        strategy_func = self._strategy_map[self.refinement_strategy]

        # === 3. ä¸»ä¼˜åŒ–å¾ªç¯ ===
        for step in range(self.max_refinement_steps):
            # å®‰å…¨é€€å‡ºï¼šé˜²æ­¢è¶…è¿‡æœ€å¤§æ­¥æ•°ï¼ˆè™½ç„¶ç­–ç•¥å‡½æ•°å¯èƒ½ä¸€æ¬¡æ‰§è¡Œå¤šæ­¥ï¼‰
            if steps_taken >= self.max_refinement_steps:
                break

            # è‹¥æ— å¯ä¼˜åŒ–èŠ‚ç‚¹ï¼Œæå‰é€€å‡º
            if not processing_nodes:
                break

            # è°ƒç”¨å…·ä½“ç­–ç•¥å‡½æ•°æ‰§è¡Œä¸€æ¬¡ä¼˜åŒ–å°è¯•
            # è¿”å›å€¼è¯´æ˜ï¼ˆè§ç­–ç•¥å‡½æ•°æ–‡æ¡£ï¼‰ï¼š
            #   imp: bool â†’ æ˜¯å¦å‘ç”Ÿæ”¹è¿›
            #   new_loss, new_acc: æ”¹è¿›åçš„æŒ‡æ ‡
            #   step_inc: int â†’ æœ¬æ¬¡å®é™…æ¶ˆè€—çš„â€œæ­¥æ•°â€ï¼ˆå¦‚ full_sweep ä¸€æ¬¡å¯èƒ½æ›¿æ¢å¤šä¸ªèŠ‚ç‚¹ï¼‰
            #   updated_nodes: List[str] â†’ æ›´æ–°åçš„å¯å¤„ç†èŠ‚ç‚¹åˆ—è¡¨ï¼ˆå›¾ç»“æ„å¯èƒ½å˜åŒ–ï¼‰
            imp, new_loss, new_acc, step_inc, updated_nodes = strategy_func(
                adaptoflux_instance, input_data, target, processing_nodes, current_loss, gp
            )

            if imp:
                # å¦‚æœå‘ç”Ÿæ”¹è¿›ï¼Œæ›´æ–°å…¨å±€çŠ¶æ€
                improvement_made = True
                current_loss = new_loss
                current_acc = new_acc
                steps_taken += step_inc
                processing_nodes = updated_nodes  # ä½¿ç”¨æœ€æ–°èŠ‚ç‚¹åˆ—è¡¨

                # æ³¨æ„ï¼šæ—¥å¿—å·²åœ¨ç­–ç•¥å‡½æ•°å†…éƒ¨æ‰“å°ï¼ˆé¿å…é‡å¤ï¼‰ï¼Œæ‰€ä»¥è¿™é‡Œä¸åšé¢å¤–è¾“å‡º
            else:
                # æ— æ”¹è¿›ï¼šç»§ç»­ä¸‹ä¸€è½®å°è¯•ï¼ˆä¸æå‰ç»ˆæ­¢ï¼Œå› ä¸ºåç»­å¯èƒ½æœ‰æ”¹è¿›ï¼‰
                # ä¹Ÿå¯åœ¨æ­¤å¤„åŠ å…¥â€œæ—©åœâ€é€»è¾‘ï¼ˆå¦‚è¿ç»­ N æ¬¡æ— æ”¹è¿›åˆ™é€€å‡ºï¼‰
                pass

        # === 4. æ‰“å°æœ€ç»ˆç»“æœæ—¥å¿— ===
        if self.verbose:
            if improvement_made:
                logger.info(f"[Phase 2] Refinement completed in {steps_taken} steps. Final Loss: {current_loss:.6f}, Acc: {current_acc:.4f}")
            else:
                logger.info(f"[Phase 2] Refinement completed. No improvements found within {self.max_refinement_steps} steps.")

        # === 5. è¿”å›ç»“æœ ===
        return {
            'final_model': adaptoflux_instance,      # ä¼˜åŒ–åçš„æ¨¡å‹ï¼ˆåŸåœ°ä¿®æ”¹ï¼‰
            'final_loss': current_loss,              # æœ€ç»ˆæŸå¤±
            'final_accuracy': current_acc,           # æœ€ç»ˆå‡†ç¡®ç‡
            'steps_taken': steps_taken,              # å®é™…æ‰§è¡Œæ­¥æ•°
            'improvement_made': improvement_made     # æ˜¯å¦æœ‰æ”¹è¿›
        }

    def _get_compatible_methods_for_node(
        self, 
        adaptoflux_instance, 
        node_name: str, 
    ) -> List[str]:
        """
        è·å–ä¸å›¾ä¸­æŒ‡å®šèŠ‚ç‚¹å…¼å®¹çš„å€™é€‰æ–¹æ³•åˆ—è¡¨ã€‚

        æ–°å¢å‚æ•°:
            allow_fallback_on_empty (bool): 
                å½“ç±»å‹å…¼å®¹æ–¹æ³•ä¸ºç©ºæ—¶ï¼Œæ˜¯å¦å…è®¸å›é€€åˆ°éç±»å‹å®‰å…¨çš„å…œåº•ç­–ç•¥ã€‚
                - Trueï¼ˆé»˜è®¤ï¼‰ï¼šå¯ç”¨å…œåº•ï¼Œä¿è¯è¿”å›éç©ºåˆ—è¡¨ï¼›
                - Falseï¼šè‹¥æ— ç±»å‹å…¼å®¹æ–¹æ³•ï¼Œç›´æ¥æŠ›å‡º RuntimeErrorã€‚

        å…¶ä½™å‚æ•°è¯´æ˜è§åŸæ³¨é‡Šã€‚
        """
        
        gp = adaptoflux_instance.graph_processor
        methods = adaptoflux_instance.methods
        all_method_names = list(methods.keys())

        if not all_method_names:
            return []

        # === 1. è·å–åŸå§‹æ–¹æ³• ===
        node_data = gp.graph.nodes[node_name]
        original_method_name = node_data.get("method_name")
        if original_method_name is None or original_method_name not in methods:
            logger.warning(
                "Node '%s' has no valid 'method_name'; falling back to all methods.",
                node_name
            )
            return all_method_names

        # === 2. æå–åŸå§‹ç±»å‹ ===
        orig_info = methods[original_method_name]
        orig_input_types = orig_info.get("input_types", []) or []
        orig_output_types = orig_info.get("output_types", []) or []
        def is_type_compatible(method_name: str) -> bool:
            info = methods[method_name]
            m_input = info.get("input_types", []) or []
            m_output = info.get("output_types", []) or []
            return m_input == orig_input_types and m_output == orig_output_types

        # === 3. æ„å»ºå€™é€‰æ±  ===
        if self.candidate_pool_mode == "all":
            candidate_pool = all_method_names
        elif self.candidate_pool_mode == "self":
            candidate_pool = [original_method_name]
        else:  # "group"
            node_group = methods[original_method_name].get("group", "default")
            candidate_pool = [
                name for name, info in methods.items()
                if info.get("group", "default") == node_group
            ]

        # === 4. ç­›é€‰ç±»å‹å…¼å®¹æ–¹æ³• ===
        compatible_methods = [name for name in candidate_pool if is_type_compatible(name)]
        # print(f"Node '{node_name}' compatible methods: {compatible_methods}")

        # === 5. å¤„ç†ç©ºç»“æœ ===
        if not compatible_methods:
            log_msg = (f"Node '{node_name}' has no compatible methods.")
            logger.debug(log_msg)

            if self.fallback_mode == "error":
                raise RuntimeError(f"Strict mode: no type-compatible methods for node '{node_name}'...")

            # === æ‰§è¡Œå…œåº•å›é€€ ===
            if self.fallback_mode == "all":
                result = all_method_names
            elif self.fallback_mode == "group_first":
                node_group = methods[original_method_name].get("group", "default")
                group_methods = [name for name, info in methods.items() if info.get("group") == node_group]
                result = group_methods[:1] if group_methods else all_method_names[:1]
            elif self.fallback_mode == "self":
                result = [original_method_name]  # ğŸ‘ˆ å…³é”®ï¼šåªè¿”å›è‡ªå·±
            else:
                result = all_method_names  # fallback

            logger.warning("Falling back to non-type-safe methods for node '%s'...", node_name)
            return result

        return compatible_methods

    def _phase_modular_compression(self, adaptoflux_instance, input_data: np.ndarray, target: np.ndarray) -> Dict[str, Any]:
        """
        é˜¶æ®µå››ï¼šæ¨¡å—åŒ–å‹ç¼© (Modular Compression)
        è¯†åˆ«å›¾ä¸­å¯è¢«æ›¿æ¢çš„é«˜æ•ˆå­å›¾ï¼Œç”¨æ›´å°æˆ–æ›´å¿«çš„ç­‰æ•ˆç»“æ„è¿›è¡Œæ›¿ä»£ã€‚

        æœ¬é˜¶æ®µæ‰§è¡Œä»¥ä¸‹æµç¨‹ï¼š
        1. **å­å›¾é‡‡æ ·**ï¼šä½¿ç”¨é…ç½®çš„é‡‡æ ·å™¨ï¼ˆå¦‚ BFSï¼‰éšæœºé€‰å–ä¸€ä¸ªè¿é€šå­å›¾ï¼›
        2. **I/O æå–**ï¼šæ‰§è¡ŒåŸå›¾ï¼Œè®°å½•è¯¥å­å›¾çš„è¾“å…¥ä¸è¾“å‡ºæ•°æ®ï¼›
        3. **æ›¿ä»£ç”Ÿæˆ**ï¼šï¼ˆå½“å‰ç®€åŒ–ä¸ºï¼‰é€‰æ‹©ä¸€ä¸ªå€™é€‰æ–¹æ³•ï¼ˆå¦‚ "add"ï¼‰ä½œä¸ºæ›¿ä»£ç»“æ„ï¼›
        4. **ç­‰æ•ˆæ€§éªŒè¯**ï¼šæ¯”è¾ƒæ›¿ä»£ç»“æ„ä¸åŸå­å›¾åœ¨ç›¸åŒè¾“å…¥ä¸‹çš„è¾“å‡ºæ˜¯å¦è¶³å¤Ÿç›¸ä¼¼ï¼›
        5. **å›¾ç»“æ„æ›¿æ¢**ï¼šè‹¥éªŒè¯é€šè¿‡ï¼Œåˆ™ç”¨æ›¿ä»£ç»“æ„ï¼ˆå½“å‰ä¸ºå•èŠ‚ç‚¹ï¼‰æ›¿æ¢åŸå­å›¾ï¼›
        6. **è®°å½•é«˜æ€§èƒ½å­å›¾**ï¼šå°†è¢«æ›¿æ¢çš„åŸå­å›¾ä¿å­˜è‡³ `high_performance_subgraphs`ï¼Œä¾›è¿›åŒ–é˜¶æ®µä½¿ç”¨ã€‚

        æ³¨æ„ï¼šå½“å‰å®ç°ä¸­ï¼Œæ›¿ä»£ç»“æ„ä¸º**å•ä¸ªèŠ‚ç‚¹**ï¼Œä¸”å€™é€‰æ–¹æ³•å›ºå®šä¸º "add"ã€‚
        åç»­å¯æ‰©å±•ä¸ºè®­ç»ƒä¸€ä¸ªå°å‹æ›¿ä»£å­å›¾ä»¥å®ç°æ›´ä¼˜å‹ç¼©ã€‚

        :param adaptoflux_instance: å¾…ä¼˜åŒ–çš„ AdaptoFlux å®ä¾‹
        :param input_data: ç”¨äºè¯„ä¼°ç­‰æ•ˆæ€§çš„è¾“å…¥æ•°æ®ï¼ˆå°æ‰¹é‡ï¼‰
        :param target: å¯¹åº”çš„æ ‡ç­¾ï¼ˆç”¨äºæœ€ç»ˆæ€§èƒ½è¯„ä¼°ï¼Œéç­‰æ•ˆæ€§éªŒè¯å¿…éœ€ï¼‰
        :return: åŒ…å«å‹ç¼©åæ¨¡å‹ä¿¡æ¯å’Œå‹ç¼©æƒ…å†µçš„å­—å…¸ï¼Œå­—æ®µåŒ…æ‹¬ï¼š
                 - 'final_model': å‹ç¼©åçš„ AdaptoFlux å®ä¾‹ï¼ˆå¯èƒ½æœªå˜ï¼‰
                 - 'final_loss': å‹ç¼©åçš„æŸå¤±å€¼
                 - 'final_accuracy': å‹ç¼©åçš„å‡†ç¡®ç‡
                 - 'compression_applied': boolï¼Œæ˜¯å¦æˆåŠŸæ‰§è¡Œäº†å‹ç¼©
                 - 'compressed_subgraphs': intï¼Œæœ¬æ¬¡å‹ç¼©çš„å­å›¾æ•°é‡ï¼ˆ0 æˆ– 1ï¼‰
        """
        if not self.enable_compression:
            return self._return_original_result(adaptoflux_instance, input_data, target)

        gp = adaptoflux_instance.graph_processor

        # 1. é‡‡æ ·å­å›¾
        subgraph = self.subgraph_sampler.sample(gp.graph)
        if subgraph is None:
            return self._return_original_result(adaptoflux_instance, input_data, target)

        # 2. æå– I/O
        try:
            sub_inputs, sub_outputs = self.io_extractor.extract(adaptoflux_instance, subgraph, input_data)
        except Exception as e:
            logger.warning(f"IO extraction failed: {e}")
            return self._return_original_result(adaptoflux_instance, input_data, target)

        # 3. ç®€åŒ–ï¼šç›´æ¥é€‰ä¸€ä¸ªå€™é€‰æ–¹æ³•ï¼ˆå¦‚ "add"ï¼‰ä½œä¸ºæ›¿ä»£ï¼ˆè·³è¿‡è®­ç»ƒï¼‰
        candidate_method = "add"  # åç»­å¯æ›¿æ¢ä¸ºè®­ç»ƒé€»è¾‘
        if candidate_method not in adaptoflux_instance.methods:
            return self._return_original_result(adaptoflux_instance, input_data, target)

        # 4. éªŒè¯ç­‰æ•ˆæ€§
        # æ„å»ºä¸´æ—¶å•èŠ‚ç‚¹å›¾å¹¶æµ‹è¯•
        temp_af = self._create_single_node_graph(adaptoflux_instance, candidate_method)
        rep_output = temp_af.infer_with_graph(input_data)
        orig_output = list(sub_outputs.values())[0]

        if not self.equivalence_checker.is_equivalent(orig_output, rep_output):
            return self._return_original_result(adaptoflux_instance, input_data, target)

        # 5. æ‰§è¡Œæ›¿æ¢
        try:
            new_node_id = self.replacer.replace_with_node(gp, subgraph, candidate_method)
            logger.info(f"Replaced subgraph with node '{new_node_id}' ({candidate_method})")

            # è¿”å›æ–°ç»“æœ
            new_loss = self._evaluate_loss_with_instance(adaptoflux_instance, input_data, target)
            new_acc = self._evaluate_accuracy_with_instance(adaptoflux_instance, input_data, target)
            return {
                'final_model': adaptoflux_instance,
                'final_loss': new_loss,
                'final_accuracy': new_acc,
                'compression_applied': True,
                'compressed_subgraphs': 1
            }

        except Exception as e:
            logger.error(f"Replacement failed: {e}")
            return self._return_original_result(adaptoflux_instance, input_data, target)

    def _return_original_result(self, adaptoflux_instance, input_data, target):
        """
        è¾…åŠ©æ–¹æ³•ï¼šè¿”å›æœªè¿›è¡Œä»»ä½•å‹ç¼©çš„åŸå§‹æ¨¡å‹è¯„ä¼°ç»“æœã€‚

        å½“æ¨¡å—åŒ–å‹ç¼©å› ä»¥ä¸‹åŸå› è·³è¿‡æ—¶è°ƒç”¨ï¼š
        - å‹ç¼©åŠŸèƒ½è¢«ç¦ç”¨ï¼ˆenable_compression=Falseï¼‰
        - æœªèƒ½é‡‡æ ·åˆ°æœ‰æ•ˆå­å›¾
        - I/O æå–å¤±è´¥
        - å€™é€‰æ›¿ä»£æ–¹æ³•ä¸å¯ç”¨
        - ç­‰æ•ˆæ€§éªŒè¯æœªé€šè¿‡
        - å›¾æ›¿æ¢è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸

        è¯¥æ–¹æ³•ç¡®ä¿å‹ç¼©é˜¶æ®µå§‹ç»ˆè¿”å›ç»“æ„ä¸€è‡´çš„ç»“æœå­—å…¸ï¼Œä¾¿äºä¸»è®­ç»ƒå¾ªç¯ç»Ÿä¸€å¤„ç†ã€‚

        :param adaptoflux_instance: å½“å‰çš„ AdaptoFlux å®ä¾‹ï¼ˆæœªä¿®æ”¹ï¼‰
        :param input_data: ç”¨äºè¯„ä¼°çš„è¾“å…¥æ•°æ®
        :param target: å¯¹åº”çš„æ ‡ç­¾
        :return: è¡¨ç¤ºâ€œæ— å‹ç¼©â€çš„æ ‡å‡†ç»“æœå­—å…¸
        """
        loss = self._evaluate_loss_with_instance(adaptoflux_instance, input_data, target)
        acc = self._evaluate_accuracy_with_instance(adaptoflux_instance, input_data, target)
        return {
            'final_model': adaptoflux_instance,
            'final_loss': loss,
            'final_accuracy': acc,
            'compression_applied': False,
            'compressed_subgraphs': 0
        }

    def _extract_node_signature(graph, node_id: str) -> Tuple[int, Tuple[int, ...], Tuple[int, ...]]:
        """
        ä»å›¾ä¸­æå–èŠ‚ç‚¹çš„æ‹“æ‰‘ç­¾åï¼š(layer, sorted_in_coords, sorted_out_coords)
        
        :param graph: NetworkX å›¾
        :param node_id: èŠ‚ç‚¹ IDï¼Œæ ¼å¼å¦‚ "L_I_method"
        :return: (layer, in_coords_tuple, out_coords_tuple)
        """
        # 1. æå– layer
        if node_id in ("root", "collapse"):
            raise ValueError("Skip special nodes")
        try:
            layer = int(node_id.split('_', 1)[0])
        except (ValueError, IndexError):
            raise ValueError(f"Cannot parse layer from node ID: {node_id}")

        # 2. æ”¶é›†æ‰€æœ‰è¾“å…¥è¾¹çš„ data_coordï¼ˆæŒ‡å‘è¯¥èŠ‚ç‚¹çš„è¾¹ï¼‰
        in_coords = []
        for src, _, edge_data in graph.in_edges(node_id, data=True):
            coord = edge_data.get('data_coord')
            if coord is not None:
                in_coords.append(coord)
        in_coords = tuple(sorted(in_coords))

        # 3. æ”¶é›†æ‰€æœ‰è¾“å‡ºè¾¹çš„ data_coordï¼ˆä»è¯¥èŠ‚ç‚¹å‡ºå‘çš„è¾¹ï¼‰
        out_coords = []
        for _, dst, edge_data in graph.out_edges(node_id, data=True):
            coord = edge_data.get('data_coord')
            if coord is not None:
                out_coords.append(coord)
        out_coords = tuple(sorted(out_coords))

        return (layer, in_coords, out_coords)
    
    def _phase_method_pool_evolution(
        self,
        adaptoflux_instance,
        snapshots: List[Any],
        max_methods: int = 1
    ) -> Dict[str, Any]:
        # æ‰“å°æ—¥å¿—ï¼šå¼€å§‹æ–¹æ³•æ± è¿›åŒ–é˜¶æ®µï¼Œè¯´æ˜å°†åŸºäºæ‹“æ‰‘ç­¾åå¯¹é½èŠ‚ç‚¹
        # å¯¹é½ä¾æ®ï¼š(å±‚å·, è¾“å…¥æ•°æ®åæ ‡é›†åˆ, è¾“å‡ºæ•°æ®åæ ‡é›†åˆ)
        # è¿™ç§ç­¾åèƒ½å”¯ä¸€æ ‡è¯†èŠ‚ç‚¹åœ¨æ•°æ®æµå›¾ä¸­çš„æ‹“æ‰‘è§’è‰²ï¼Œä¸èŠ‚ç‚¹IDæˆ–æ–¹æ³•åæ— å…³
        if self.verbose:
            logger.info(f"[Phase 3] Method Pool Evolution: Aligning nodes via (layer, in_coords, out_coords) "
                        f"across {len(snapshots)} snapshots...")

        # å®‰å…¨æ£€æŸ¥ï¼šè‹¥æ— å¿«ç…§å¯ä¾›åˆ†æï¼Œç›´æ¥è¿”å›ç©ºç»“æœ
        if not snapshots:
            return {'methods_added': 0, 'new_method_names': []}

        # Step 1: æ„å»ºæ‹“æ‰‘ç­¾ååˆ°æ–¹æ³•é¢‘æ¬¡çš„æ˜ å°„
        # ç»“æ„ï¼šsignature_freq[signature][method_name] = count
        # å…¶ä¸­ signature = (layer, (in_coord1, in_coord2, ...), (out_coord1, out_coord2, ...))
        signature_freq = defaultdict(lambda: defaultdict(int))

        # éå†æ¯ä¸ªå¿«ç…§ï¼ˆå³æ¯æ¬¡ä¿å­˜çš„å›¾ç»“æ„ï¼‰
        for snap in snapshots:
            graph = snap.graph_processor.graph
            # éå†å›¾ä¸­æ¯ä¸ªèŠ‚ç‚¹
            for node_id in graph.nodes():
                try:
                    # æå–è¯¥èŠ‚ç‚¹çš„æ‹“æ‰‘ç­¾åï¼ˆåŸºäºå±‚å·å’Œè¾¹çš„ data_coordï¼‰
                    sig = _extract_node_signature(graph, node_id)
                    # è·å–è¯¥èŠ‚ç‚¹å½“å‰ä½¿ç”¨çš„æ–¹æ³•åï¼Œè‹¥ç¼ºå¤±åˆ™æ ‡è®°ä¸º 'unknown'
                    method = graph.nodes[node_id].get('method_name', 'unknown')
                    # ç´¯åŠ è¯¥æ–¹æ³•åœ¨è¯¥æ‹“æ‰‘ä½ç½®å‡ºç°çš„æ¬¡æ•°
                    signature_freq[sig][method] += 1
                except ValueError:
                    # è·³è¿‡æ— æ³•è§£æçš„èŠ‚ç‚¹ï¼ˆå¦‚ "root"ã€"collapse" ç­‰ç‰¹æ®ŠèŠ‚ç‚¹ï¼‰
                    continue

        # æ—¥å¿—ï¼šæŠ¥å‘Šå…±è¯†åˆ«å‡ºå¤šå°‘ç§å”¯ä¸€çš„æ‹“æ‰‘è§’è‰²ï¼ˆå³ä¸åŒçš„èŠ‚ç‚¹ä½ç½®ï¼‰
        # è¿™åæ˜ äº†å¿«ç…§é—´å›¾ç»“æ„çš„ä¸€è‡´æ€§ç¨‹åº¦
        if self.verbose:
            logger.debug(f"Aligned {len(signature_freq)} unique node roles across snapshots.")

        # Step 2: ï¼ˆå ä½ï¼‰ç”Ÿæˆæ–°æ–¹æ³•
        new_method_names = []
        num_to_add = min(len(snapshots), max_methods)
        for i in range(num_to_add):
            name = f"evolved_method_{len(adaptoflux_instance.methods) + i + 1}"
            adaptoflux_instance.methods[name] = {
                'output_count': 1,
                'input_types': ['scalar'],
                'output_types': ['scalar'],
                'group': 'evolved',
                'weight': 1.0,
                'vectorized': True,
                'is_evolved': True,
                'aligned_roles': len(signature_freq)
            }
            new_method_names.append(name)

        return {
            'methods_added': len(new_method_names),
            'new_method_names': new_method_names
        }
    def _evaluate_loss_with_instance(self, adaptoflux_instance, input_data: np.ndarray, target: np.ndarray) -> float:
        """
        è¾…åŠ©æ–¹æ³•ï¼šä½¿ç”¨æŒ‡å®šçš„ AdaptoFlux å®ä¾‹è®¡ç®—æŸå¤±ã€‚
        """
        try:
            output = adaptoflux_instance.infer_with_graph(values=input_data)
            loss = self.loss_fn(output, target)
            return float(loss)
        except Exception as e:
            logger.error(f"Evaluation failed for instance: {e}")
            logger.error(traceback.format_exc())
            raise RuntimeError("Failed to evaluate loss with the given AdaptoFlux instance.")

    def _evaluate_accuracy_with_instance(self, adaptoflux_instance, input_data: np.ndarray, target: np.ndarray) -> float:
        """
        è¾…åŠ©æ–¹æ³•ï¼šä½¿ç”¨æŒ‡å®šçš„ AdaptoFlux å®ä¾‹è®¡ç®—å‡†ç¡®ç‡ã€‚
        """
        try:
            output = adaptoflux_instance.infer_with_graph(values=input_data)
            output = np.array(output)
            if len(output.shape) == 1 or output.shape[1] == 1:
                pred_classes = (output >= 0.5).astype(int).flatten()
            else:
                pred_classes = np.argmax(output, axis=1)
            true_labels = np.array(target).flatten()
            accuracy = float(np.mean(pred_classes == true_labels))
            return accuracy
        except Exception as e:
            logger.error(f"Accuracy evaluation failed for instance: {e}")
            raise RuntimeError("Failed to evaluate loss with the given AdaptoFlux instance.")

    def train(
        self,
        input_data: np.ndarray,
        target: np.ndarray,
        max_evo_cycles: int = 5,
        save_model: bool = True,
        model_save_path: Optional[str] = None,
        save_best_model: bool = True,
        best_model_subfolder: str = "best",
        final_model_subfolder: str = "final",
        **kwargs
    ) -> dict:
        """
        å®ç°åŸºç±»çš„ train æ–¹æ³•ã€‚
        æ‰§è¡Œå®Œæ•´çš„â€œå›¾æ¼”â€ä¼˜åŒ–å¾ªç¯ã€‚

        :param input_data: ç”¨äºå¿«é€Ÿè¯„ä¼°çš„è¾“å…¥æ•°æ®ï¼ˆå°æ‰¹é‡ï¼‰
        :param target: å¯¹åº”çš„æ ‡ç­¾
        :param max_evo_cycles: æœ€å¤šæ‰§è¡Œçš„è®­ç»ƒè½®æ¬¡æ•°ã€‚
        :param save_model: æ˜¯å¦åœ¨è®­ç»ƒç»“æŸåä¿å­˜æ¨¡å‹
        :param model_save_path: æ¨¡å‹ä¿å­˜çš„æ–‡ä»¶å¤¹è·¯å¾„
        :param save_best_model: æ˜¯å¦ä¿å­˜è¿‡ç¨‹ä¸­é‡åˆ°çš„æœ€ä½³æ¨¡å‹
        :param best_model_subfolder: æœ€ä½³æ¨¡å‹ä¿å­˜çš„å­ç›®å½•
        :param final_model_subfolder: æœ€ç»ˆæ¨¡å‹ä¿å­˜çš„å­ç›®å½•
        :param kwargs: å…¶ä»–å¯é€‰å‚æ•°
        :return: ä¸€ä¸ªåŒ…å«è®­ç»ƒè¿‡ç¨‹ä¿¡æ¯çš„å­—å…¸
        """
        if self.verbose:
            logger.info(f"Starting GraphEvoTrainer. Max evolution cycles: {max_evo_cycles}")

        results = {
            "evo_cycles_completed": 0,
            "phase_results": [],
            "total_refinement_steps": 0,
            "total_compressions": 0,
            "total_methods_evolved": 0,
            "best_accuracy": -1.0,
            "best_accuracy_cycle": -1,
            "best_model_snapshot": None,
            "best_model_cycle": -1
        }

        # é˜¶æ®µä¸€ï¼šå¤šæ ·åˆå§‹åŒ–
        init_result = self._phase_diverse_initialization(input_data, target)
        current_af = init_result['best_model']
        current_loss = init_result['best_loss']
        current_acc = init_result['best_accuracy']

        # æ›´æ–°å…¨å±€çŠ¶æ€
        self.adaptoflux = current_af

        # è®°å½•æœ€ä½³æ¨¡å‹
        if current_acc > results['best_accuracy']:
            results['best_accuracy'] = current_acc
            results['best_accuracy_cycle'] = 0
            results['best_model_snapshot'] = copy.deepcopy(current_af)
            results['best_model_cycle'] = 0

        results['phase_results'].append({
            'cycle': 0,
            'phase': 'initialization',
            'result': init_result
        })

        # å¼€å§‹è¿›åŒ–å¾ªç¯
        for cycle in range(1, max_evo_cycles + 1):
            if self.verbose:
                logger.info(f"--- Starting Evolution Cycle {cycle}/{max_evo_cycles} ---")

            cycle_results = {'cycle': cycle}

            # é˜¶æ®µäºŒï¼šé€èŠ‚ç‚¹ç²¾ç‚¼
            refinement_result = self._phase_node_refinement(current_af, input_data, target)
            current_af = refinement_result['final_model']
            current_loss = refinement_result['final_loss']
            current_acc = refinement_result['final_accuracy']
            results['total_refinement_steps'] += refinement_result['steps_taken']

            cycle_results['refinement'] = refinement_result

            # é˜¶æ®µä¸‰ï¼šæ–¹æ³•æ± è¿›åŒ–ï¼ˆåŸºäºå¿«ç…§è§¦å‘ï¼‰
            if self.enable_evolution:
                # 1. æŒ‰é¢‘ç‡è®°å½•å›¾å¿«ç…§
                if cycle % self.evolution_sampling_frequency == 0:
                    snapshot = copy.deepcopy(current_af)
                    self.graph_snapshots.append(snapshot)
                    if self.verbose:
                        logger.debug(f"Saved graph snapshot #{len(self.graph_snapshots)} at cycle {cycle}")

                # 2. æ£€æŸ¥æ˜¯å¦è§¦å‘è¿›åŒ–
                if len(self.graph_snapshots) >= self.evolution_trigger_count:
                    # 3. æ‰§è¡Œè¿›åŒ–
                    evolution_result = self._phase_method_pool_evolution(
                        current_af,
                        snapshots=self.graph_snapshots,
                        max_methods=self.methods_per_evolution
                    )
                    results['total_methods_evolved'] += evolution_result['methods_added']
                    cycle_results['evolution'] = evolution_result

                    # 4. æ¸…ç†å¿«ç…§
                    if self.evolution_cleanup_mode == "full":
                        self.graph_snapshots.clear()
                    elif self.evolution_cleanup_mode == "oldest":
                        if self.graph_snapshots:
                            self.graph_snapshots.pop(0)
                else:
                    cycle_results['evolution'] = {
                        'skipped': True,
                        'reason': 'insufficient_snapshots'
                    }
            else:
                cycle_results['evolution'] = {
                    'skipped': True,
                    'reason': 'disabled'
                }

            # æ›´æ–°å…¨å±€çŠ¶æ€
            self.adaptoflux = current_af

            # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°çš„æœ€ä½³æ¨¡å‹
            if current_acc > results['best_accuracy']:
                results['best_accuracy'] = current_acc
                results['best_accuracy_cycle'] = cycle
                results['best_model_snapshot'] = copy.deepcopy(current_af)
                results['best_model_cycle'] = cycle

            results['evo_cycles_completed'] += 1
            results['phase_results'].append(cycle_results)

            if self.verbose:
                logger.info(f"--- Cycle {cycle} completed. Current Acc: {current_acc:.4f}, Best Acc: {results['best_accuracy']:.4f} ---")

        if self.verbose:
            logger.info(f"GraphEvoTrainer finished after {results['evo_cycles_completed']} cycles.")


        # === é˜¶æ®µå››ï¼ˆåå¤„ç†ï¼‰ï¼šæ¨¡å—åŒ–å‹ç¼©ï¼ˆä»…æ‰§è¡Œä¸€æ¬¡ï¼‰===
        final_compression_result = None
        if self.enable_compression:
            if self.verbose:
                logger.info("[Post-Training] Applying Modular Compression once on final model...")
            final_compression_result = self._phase_modular_compression(self.adaptoflux, input_data, target)
            # æ›´æ–°æœ€ç»ˆæ¨¡å‹
            self.adaptoflux = final_compression_result['final_model']
            results['total_compressions'] = final_compression_result['compressed_subgraphs']
        else:
            if self.verbose:
                logger.info("[Post-Training] Modular Compression: SKIPPED (disabled by enable_compression=False)")
            results['total_compressions'] = 0
        
        if final_compression_result:
            results['final_compression_applied'] = final_compression_result['compression_applied']
        else:
            results['final_compression_applied'] = False

        # ä¿å­˜æ¨¡å‹
        if save_model:
            try:
                base_save_path = model_save_path or "models"
                os.makedirs(base_save_path, exist_ok=True)

                # ä¿å­˜æœ€ç»ˆæ¨¡å‹
                final_path = os.path.join(base_save_path, final_model_subfolder)
                self.adaptoflux.save_model(folder=final_path)
                if self.verbose:
                    logger.info(f"Final model saved to '{final_path}'")
                results["final_model_saved"] = final_path

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if save_best_model and results['best_model_snapshot'] is not None:
                    best_path = os.path.join(base_save_path, best_model_subfolder)

                    # ä¸´æ—¶åˆ‡æ¢
                    original_af = self.adaptoflux
                    self.adaptoflux = results['best_model_snapshot']
                    try:
                        self.adaptoflux.save_model(folder=best_path)
                        if self.verbose:
                            logger.info(f"Best model (Cycle {results['best_model_cycle']}, Acc={results['best_accuracy']:.4f}) saved to '{best_path}'")
                    finally:
                        self.adaptoflux = original_af

                    results["best_model_saved"] = best_path

                # ä¿å­˜è®­ç»ƒæ—¥å¿—
                log_filename = kwargs.get("log_filename", "graph_evo_training_log.json")
                log_path = os.path.join(base_save_path, log_filename)
                with open(log_path, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=4, default=str)
                results["training_log_saved"] = log_path

            except Exception as e:
                logger.error(f"Failed to save model(s): {e}")
                import traceback
                logger.error(traceback.format_exc())
        
        return results
    
    def _refine_random_single_step(
        self,
        adaptoflux_instance,
        input_data: np.ndarray,
        target: np.ndarray,
        processing_nodes: List[str],
        current_loss: float,
        gp: Any  # GraphProcessor å®ä¾‹
    ) -> Tuple[bool, float, float, int, List[str]]:
        """
        ã€ç­–ç•¥ï¼šéšæœºå•ç‚¹ä¼˜åŒ–ã€‘
        åœ¨å½“å‰å›¾ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªå¯ä¼˜åŒ–çš„å¤„ç†èŠ‚ç‚¹ï¼Œå°è¯•å°†å…¶æ–¹æ³•æ›¿æ¢ä¸ºæ‰€æœ‰å…¼å®¹æ–¹æ³•ä¸­çš„æœ€ä¼˜è€…ï¼ˆåŸºäºæŸå¤±ä¸‹é™ï¼‰ã€‚
        ä»…æ‰§è¡Œä¸€æ¬¡æ›¿æ¢å°è¯•ï¼ˆå³ä¸€ä¸ªèŠ‚ç‚¹çš„ä¸€æ¬¡ä¼˜åŒ–ï¼‰ï¼Œé€‚ç”¨äºè½»é‡çº§ã€ä½å¼€é”€çš„å±€éƒ¨æœç´¢ã€‚
        æ³¨æ„ï¼šæœ¬å‡½æ•°ä¼š in-place ä¿®æ”¹ adaptoflux_instance çš„å›¾ç»“æ„ï¼

        è¿”å›å€¼è¯´æ˜ï¼š
        - bool: æœ¬è½®æ˜¯å¦æˆåŠŸæ”¹è¿›ï¼ˆå³æ‰¾åˆ°æ›´ä¼˜æ–¹æ³•ï¼‰
        - float: æ”¹è¿›åçš„æŸå¤±å€¼ï¼ˆè‹¥æœªæ”¹è¿›åˆ™è¿”å›åŸæŸå¤±ï¼‰
        - float: æ”¹è¿›åçš„å‡†ç¡®ç‡ï¼ˆè‹¥æœªæ”¹è¿›åˆ™è¿”å› 0.0ï¼Œè°ƒç”¨æ–¹åº”å¿½ç•¥ï¼‰
        - int: æœ¬æ¬¡å®é™…æ‰§è¡Œçš„ä¼˜åŒ–æ­¥æ•°ï¼ˆ0 æˆ– 1ï¼‰
        - List[str]: æ›´æ–°åçš„å¯å¤„ç†èŠ‚ç‚¹åˆ—è¡¨ï¼ˆå› å›¾ç»“æ„å¯èƒ½å˜åŒ–ï¼Œéœ€é‡æ–°è·å–ï¼‰

        :param adaptoflux_instance: å½“å‰å¾…ä¼˜åŒ–çš„ AdaptoFlux å®ä¾‹
        :param input_data: ç”¨äºè¯„ä¼°çš„å°æ‰¹é‡è¾“å…¥æ•°æ®
        :param target: å¯¹åº”çš„çœŸå®æ ‡ç­¾
        :param processing_nodes: å½“å‰å›¾ä¸­æ‰€æœ‰å¯è¢«ä¼˜åŒ–çš„å¤„ç†èŠ‚ç‚¹åç§°åˆ—è¡¨ï¼ˆå·²æ’é™¤å†»ç»“èŠ‚ç‚¹ï¼‰
        :param current_loss: å½“å‰æ¨¡å‹çš„æŸå¤±å€¼ï¼ˆç”¨äºæ¯”è¾ƒï¼‰
        :param gp: å›¾å¤„ç†å™¨å®ä¾‹ï¼Œç”¨äºè®¿é—®å’Œä¿®æ”¹å›¾ç»“æ„
        :return: (æ˜¯å¦æ”¹è¿›, æ–°æŸå¤±, æ–°å‡†ç¡®ç‡, æ­¥æ•°å¢é‡, æ›´æ–°åçš„èŠ‚ç‚¹åˆ—è¡¨)
        """
        # è‹¥æ— å¯ä¼˜åŒ–èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›æ— æ”¹è¿›
        if not processing_nodes:
            return False, current_loss, 0.0, 0, processing_nodes

        # éšæœºé€‰æ‹©ä¸€ä¸ªå¾…ä¼˜åŒ–èŠ‚ç‚¹
        target_node = random.choice(processing_nodes)
        original_method_name = gp.graph.nodes[target_node]['method_name']
        
        # è·å–ä¸è¯¥èŠ‚ç‚¹å…¼å®¹çš„å€™é€‰æ–¹æ³•åˆ—è¡¨ï¼ˆåŸºäºç»„åˆ«æˆ–ç±»å‹åŒ¹é…ï¼‰
        candidate_methods = self._get_compatible_methods_for_node(
            adaptoflux_instance, 
            target_node
        )

        best_candidate = None
        best_loss = current_loss  # åˆå§‹åŒ–ä¸ºå½“å‰æŸå¤±ï¼Œç”¨äºæ¯”è¾ƒ
        
        # éå†æ‰€æœ‰å€™é€‰æ–¹æ³•ï¼ˆè·³è¿‡å½“å‰æ–¹æ³•ï¼‰
        for candidate_method_name in candidate_methods:
            if candidate_method_name == original_method_name:
                continue

            # åˆ›å»ºä¸´æ—¶å‰¯æœ¬ï¼Œé¿å…æ±¡æŸ“åŸæ¨¡å‹
            temp_af = copy.deepcopy(adaptoflux_instance)
            temp_gp = temp_af.graph_processor
            # å°è¯•æ›¿æ¢èŠ‚ç‚¹æ–¹æ³•
            temp_gp.graph.nodes[target_node]['method_name'] = candidate_method_name

            # è¯„ä¼°æ›¿æ¢åçš„æŸå¤±
            new_loss = self._evaluate_loss_with_instance(temp_af, input_data, target)


            # è®°å½•æŸå¤±æ›´ä½çš„æœ€ä¼˜å€™é€‰
            if new_loss < best_loss:
                best_loss = new_loss
                best_candidate = candidate_method_name

        # å¦‚æœæ‰¾åˆ°æ›´ä¼˜æ–¹æ³•ï¼Œåˆ™åº”ç”¨åˆ°åŸå›¾
        if best_candidate and best_loss < current_loss:
            # === æ›¿æ¢èŠ‚ç‚¹ï¼ˆè‡ªåŠ¨æ›´æ–° ID å’Œè¾¹ï¼‰ ===
            new_node_id = gp.replace_node_method(target_node, best_candidate)
            
            # æ³¨æ„ï¼štarget_node å·²è¢«åˆ é™¤ï¼Œåç»­æ“ä½œåº”ä½¿ç”¨ new_node_idï¼ˆä½†æœ¬ç­–ç•¥ä¸éœ€è¦ï¼‰
            # å¦‚æœä½œè€…æœ‰ç©ºè€Œä¸”æ²¡å¿˜è®°å¯èƒ½ä¼šåœ¨gpé‡Œé¢åŠ ä¸€ä¸ªåˆ·æ–°å›¾èŠ‚ç‚¹idçš„æ–¹æ³•æå‡å¯è¯»æ€§

            # é‡æ–°è¯„ä¼°å‡†ç¡®ç‡
            new_acc = self._evaluate_accuracy_with_instance(adaptoflux_instance, input_data, target)

            # é‡æ–°è·å–å¤„ç†èŠ‚ç‚¹åˆ—è¡¨ï¼ˆå› ä¸ºèŠ‚ç‚¹ ID å·²å˜ï¼‰
            updated_nodes = [node for node in gp.graph.nodes() if gp._is_processing_node(node)]
            
            return True, best_loss, new_acc, 1, updated_nodes
        
        else:
            # æ— æ”¹è¿›ï¼Œè¿”å›åŸå§‹çŠ¶æ€
            return False, current_loss, 0.0, 0, processing_nodes

    def _refine_full_sweep_step(
        self,
        adaptoflux_instance,
        input_data: np.ndarray,
        target: np.ndarray,
        processing_nodes: List[str],
        current_loss: float,
        gp: Any
    ) -> Tuple[bool, float, float, int, List[str]]:
        """
        ã€ç­–ç•¥ï¼šå®Œæ•´éå†ä¼˜åŒ–ã€‘
        å¯¹å½“å‰å›¾ä¸­æ‰€æœ‰å¯ä¼˜åŒ–çš„å¤„ç†èŠ‚ç‚¹è¿›è¡Œä¸€è½®å®Œæ•´éå†ã€‚
        å¯¹æ¯ä¸ªèŠ‚ç‚¹ï¼Œå°è¯•æ‰€æœ‰å…¼å®¹æ–¹æ³•ï¼Œè‹¥å‘ç°èƒ½é™ä½æŸå¤±çš„æ›¿æ¢ï¼Œåˆ™ç«‹å³åº”ç”¨ï¼ˆè´ªå¿ƒç­–ç•¥ï¼‰ã€‚
        ä¸€è½®ä¸­å¯èƒ½å¤šæ¬¡ä¿®æ”¹å›¾ç»“æ„ï¼Œé€‚ç”¨äºæ›´å½»åº•çš„å±€éƒ¨ä¼˜åŒ–ï¼Œä½†è®¡ç®—å¼€é”€è¾ƒå¤§ã€‚
        æ³¨æ„ï¼šæœ¬å‡½æ•°ä¼š in-place ä¿®æ”¹ adaptoflux_instance çš„å›¾ç»“æ„ï¼

        è¿”å›å€¼è¯´æ˜ï¼š
        - bool: æœ¬è½®æ˜¯å¦è‡³å°‘æœ‰ä¸€æ¬¡æˆåŠŸæ”¹è¿›
        - float: æœ¬è½®ç»“æŸåçš„æœ€ç»ˆæŸå¤±å€¼
        - float: æœ¬è½®ç»“æŸåçš„æœ€ç»ˆå‡†ç¡®ç‡
        - int: æœ¬è½®æ€»å…±æ‰§è¡Œçš„æ–¹æ³•æ›¿æ¢æ¬¡æ•°
        - List[str]: æœ€ç»ˆçš„å¯å¤„ç†èŠ‚ç‚¹åˆ—è¡¨ï¼ˆå¯èƒ½å› æ–¹æ³•å˜æ›´è€ŒåŠ¨æ€å˜åŒ–ï¼‰

        :param adaptoflux_instance: å½“å‰å¾…ä¼˜åŒ–çš„ AdaptoFlux å®ä¾‹
        :param input_data: ç”¨äºè¯„ä¼°çš„å°æ‰¹é‡è¾“å…¥æ•°æ®
        :param target: å¯¹åº”çš„çœŸå®æ ‡ç­¾
        :param processing_nodes: åˆå§‹çš„å¯ä¼˜åŒ–èŠ‚ç‚¹åˆ—è¡¨
        :param current_loss: å½“å‰æŸå¤±ï¼ˆä½œä¸ºèµ·ç‚¹ï¼‰
        :param gp: å›¾å¤„ç†å™¨å®ä¾‹
        :return: (æ˜¯å¦æ”¹è¿›, æœ€ç»ˆæŸå¤±, æœ€ç»ˆå‡†ç¡®ç‡, æ›¿æ¢æ¬¡æ•°, æœ€ç»ˆèŠ‚ç‚¹åˆ—è¡¨)
        """
        if not processing_nodes:
            return False, current_loss, 0.0, 0, processing_nodes

        # éšæœºæ‰“ä¹±èŠ‚ç‚¹é¡ºåºï¼Œé¿å…é¡ºåºåå·®ï¼ˆä¾‹å¦‚æ€»æ˜¯å…ˆä¼˜åŒ–é å‰çš„èŠ‚ç‚¹ï¼‰
        nodes_to_try = random.sample(processing_nodes, len(processing_nodes))
        improvement_made = False
        total_replacements = 0
        current_acc = self._evaluate_accuracy_with_instance(adaptoflux_instance, input_data, target)

        # éå†æ¯ä¸€ä¸ªå¾…ä¼˜åŒ–èŠ‚ç‚¹
        for target_node in nodes_to_try:
            # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦ä»ç„¶å­˜åœ¨äºå›¾ä¸­ï¼ˆå¯èƒ½è¢«ä¹‹å‰çš„æ›¿æ¢æ“ä½œåˆ é™¤æˆ–é‡å‘½åï¼‰
            if target_node not in gp.graph.nodes:
                if self.verbose:
                    logger.debug(f"Node '{target_node}' no longer exists in graph; skipping.")
                continue

            original_method_name = gp.graph.nodes[target_node].get('method_name')
            if original_method_name is None:
                if self.verbose:
                    logger.warning(f"Node '{target_node}' has no 'method_name'; skipping.")
                continue

            # è·å–ä¸è¯¥èŠ‚ç‚¹å…¼å®¹çš„å€™é€‰æ–¹æ³•åˆ—è¡¨ï¼ˆè€ƒè™‘ç»„åˆ«ã€ç±»å‹å…¼å®¹æ€§åŠå†»ç»“è§„åˆ™ï¼‰
            candidate_methods = self._get_compatible_methods_for_node(
                adaptoflux_instance,
                target_node,
                compatibility_mode=self.compatibility_mode
            )

            best_candidate = None
            best_loss = current_loss  # ä»¥å½“å‰å…¨å±€æŸå¤±ä¸ºåŸºå‡†è¿›è¡Œæ¯”è¾ƒ

            # å°è¯•æ‰€æœ‰å…¼å®¹æ–¹æ³•ï¼ˆè·³è¿‡å½“å‰æ–¹æ³•ï¼‰
            for candidate_method_name in candidate_methods:
                if candidate_method_name == original_method_name:
                    continue

                # åˆ›å»ºä¸´æ—¶å‰¯æœ¬è¿›è¡Œå®‰å…¨è¯„ä¼°ï¼Œé¿å…æ±¡æŸ“åŸæ¨¡å‹
                try:
                    temp_af = copy.deepcopy(adaptoflux_instance)
                    temp_gp = temp_af.graph_processor
                    # å®‰å…¨æ›¿æ¢ï¼šä½¿ç”¨å›¾å¤„ç†å™¨çš„æ ‡å‡†æ–¹æ³•ï¼ˆä¼šå¤„ç†è¾“å…¥/è¾“å‡ºç±»å‹å˜åŒ–ï¼‰
                    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿæ›¿æ¢ï¼Œä½†ä¸å®é™…è°ƒç”¨ replace_node_methodï¼ˆå› ä¸ºåªæ˜¯è¯„ä¼°ï¼‰
                    # æ‰€ä»¥ç›´æ¥ä¿®æ”¹ method_name æ˜¯å®‰å…¨çš„ï¼Œå‰ææ˜¯ä¸ä¾èµ–è¾¹ç»“æ„å˜åŒ–
                    temp_gp.graph.nodes[target_node]['method_name'] = candidate_method_name

                    new_loss = self._evaluate_loss_with_instance(temp_af, input_data, target)

                    if new_loss < best_loss:
                        best_loss = new_loss
                        best_candidate = candidate_method_name
                except Exception as e:
                    logger.warning(f"Failed to evaluate candidate method '{candidate_method_name}' "
                                   f"for node '{target_node}': {e}")
                    continue

            # å¦‚æœæ‰¾åˆ°æ›´ä¼˜æ–¹æ³•ï¼Œç«‹å³åº”ç”¨åˆ°åŸå›¾ï¼ˆè´ªå¿ƒç­–ç•¥ï¼‰
            if best_candidate and best_loss < current_loss:
                try:
                    # ä½¿ç”¨å›¾å¤„ç†å™¨çš„æ ‡å‡†æ›¿æ¢æ–¹æ³•ï¼Œç¡®ä¿å›¾ç»“æ„ä¸€è‡´æ€§ï¼ˆå¦‚è¾¹æ›´æ–°ã€IDåˆ·æ–°ç­‰ï¼‰
                    new_node_id = gp.replace_node_method(target_node, best_candidate)
                    # æ›´æ–°å½“å‰æŸå¤±å’Œå‡†ç¡®ç‡
                    current_loss = best_loss
                    current_acc = self._evaluate_accuracy_with_instance(adaptoflux_instance, input_data, target)
                    improvement_made = True
                    total_replacements += 1

                    if self.verbose:
                        logger.info(f"  Replacement {total_replacements}: Node '{target_node}' "
                                    f"{original_method_name} â†’ {best_candidate}, Loss: {current_loss:.6f}")

                    # é‡è¦ï¼šèŠ‚ç‚¹æ›¿æ¢åï¼ŒåŸ target_node å¯èƒ½å·²è¢«åˆ é™¤æˆ–é‡å‘½åï¼ˆnew_node_idï¼‰
                    # å› æ­¤éœ€è¦é‡æ–°è·å–å½“å‰æ‰€æœ‰å¤„ç†èŠ‚ç‚¹ï¼Œç¡®ä¿åç»­æ“ä½œåŸºäºæœ€æ–°å›¾çŠ¶æ€
                    processing_nodes = [
                        node for node in gp.graph.nodes()
                        if gp._is_processing_node(node)
                    ]
                    # æ³¨æ„ï¼šnodes_to_try æ˜¯æ—§åˆ—è¡¨ï¼Œä½†ä»…åŒ…å«èŠ‚ç‚¹åï¼Œåç»­èŠ‚ç‚¹è‹¥ä»å­˜åœ¨ä»å¯å¤„ç†ï¼›
                    # è‹¥éœ€æ›´ä¸¥æ ¼çš„ä¸€è‡´æ€§ï¼Œå¯è€ƒè™‘ break å¹¶é‡å¯æœ¬è½®ï¼Œä½†ä¼šå¢åŠ å¼€é”€ï¼Œæ­¤å¤„æš‚ä¸å¤„ç†ã€‚

                except Exception as e:
                    logger.error(f"Failed to apply method replacement for node '{target_node}': {e}")
                    # æ›¿æ¢å¤±è´¥ï¼Œè·³è¿‡è¯¥èŠ‚ç‚¹ï¼Œç»§ç»­ä¼˜åŒ–å…¶ä»–èŠ‚ç‚¹
                    continue

        return improvement_made, current_loss, current_acc, total_replacements, processing_nodes
        
    def build_candidate_pool(compatibility_mode, methods, original_method_name, all_method_names):
        if compatibility_mode == "all":
            return all_method_names
        else:
            node_group = methods[original_method_name].get("group", "default")
            group_methods = [
                name for name, info in methods.items()
                if info.get("group", "default") == node_group
            ]
            if compatibility_mode == "group_only":
                return group_methods if group_methods else all_method_names[:1]
            else:  # group_with_fallback
                return group_methods if len(group_methods) >= 2 else all_method_names