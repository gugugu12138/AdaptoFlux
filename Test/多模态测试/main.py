# Test/test_multimodal.py
import numpy as np
from ATF import AdaptoFlux

# å®šä¹‰å¤šæ¨¡æ€æ–¹æ³•ï¼ˆç›´æ¥å†…è”ï¼Œä¸ä¾èµ–å¤–éƒ¨æ–‡ä»¶ï¼‰
def process_text(txt):
    """å¤„ç†å­—ç¬¦ä¸²ï¼šè¿”å›é•¿åº¦å’Œå¤§å†™"""
    return [len(txt), txt.upper()]

def process_image(img):
    """å¤„ç†å›¾åƒï¼ˆæ¨¡æ‹Ÿï¼‰ï¼šè¿”å›å‡å€¼å’Œå½¢çŠ¶"""
    return [float(img.mean()), img.shape]

def fuse_features(num, txt_len, txt_upper, img_mean, img_shape):
    """èåˆæ‰€æœ‰ç‰¹å¾ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœ"""
    return {
        "number": num,
        "text_length": txt_len,
        "text_upper": txt_upper,
        "image_mean": img_mean,
        "image_shape": img_shape
    }

# æ‰‹åŠ¨æ³¨å†Œæ–¹æ³•ï¼ˆç»•è¿‡ methods_pathï¼‰
methods = {
    "process_text": {
        "function": process_text,
        "input_count": 1,
        "output_count": 2,
        "input_types": ["str"],
        "output_types": ["int", "str"],
        "group": "default",
        "weight": 1.0,
        "vectorized": False
    },
    "process_image": {
        "function": process_image,
        "input_count": 1,
        "output_count": 2,
        "input_types": ["image"],
        "output_types": ["float", "tuple"],
        "group": "default",
        "weight": 1.0,
        "vectorized": False
    },
    "fuse_features": {
        "function": fuse_features,
        "input_count": 5,
        "output_count": 1,
        "input_types": ["scalar", "int", "str", "float", "tuple"],
        "output_types": ["dict"],
        "group": "default",
        "weight": 1.0,
        "vectorized": False
    }
}

# æ„é€ å¤šæ¨¡æ€è¾“å…¥ï¼šæ¯ä¸ªæ ·æœ¬ = [æ•°å€¼, å­—ç¬¦ä¸², å›¾åƒ]
values = [
    [42, "hello", np.random.rand(8, 8, 3)],
    [-10, "world", np.random.rand(16, 16, 1)]
]

# æ˜¾å¼å£°æ˜æ¯åˆ—çš„è¯­ä¹‰ç±»å‹
input_types_list = ["scalar", "str", "image"]

# åˆå§‹åŒ– AdaptoFluxï¼ˆä¸ä¼  values/labels ä»¥é¿å…è‡ªåŠ¨æ¨æ–­ï¼‰
af = AdaptoFlux(input_types_list=input_types_list)
af.set_methods(methods)

# === æ‰‹åŠ¨æ„å»ºå›¾ ===
G = af.graph

# æ¸…ç©ºé»˜è®¤è¾¹ï¼ˆroot â†’ collapseï¼‰
G.remove_edges_from(list(G.in_edges("collapse")))

# æ·»åŠ å¤„ç†èŠ‚ç‚¹
G.add_node("text_proc", method_name="process_text", layer=1)
G.add_node("img_proc", method_name="process_image", layer=1)
G.add_node("fuser", method_name="fuse_features", layer=2)

# root â†’ å¤„ç†èŠ‚ç‚¹
G.add_edge("root", "text_proc", output_index=1, data_coord=1, data_type="str")      # å­—ç¬¦ä¸²åˆ—
G.add_edge("root", "img_proc", output_index=2, data_coord=2, data_type="image")    # å›¾åƒåˆ—

# å¤„ç†èŠ‚ç‚¹ â†’ fuser
# fuser çš„è¾“å…¥é¡ºåºå¿…é¡»åŒ¹é… fuse_features(num, txt_len, txt_upper, img_mean, img_shape)

# 1. åŸå§‹æ•°å€¼ (num) â†’ å‚æ•° 0
G.add_edge("root", "fuser", output_index=0, data_coord=0, data_type="scalar")

# 2. text_len â†’ å‚æ•° 1
G.add_edge("text_proc", "fuser", output_index=0, data_coord=1, data_type="int")
# 3. text_upper â†’ å‚æ•° 2
G.add_edge("text_proc", "fuser", output_index=1, data_coord=2, data_type="str")

# 4. img_mean â†’ å‚æ•° 3
G.add_edge("img_proc", "fuser", output_index=0, data_coord=3, data_type="float")
# 5. img_shape â†’ å‚æ•° 4
G.add_edge("img_proc", "fuser", output_index=1, data_coord=4, data_type="tuple")

# fuser â†’ collapse
G.add_edge("fuser", "collapse", output_index=0, data_coord=0, data_type="dict")

# è‡ªå®šä¹‰ collapseï¼šç›´æ¥è¿”å›ç»“æœï¼ˆä¸èšåˆï¼‰
af.set_custom_collapse(lambda x: x[0])  # x æ˜¯ [dict]ï¼Œå–ç¬¬ä¸€ä¸ª

# æ‰§è¡Œæ¨ç†
results = af.infer_with_graph(values)

print("âœ… å¤šæ¨¡æ€æ¨ç†æˆåŠŸï¼ç»“æœç¤ºä¾‹ï¼š")
for i, res in enumerate(results):
    print(f"\næ ·æœ¬ {i}:")
    print(f"  number: {res['number']}")
    print(f"  text_length: {res['text_length']}")
    print(f"  text_upper: {res['text_upper']}")
    print(f"  image_mean: {res['image_mean']:.4f}")
    print(f"  image_shape: {res['image_shape']}")

import time

def slow_exp_sum(a, b):
    """é€æ ·æœ¬ï¼šè®¡ç®— exp(a) + exp(b)"""
    return np.exp(a) + np.exp(b)  # æ³¨æ„ï¼šè¿™é‡Œç”¨ np.exp ä½†ä»æ˜¯é€æ ·æœ¬ï¼

def fast_exp_sum(a, b):
    """å‘é‡åŒ–ï¼šæ‰¹é‡è®¡ç®— exp(a) + exp(b)"""
    return np.exp(a) + np.exp(b)  # a, b æ˜¯ (N,) æ•°ç»„

# ======================
# 2. æ³¨å†Œæ–¹æ³•
# ======================
methods = {
    "slow_exp": {
        "function": slow_exp_sum,
        "input_count": 2,
        "output_count": 1,
        "input_types": ["scalar", "scalar"],
        "output_types": ["scalar"],
        "vectorized": False  # é€æ ·æœ¬
    },
    "fast_exp": {
        "function": fast_exp_sum,
        "input_count": 2,
        "output_count": 1,
        "input_types": ["scalar", "scalar"],
        "output_types": ["scalar"],
        "vectorized": True   # å‘é‡åŒ–
    }
}

# ======================
# 3. æ„å»ºçº¯æ•°å€¼è¾“å…¥ï¼ˆæ— å­—ç¬¦ä¸²/å›¾åƒï¼‰
# ======================
N = 100000  # å¤§æ ·æœ¬é‡
values = np.random.rand(N, 2)  # çº¯æ•°å€¼çŸ©é˜µ
input_types_list = ["scalar", "scalar"]

# ======================
# 4. æµ‹è¯•å‡½æ•°ï¼ˆçº¯æ•°å€¼å›¾ï¼‰
# ======================
def test_pure_vectorized(method_name, description):
    af = AdaptoFlux(input_types_list=input_types_list)
    af.set_methods(methods)
    G = af.graph
    
    # æ¸…ç©ºé»˜è®¤è¾¹
    G.remove_edges_from(list(G.in_edges("collapse")))
    
    # æ·»åŠ çº¯æ•°å€¼èŠ‚ç‚¹
    G.add_node("exp_node", method_name=method_name, layer=1)
    G.add_edge("root", "exp_node", output_index=0, data_coord=0, data_type="scalar")
    G.add_edge("root", "exp_node", output_index=1, data_coord=1, data_type="scalar")
    G.add_edge("exp_node", "collapse", output_index=0, data_coord=0, data_type="scalar")
    
    # ç®€å• collapse
    af.set_custom_collapse(lambda x: x[0])
    
    # è®¡æ—¶
    start = time.time()
    results = af.infer_with_graph(values)
    elapsed = time.time() - start
    
    # éªŒè¯
    expected = np.exp(values[:, 0]) + np.exp(values[:, 1])
    assert np.allclose(results, expected, atol=1e-6)
    
    print(f"{description}: {elapsed:.4f} ç§’ (N={N})")
    return elapsed

# ======================
# 5. æ‰§è¡Œæµ‹è¯•
# ======================
if __name__ == "__main__":
    print("ğŸš€ æµ‹è¯•çº¯æ•°å€¼å‘é‡åŒ–åŠ é€Ÿ...\n")
    
    time_slow = test_pure_vectorized("slow_exp", "é€æ ·æœ¬ (vectorized=False)")
    time_fast = test_pure_vectorized("fast_exp", "å‘é‡åŒ– (vectorized=True)")
    
    speedup = time_slow / time_fast
    print(f"\nğŸ”¥ åŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    if speedup > 10:
        print("âœ… å‘é‡åŒ–æˆåŠŸï¼æ€§èƒ½æ˜¾è‘—æå‡ã€‚")
    else:
        print("âš ï¸ ä»æœªåŠ é€Ÿï¼ˆæ£€æŸ¥ NumPy å®‰è£…æˆ–æ“ä½œå¤æ‚åº¦ï¼‰")

import numpy as np
import time

N = 10000

# é€æ ·æœ¬
a = np.random.rand(N)
b = np.random.rand(N)

start = time.time()
result1 = [np.exp(ai) + np.exp(bi) for ai, bi in zip(a, b)]
print("é€æ ·æœ¬:", time.time() - start)

# å‘é‡åŒ–
start = time.time()
result2 = np.exp(a) + np.exp(b)
print("å‘é‡åŒ–:", time.time() - start)

print("åŠ é€Ÿæ¯”:", (time.time() - start) / (time.time() - start))  # ä¼ªä»£ç 